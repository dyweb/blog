[{"content":"\n\n机器学习平台对于不同的工程师角色而言，有着不同的意涵。随着之前几篇关于 [Kubeflow][] 的文章发布后，有不少的网友私下询问到底什么是机器学习平台，它与机器学习框架有何不同等问题。这篇文章希望能够从不同的维度来介绍一下，我们一直谈论的机器学习平台，到底是什么。首先给大家讲个故事：\n\n小咩是 TooYoung 科技的算法工程师，最近他正在为公司实现一个图像识别的模型。为了支持他的工作，公司的 Infra 团队的工程师小婶给了他四台每台带有 4 块英伟达显卡的机器，告诉他在接下来的一个月，他可以完全占有这几台机器。小咩表面风轻云淡，心里其实已经乐开了花，他已经很久没有在这么多显卡的机器上放飞过自我了。\n\n小咩拿到机器后，需要做的第一件事就是先配环境。经过简单的权衡后，小咩决定使用自己熟悉的 TensorFlow 来进行模型的开发工作。尽管 TensorFlow 已经发布了 2.0，但小咩是一个恋旧的人，他还是习惯使用经典的 1.4 版本。小咩登录到了机器上，他发现问题并不简单。四台机器中，机器 A 的 TensorFlow 只支持 CPU；机器 B 的 TensorFlow 的版本是 1.13；机器 C 之前是给公司里的算法科学家小莎在投稿 NIPS 时做实验用的，只安装了 PyTorch。\n\n小咩叹了一口气，飞快地下楼买了一瓶快乐水，撸起了袖子，开始了环境的配置之旅。他熟练地卸载了机器 A 的环境，首先确认了机器 A 上没有安装开源的英伟达驱动 nouveau。然后小咩开始安装英伟达的官方驱动以及 cudnn，最后安装了支持 GPU 的 TensorFlow 版本。看着屏幕上打出的 Hello World，小咩嘴角扬起了微笑。第二台机器的问题在于 TensorFlow 的版本太新了，于是小咩按图索骥，同样卸载了新的版本，安装了旧的版本，但发现 TensorFlow 1.4 不支持机器 B 上的 CUDA 9.0。小咩不情愿地打开了谷歌，熟练地键入了 \"Remove CUDA 9.0 and install CUDA 8.0\"。按照网友的指示，他终于解决了这个问题。第三台机器问题比较少，小咩很快就处理好了。\n\n接下来，小咩开始了自己的算法实验。小咩为了方便，先在自己的笔记本电脑上建立了一个 Jupyter Notebook，将数据下载到了电脑上，进行了小规模的实验。经过一天的努力后，小咩觉得自己的算法效果还算不错，可以放到服务器上进行分布式训练，以期更快的训练速度。小咩利用 /etc/hosts 给四个服务器做了一个简单的服务发现，利用 TensorFlow 的分布式训练功能进行了分布式的训练。\n\n小咩又进行了数天的调参后，模型的各项 metrics 都达到了公司的要求，于是准备将模型发布到公司的生产环境中。小咩利用了开源项目 TensorFlow Serving，在公司内部的 PaaS 平台上新建了一个服务，将自己训练的模型发布到了公司的生产集群上。小咩看着流量稳定地灌入自己的模型服务，深吸了一口气。\n\n时间很快地过去了，小咩开发的模型不知不觉已经在线上服务了许多个月，小咩也由工程师变成了公司里的 Tech Lead。最近有了关于这一模型的增量模型，但小咩已经没有足够的时间再参与到一线的开发工作中了。小咩团队里的新进工程师小豆接起了这项任务，利用新的数据集重新训练模型并发布出去。\n\n在几个星期之前，Infra 团队利用开源项目 [Kubeflow][] 为算法工程师和算法科学家们搭建了一个麻雀虽小五脏俱全的机器学习平台。小豆决定利用这一平台对模型进行训练。他首先利用 Infra 团队已经打包好的 TensorFlow 1.4 的 Docker 镜像，利用平台发起了一次分布式的训练。训练中的服务发现，异常处理等都由平台自动完成。小豆在训练的间隙看文档发现平台还有超参数训练的支持，而且使用起来非常简单，只需要指定相应的参数搜索空间即可。于是小豆又发起了一次超参数学习任务，优化了一下模型的超参数选择。\n\n在训练结束后，小豆利用平台上已有的模型服务功能，直接将训练好的模型上传到了分布式存储中，平台根据配置将模型自动地部署了起来，并且针对算法工程师们关注的指标进行了细粒度的监控。\n\n通过这个故事，我们可以了解到，机器学习平台的用户往往是机器学习算法科学家或者工程师。而机器学习平台希望解决的是机器学习工程化落地的问题。在小咩第一次进行模型开发与部署的时候，他遇到了很多来自系统环境和服务发现等原本应该由 Infra 来解决的问题。这其中包括服务器上的显卡驱动问题，TensorFlow 版本的问题，服务发现的问题，训练过程中的跟踪与错误恢复等。而小豆在机器学习平台上进行训练和模型发布时，这些问题都交由平台来处理，他可以专注于业务的开发上。\n\n从机器学习工程师的角度而言，如 TensorFlow，PyTorch 等框架改变的是机器学习的编程范式，而机器学习平台改变的是机器学习的开发与发布流程。而从一个基础架构工程师的角度来讲，机器学习平台是与 PaaS 有一定类似，但又有所不同的新的平台系统。两者相同之处在于都涉及到资源的管理与调度，服务发现等功能，不同之处在于机器学习平台对于 GPU 有极其强烈的需求，与此同时与传统的应用相比，机器学习有着不同的工作流程。举个例子说明，传统的应用可以很好地抽象出持续集成与持续部署工作流，应用的每一次提交都可以触发对应的测试与发布流程。而对于机器学习任务来说，测试往往并不是对代码本身的测试，而是对模型的效果的测试。这样的诸多不同导致了目前的 PaaS 等平台不能很好地处理机器学习这一应用场景的需求。这时候我们需要一个新的平台，它会继承 PaaS 的某些功能，但为了更好地支持机器学习业务，也多了许多新的特性。下面我们来具体地谈谈，这些特性包括什么。\n\n准备数据是一次机器学习任务的起点，但数据准备的平台化，从实现上来说应该是比较困难的。因为数据准备是一个需求差异非常大，很难将其标准化的过程。目前开源领域也有一些针对不同场景的打标工具，如 [labelme](https://github.com/wkentaro/labelme) 等，但这一方面的工程实现和研究性工作都不太常见。\n\n训练是机器学习任务中的重中之重。机器学习平台的训练支持指的是，用户通过指定使用的资源数量，分布式模型（AllReduce，ParameterServer 等），分布式配置（ParameterServer 数量等）等，直接在平台上进行模型训练。从平台的角度来看，这一特性主要涉及到对不同类型的框架，不同的分布式模型，不同的硬件的支持，以及分布式训练任务的服务发现，错误处理，不同任务的资源隔离与复用等问题。除此之外，有些场景对于在线训练也有需要，如何支持在线训练，是一个机器学习全流程都需要考虑的问题。\n\n再接下来，就是模型服务。这一特性与传统的 PaaS 比较类似，因为模型服务目前多是以 RESTful API 暴露给外部的，与传统的 Web 服务非常类似。不过从实现角度而言，不同的框架训练的模型往往需要用不同的方式发布出去，而且模型服务关注的度量指标与传统 Web 服务也有较大差别。\n\n回到版本管理，传统的应用往往只涉及配置与代码的管理，而机器学习则多了不少新的维度，如模型，版本等。这一部分的特性虽然比较 dirty work，但是却与用户的使用体验息息相关。除此之外，还有整个过程中的监控问题（训练过程监控，服务过程监控），也是同样性质的工作。\n\n在解决完上面的问题后，机器学习工作流的构建这一特性就水到渠成地摆到台面上了。如何让用户用尽可能少的交互取得他/她想要的效果，以及如何加强这一过程的自动化，是离不开工作流方面的工作的。\n\n而超参数训练与模型结构搜索，是机器学习平台的高级特性。虽然自动机器学习听上去非常有前途，但目前仍主要处于研究阶段。对于超参数训练来说，最难过的一关是性价比问题。最常用的 Random Search 与 Grid Search，和贝叶斯优化方法，寻找到的参数确实可能比人工调参有更好的效果，但与此同时也会需要更多的硬件资源。因此这一特性属于锦上添花性质的功能，而不能起到雪中送炭的作用。至于模型结构搜索，就更遥远了。\n\n上述的介绍是挂一漏万的，一个成熟的平台系统一定有更多的细节值得去讨论，限于篇幅关系不再展开。这里只是大致说明一下，机器学习平台究竟是怎样的一个存在，它可以帮助到用户做到什么事情，提高了哪些方面的效率。\n\n在介绍完之后，再打一个小小的广告。我们团队目前正在招聘中。如果你对构建基于 Kubernetes 的 Cloud Native 的机器学习平台系统感兴趣，可以了解下我司[才云科技](https://caicloud.io)的产品 [Clever](https://caicloud.io/products/clever)。我们很早以来就一直在做这一方面的工作，目前我司在机器学习平台开源项目 [Kubeflow][] 上贡献位列全球前三。如果各位读者对[我司的工作职位](https://www.lagou.com/gongsi/j85122.html)感兴趣，欢迎投递简历到 gaoce@caicloud.io。\n\n也欢迎各位同行多多指教，共同促进这一领域的进展。\n\n## 关于作者\n\n[高策](http://gaocegege.com)，[才云科技](https://caicloud.io) AI 平台组工程师，东岳 MOS 组前组长。欢迎关注东岳的 [GitHub](https://github.com/dyweb) 以及[博客](http://blog.dongyueweb.com/) :)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n\n[Kubeflow]: https://github.com/kubeflow/kubeflow\n","cover":"","link":"当我们在谈论机器学习平台时，我们在谈论什么.html","preview":"\u003cp\u003e机器学习平台对于不同的工程师角色而言，有着不同的意涵。随着之前几篇关于 Kubeflow 的文章发布后，有不少的网友私下询问到底什么是机器学习平台，它与机器学习框架有何不同等问题。这篇文章希望能够从不同的维度来介绍一下，我们一直谈论的机器学习平台，到底是什么。\u003c/p\u003e\n","title":"当我们在谈论机器学习平台时，我们在谈论什么"},{"content":"\n\n随着深度学习的兴起，机器学习在最近几年以星火燎原之势席卷了整个科技行业。而在整个机器学习的工作流中，模型训练的代码只是其中的一小部分。除此之外，训练任务的监控，日志的回收，超参数的选择与优化，模型的发布与集成，数据清洗，特征提取等等，都是流程中不可或缺的部分。因此，有一些工具和公司的产品，致力于为机器学习从业者提供一个统一的平台，帮助用户更好地完成其机器学习业务的落地。这篇文章是关于机器学习平台产品的分析对比，由于利益相关性只放出国外的产品，如有遗漏或错误还请指出。\n\n### [RiseML][] - Machine Learning Platform for Kubernetes\n\nRiseML 是一个基于 Kubernetes 的机器学习平台，是一家在德国的创业公司的产品。RiseML 的产品特点在于其简约的 API，以及简易的安装方式。RiseML 支持通过 helm chart 进行安装，相比于 Kubernetes 原生的 API，RiseML 提供了更 high level 的 API 定义，有着更高一级的抽象。\n\n```yaml\nproject: ai-toaster\ntrain:\n  framework: tensorflow\n  resources:\n    cpus: 2\n    mem: 16384\n    gpus: 4\n  tensorflow:\n    version: 1.2.1-gpu\n    distributed:\n      worker: 3\n      ps:\n        count: 2\n        resources:\n          cpus: 2\n          mem: 16384\n          gpus: 0\n  run: \u003e-\n    python train_model.py --num-layers {{ num-layers }}\n                          --learning-rate {{ learning-rate }}\n                          --training-data /data/my-project\n```\n\n以分布式的 TensorFlow 训练任务为例，RiseML 只需要指定参数服务器和 worker 的资源情况以及实例个数即可。这样的设计对于平台的初见者非常友好，但同时也会导致一些比较特化的需求难以得到很好地支持。比如 ML 工程师想控制任务的重启逻辑，以及为容器开放特定的端口，绑定数据卷时，就难以用这一套高级的 API 来完成。易用性与灵活性之间的 trade-off 在机器学习平台系统的设计中也值得考虑。\n\n除此之外，RiseML 支持超参数训练。其 API 同样十分简洁：\n\n```yaml\nproject: ai-toaster\ntrain:\n  resources:\n    cpus: 2\n    mem: 4096\n    gpus: 2\n  parameters:\n    lr:\n      - 0.0001\n      - 0.001\n    lr-decay:\n      - 1e-6\n      - 1e-7\n    epochs:\n      - 20\n      - 30\n  concurrency: 2\n  image:\n    name: nvidia/cuda:8.0-cudnn7-runtime\n  run: \u003e-\n    python train_model.py --num-layers {{num-layers}}\n                          --learning-rate {{learning-rate}}\n                          --training-data /data/ai-toaster\n```\n\n在示例中，RiseML 并没有指定超参数的搜索算法，所以对于其的支持相对而言也并不是十分完善的。RiseML 目前的工作主要在训练方面。其提供了一个 CLI，可以发起训练/超参数训练任务以及查看日志和状态等等。而对于推理服务的支持目前还未可见。\n\n### [FloydHub][] - Deep Learning Platform for Productive Data Science Teams\n\nFloydHub 是同类产品里比较有名的存在了，因为它收到了来自 Y Combinator，真格基金等知名的孵化器和投资机构的投资。FloydHub 有几个核心概念：项目，工作空间，任务，数据集，环境和输出。其中项目是一群目标一样的工作空间和任务的集合。工作空间是为了模型开发而存在的交互式的交互式开发环境，是基于 JupiterHub 实现的。而任务就是一次模型训练代码的运行，在运行的过程中，任务会拉取代码以及数据集，并且配置相应的环境。数据集是用户上传的数据集合，在 FloydHub 中支持版本化的数据集。之所以分离数据和代码，是因为数据不是经常变动的而代码则是会经常因为调参等等原因发生变化的。这样的分离可以节约数据上传和准备的时间，也利于数据的共享。环境是指代码运行的环境，比如需要的机器学习框架的版本，以及是否需要 GPU 等；值得一提的是 FloydHub 也是基于 Docker 的。最后是输出，输出是在一次任务中用户希望保留的日志，文件或者数据等内容。一个典型的用例是保存 checkpoint。因为是基于 Docker 进行的训练，最后训练任务的容器会要被清理的，这时需要通过输出这一概念来持久化对应的内容。\n\nFloydHub 已经实现了较好的流水线支持，以下面的例子来说明：\n\n```yaml\nenv: pytorch-0.4\n\ntask:\n  train:\n    machine: gpu2\n    description: train with lstm\n    input:\n      - source: foo/datasets/wine-reviews/1\n        destination: input\n    command: train.py /floyd/input/input\n\n  test:\n    machine: gpu\n    description: evaluate model1\n    input:\n      - source: foo/projects/nlp/output\n        destination: model\n      - source: foo/datasets/wine-reviews-test/1\n        destination: test\n    command: test.py --model /floyd/input/model --data /floyd/input/test\n\n  serve:\n    machine: cpu\n    mode: serve\n    input:\n      - source: foo/projects/nlp/output\n        destination: model\n```\n\n与 RiseML 类似，FloydHub 也设计了自己的 high level API，同时有流水线的概念。关于 API 的易用性以及灵活性的问题之前已经讨论过，这里不再多说。而流水线的支持应该是所有的机器学习平台都需要支持的功能，FloydHub 相对是要完善一些。\n\n除此之外，FloydHub 有一个简洁而且与 GitHub 类似的 UI，这在易用性上是非常巨大的优势。\n\n\u003cfigure\u003e\n\t\u003cimg src=\"/images/posts/mlp/floydhub.jpg\"\u003e\n\u003c/figure\u003e\n\n## [Comet][] - Supercharge Machine Learning\n\nComet 是与上面的两个产品的做法不太一样的产品，其对用户的训练代码是有侵入性的。举例说明：\n\n```python\nfrom comet_ml import Experiment\nexperiment = Experiment(api_key=\"YOUR_API_KEY\",\n                        project_name=\"my project name\",\n                        auto_param_logging=False)\nbatch_size = 128\nnum_classes = 10\nepochs = 20\n\nparams={\n    \"batch_size\":batch_size,\n    \"epochs\":epochs,\n    \"num_classes\":num_classes}\n\nexperiment.log_multiple_params(params)\n```\n\nComet 目前主要关注在训练和超参数训练这边，它现在主要的使用方式是 Python SDK。我觉得这样有侵入性的使用方式，是不让人喜欢的。其 Python SDK 最大的抽象是 Experiment，对应一次训练。在某些框架下，需要用户将参数告诉 Comet，以便在 Comet 的 Web 端显示对应的参数值。其对超参数训练的支持是引入了一个新的抽象，Optimizer，可以将其看做是一个基于 cloud 的参数搜索的策略。至于参数搜索的策略到底是什么，文档中并未表述。通过这一抽象可以得到下一次尝试的超参数，并且训练模型记录模型的准确性指标。随后在 Comet 的 Web 端可以得到一个不同超参数下准确性的报表。\n\n### [mlflow][] - Open Source Framework for the Complete Machine Learning Lifecycle\n\nmlflow 可以被当做是加强版和开源版的 Comet，它们两个的实现思路是一致的，就是通过 Python API 侵入性地获得一些信息。其核心的概念一共有三个：Project，Tracking，Model。其中 Project 可以当成是类似 Dockerfile 的一种简化的替代产物，其存在的意思是定义了一种项目打包的方式，使得工作可以被完整复现。它支持如下的定义：\n\n```yaml\nname: My Project\n\nconda_env: my_env.yaml\n\nentry_points:\n  main:\n    parameters:\n      data_file: path\n      regularization: {type: float, default: 0.1}\n    command: \"python train.py -r {regularization} {data_file}\"\n  validate:\n    parameters:\n      data_file: path\n    command: \"python validate.py {data_file}\"\n```\n\n其中 conda_env 是配置代码运行环境时的配置文件，entry_points 与 Dockerfile 中的 entrypoint 概念类似。窃以为这种重新造轮子的方式并不可取，而且一定解决地不如 Docker 好。\n\nModel 对应一个训练好的模型。其支持方式与 Project 类似，也存在一个描述文件，如下所示：\n\n```yaml\ntime_created: 2018-05-25T17:28:53.35\n\nflavors:\n  tensorflow:\n    saved_model_dir: estimator\n    signature_def_key: predict\n```\n\n描述文件中最主要的字段是 `flavors`，可以把它当做转换器的概念，通过 flavor，系统才能知道模型的相关信息，以及需要如何去部署它。以 TensorFlow flavor 为例，它共有两个参数，分别是 `saved_model_dir`，也就是 saved model 所在的目录，和 `signature_def_key`，也就是进行预测的函数签名。在 mlflow 的 TFWrapper 中我们可以看到其对应的处理逻辑：\n\n```python\nclass _TFWrapper(object):\n    \"\"\"\n    Wrapper class that creates a predict function such that\n    predict(data: pandas.DataFrame) -\u003e pandas.DataFrame\n    \"\"\"\n    def __init__(self, saved_model_dir):\n        model = Model.load(os.path.join(saved_model_dir, \"MLmodel\"))\n        assert \"tensorflow\" in model.flavors\n        if \"signature_def_key\" not in model.flavors[\"tensorflow\"]:\n            self._signature_def_key = tf.saved_model.signature_constants \\\n                .DEFAULT_SERVING_SIGNATURE_DEF_KEY\n        else:\n            self._signature_def_key = model.flavors[\"tensorflow\"][\"signature_def_key\"]\n            self._saved_model_dir = model.flavors[\"tensorflow\"][\"saved_model_dir\"]\n     def predict(self, df):\n        graph = tf.Graph()\n        with tf.Session(graph=graph) as sess:\n            meta_graph_def = tf.saved_model.loader.load(sess,\n                                                        [tf.saved_model.tag_constants.SERVING],\n                                                        self._saved_model_dir)\n            sig_def = tf.contrib.saved_model.get_signature_def_by_key(meta_graph_def,\n            self._signature_def_key)\n        ...\n```\n\n也就是将对应的参数取出来，然后按照参数的设定去寻找对应的模型目录，然后运行相应的 serving 逻辑。其最大的特色在于定义了一套标准化的方案，使得对模型部署的支持扩展相对比较方便。\n\n最后的概念就是 Tracking，这个与 Comet 的 Experiment 类似，但是支持更多的保存内容，也支持保存到不同的介质中（本地，HTTP server，Databricks Workspace)，但核心概念并无二致，因此不再赘述。\n\n### [Kubeflow][] - ML stack on Kubernetes\n\nKubeflow 是在 2017 年底由 Google 开源，Caicloud，Intel，思科，阿里巴巴等等公司共同参与的一个项目，其旨在简化在 Kubernetes 上运维机器学习工作负载的流程。目前其相比于其他的项目，独特之处在于整个生态都是构建在 Kubernetes 之上的，是完完全全 Kubernetes Native 的。Kubeflow 希望能够实现一套流水线，支持从准备数据到模型发布的一整套机器学习端到端的过程。目前支持较好的是训练过程，其实现可参考 [Kubeflow 安利：在 Kubernetes 上进行机器学习](http://gaocegege.com/Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kubeflow)一文。\n\n其缺点有两个方面，一方面是目前缺乏完整的端到端的系统支持，另一方面是需要用户了解 Kubernetes 相关的知识。针对第一个问题，社区背靠谷歌正在积极地探索完善的解决方案。第二个问题，目前也有一些探索，比如阿里云容器服务组开源的 [arena](https://github.com/AliyunContainerService/arena)，它实现了一个 CLI，屏蔽了底层的 Kubernetes，Kubeflow 等系统的细节，是与 FloydCLI 类似的思路。\n\n### [Seldon][] - Machine Learning Deployment Platform for Enterprise\n\nSeldon 主打的也是基于 Kubernetes 的开源机器学习系统，它在 KubeCon 上有过一个 Talk：[Serving ML Models at Scale with Seldon and Kubeflow](https://www.youtube.com/watch?v=pDlapGtecbY)。它对 Serving 解决地比较好，是通过定义而了一个 CRD，也就是 [SeldonDeployment](https://github.com/SeldonIO/seldon-core/blob/master/docs/reference/seldon-deployment.md) 来实现的，除此之外目前还看不到太多独特之处。值得一提的是，它是用 Java 来实现的，这也是目前我看到的唯一使用 Java 作为主要开发语言的开源机器学习平台系统。\n\n### [SageMaker][] - Build, train, and deploy machine learning models at scale\n\nSageMaker 是由传统云计算厂商 AWS 推出的机器学习平台，其对传统的机器学习有内置的算法支持，是一个非常完整的系统，但是对于 TensorFlow 等等的支持并不是很好，有很多额外的限制。相比于之前提到的各个平台，SageMaker 先把脏活累活做好了（对传统机器学习各种模型的支持），这部分工作是需要人力但标准化较简单的，对于 AWS 这样不缺人的大公司而言是很好的选择。\n\n## 关于作者\n\n[高策](http://gaocegege.com)，上海交通大学软件学院研究生，预计 2019 年毕业，Kubeflow Maintainer。如有问题还请不吝赐教。\n\n欢迎关注我们的 [GitHub](https://github.com/dyweb) 以及[博客](http://blog.dongyueweb.com/) :)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n\n[RiseML]: https://riseml.com/\n[Comet]: https://www.comet.ml/\n[FloydHub]: https://www.floydhub.com/\n[mlflow]: https://databricks.com/mlflow\n[Kubeflow]: https://www.kubeflow.org/\n[Seldon]: https://www.seldon.io/\n[SageMaker]: https://aws.amazon.com/cn/sagemaker/\n","cover":"","link":"机器学习平台漫谈.html","preview":"\u003cp\u003e随着深度学习的兴起，机器学习在最近几年以星火燎原之势席卷了整个科技行业。而在整个机器学习的工作流中，模型训练的代码只是其中的一小部分。除此之外，训练任务的监控，日志的回收，超参数的选择与优化，模型的发布与集成，数据清洗，特征提取等等，都是流程中不可或缺的部分。因此，有一些工具和公司的产品，致力于为机器学习从业者提供一个统一的平台，帮助用户更好地完成其机器学习业务的落地。这篇文章是关于机器学习平台产品的分析对比，由于利益相关性只放出国外的产品，如有遗漏或错误还请指出。\u003c/p\u003e\n","title":"机器学习平台漫谈"},{"content":"\n\n8012 年了，Kubernetes 已经成为了集群调度领域最炙手可热的开源项目之一。而多工作负载支持，是讨论到集群调度时不得不谈的一个话题。CRD 是 Kubernetes 的一个特性，通过它，集群可以支持自定义的资源类型，这是在 Kubernetes 集群上支持多工作负载的方式之一。本文希望讨论在实现一个 Kubernetes CRD Operator 时可能遇到的问题以及解决方案，抛砖引玉，探索实现的最佳实践。文章其余部分如下安排：首先在“导论”中，讨论了多工作负载的意义以及不同架构的调度系统的支持方式。其次在“预热”一节详细介绍了在 Kubernetes 上对多工作负载的不同支持方案，进一步划定本文的讨论范围。最后在“正文”一节介绍实现 CRD Operator 的注意事项。本文主要内容来自笔者在实现 [kubeflow/tf-operator][] 时的经验教训。\n\n## 导论\n\n集群资源的调度问题，是云计算领域一直很火热的研究方向。学术界与工业界目前也有诸多如 Borg, Docker Swarm, Firmament, Fuxi, Hawk, Kubernetes, Mercury, Mesos, Nomad, Omega, Sparrow, Yarn 等等各具特色的调度器进入我们的视线。机器集群在调度器的帮助下混合运行多种工作负载，大幅度提高了整个集群的利用率。不同的工作负载之间相互地削峰填谷，这也是集群化的管理能够提高机器利用率的一大原因。举例说明，在线任务白天利用大部分资源，而晚上业务空闲时让渡资源给离线计算任务。因此多工作负载的支持对于一个生产环境可用的集群管理系统而言，是非常重要的特性。\n\n单体架构的调度器，对于多工作负载的支持并不是特别让人满意。由于只有一个单体的调度器，所有的工作负载都需要被统一地调度。在大规模的场景下会成为系统的瓶颈。因此目前 Kubernetes 也在探索支持多调度器，但目前在解决调度冲突问题时尚不成熟。以 Mesos 为代表的两层式调度器架构，对多工作负载的支持相对较好，但也有受制于中心化的调度器接口限制，难以获取集群全局状态的问题。以 Sparrow 为代表的去中心化架构，是为了短时批处理任务的负载优化的调度器，因此对于长时任务并不友好。以 Hawk 为代表的混合式调度器架构，和以 Nomad 为代表的共享状态的调度器架构，是对多工作负载支持最好的调度器，但是，它们都没有 Kubernetes 火= =\n\n目前的 Kubernetes 在笔者心中，并不是最完美的集群调度系统，但是架构并不能说明一切，它的开发速度，社区活跃程度是其他项目完全不能媲美的，因此讨论在 Kubernetes 上的多工作负载支持是最具有实际意义的。\n\n## 预热\n\nKubernetes 对多工作负载的支持见仁见智，有着各种不同的实现方式。这里举一些比较具有代表性的例子。首先就利用 Kubernetes 自带的资源类型，如 Pod, Service, Job 等，在 Kubernetes 之上构建出满足工作负载的应用。Kubernetes 本身的资源类型足以支撑大多数互联网应用的需求，比如前后端应用等。之前介绍的 [kubeflow/katib][] 也是这样的实现方式。这样的实现优点在于简单方便。这样的方式适合以网站前端后端为代表的应用逻辑。\n\n而基础资源类型的表达能力是有限的，哪怕是经过了内置的 controller 的扩展。因此 Kubernetes 支持 Custom Resource Definition，也就是我们一直提到的 CRD。通过这一特性，用户可以自己定义资源类型，并对其提供支持。这种方式也是我们本次讨论的重点。相比于之前的方式，这样的实现更加原生一点，Kubernetes 会将其视为资源的一种，apiserver 中的各种检查对其也是起作用的。因此 CRD 是可以起到四两拨千斤的作用，与其相关的生命周期的管理是由用户代码进行的，与此同时 Kubernetes 的一些公共特性，比如 kubectl 展示，namespace，权限管理等等都是可以被复用的。以 [kubeflow/tf-operator][]，[coreos/prometheus-operator](https://github.com/coreos/prometheus-operator)，[coreos/etcd-operator](https://github.com/coreos/etcd-operator) 为代表的 operator 都是利用了 CRD 这一特性对 TensorFlow，Prometheus 或 etcd 等不同的系统或框架进行了支持。\n\n第三种实现方式，是 custom API Server。这种方式是复用 Kubernetes 的一些特性的同时，自由度最高的方式。在这种方式中，你可以自定义存储，以及其他在上述方式中不能自定义的内容，同时保有一定程度的公共特性。据我所知，Cloud TiDB 的实现就是通过自定义 API server 进行的。\n\n本文主要聚焦在第二种方式上，大多数的工作负载的需求通过第二种方式即可得到满足。因此这里首先大致介绍下第二种方式的实现思路。让我们从一个 Kubernetes 用户的角度，来看第二种方式的支持。首先，与声明其他资源的方式一样，在创建自定义的资源时，我们需要一个定义的文件，通常是 YAML 格式的。随后，我们的会利用 `kubectl create` 命令来创建出对应的资源，这时，Kubernetes 会负责处理一系列对象创建的工作，随后我们可以利用 `kubectl describe` 命令来获得创建的对象的相关信息。\n\n从一个程序员的角度来看，用户的资源请求到达 Kubernetes API server 后，需要被处理，处理特定资源请求的控制器，一般被称为 controller 或者 operator（关于 controller 和 operator 的异同的讨论可见 [kubeflow/tf-operator#300](https://github.com/kubeflow/tf-operator/issues/300)，以下统称为 operator）。Operator 会监听 API server，当有关于该 CRD 的请求到来时，operator 会通过回调函数处理请求，随后更新资源的状态。\n\n这种方式始于 coreos 的 etcd operator，目前有了非常多的实践，在这篇文章中，主要介绍一下在实现过程中的一些问题，抛砖引玉地讨论 operator 的实现的最佳实践。\n\n## 正文\n\n正文终于开始了，这部分将从 CRD 定义，校验，状态管理，容错问题和架构等方面对 operator 的实现进行介绍。\n\n### CRD 定义\n\nCRD 是一切的开始，因此从这里出发。CRD 的定义并没有太多值得注意的地方，只是有一些惯例需要稍微关注一下。比如 CRD 的 name 通常是 plural 和 group 的结合。另外，一般来说 CRD 的作用域是 namespaced 就可以了。还有 kind 一般采用驼峰命名法等等。这里给出一个例子，不再赘述。\n\n```yaml\napiVersion: apiextensions.k8s.io/v1beta1\nkind: CustomResourceDefinition\nmetadata:\n  name: tfjobs.kubeflow.org\nspec:\n  group: kubeflow.org\n  version: v1alpha2\n  scope: Namespaced\n  names:\n    kind: TFJob\n    singular: tfjob\n    plural: tfjobs\n```\n\n### CRD Validation\n\nCRD validation 是 Kubernetes 1.9 的新特性，这是由一个学生的 Google Summer of Code 项目。通过这一特性，API server 会检查来自用户的输入是否是合法的，如果不合法会挡在 API server 门外，operator 可以不需要处理非法输入。这一特性是通过 OpenAPI 3.0 支持的，但是并不是完整的实现，诸如 `$ref` 和 `additionalProperties` 等等字段，在这一特性中是不支持使用的，可以说在 1.9 中是部分的实现。而仍有一部分函需支持的功能正在实现当中。\n\n由于不支持 `$ref`，因此对于某些使用了 Kubernetes 的结构定义（如 PodTemplate）的 CRD 而言，就相对不是非常友好。因为如果要完全校验输入的合法，使用到的 Kubernetes 资源也需要定义 schema 进行校验，这相当于是重复性的工作。因此这里的建议是利用 [kubernetes/kubernetes#54579](https://github.com/kubernetes/kubernetes/issues/54579#issuecomment-370372942) 提供的方法，将 Kubernetes 的对象的引用进行一次自动 inline，参考实现可见 [gaocegege/crd-validation](https://github.com/gaocegege/crd-validation)。生成的 validation 定义如下所示：\n\n```yaml\nvalidation:\n    openAPIV3Schema:\n        description: TFJob represents the configuration of signal TFJob\n        properties:\n        apiVersion:\n            description: 'APIVersion defines the versioned schema of this representation\n            of an object. Servers should convert recognized schemas to the latest\n            internal value, and may reject unrecognized values. More info:\n            https://git.k8s.io/community/contributors/devel/api-conventions.md#resources'\n            type: string\n            ...\n```\n\n另外，如果 CRD 定义中存在 map 数据结构，比如这样：\n\n```go\ntype TFJobSpec struct {\n\t// TFReplicaSpecs is map of TFReplicaType and TFReplicaSpec\n\t// specifies the TF replicas to run.\n\t// For example,\n\t//   {\n\t//     \"PS\": TFReplicaSpec,\n\t//     \"Worker\": TFReplicaSpec,\n\t//   }\n\tTFReplicaSpecs map[TFReplicaType]*TFReplicaSpec `json:\"tfReplicaSpecs\"`\n}\n```\n\n由于缺乏对 `additionalProperties` 的支持，对于其的校验目前也是无法进行的，具体可见 [kubernetes/kubernetes#59485](https://github.com/kubernetes/kubernetes/issues/59485#issuecomment-380032718)。在 Kubernetes 1.11 之后这一问题会得到解决。\n\n### 状态管理\n\n由于 Kubernetes API Server 背后的存储是 etcd，对于频繁读写的需求并不是特别友好，因此这对 CRD 的状态管理也有一些建议。在实现时，尽量使用状态的 summary 来描述状态。以 Job 为例，Job 本身并不会维护其创建的所有的 pod 的状态，而是以汇总的方式维护一共有多少 running 的 pod，多少 succeeded 的 pod。这样可以尽量避免频繁的写。\n\n除此之外，尽量用 Condition 而非 Phase 来描述状态，这也是 Kubernetes 社区推崇的方式。之前的很多资源都用 Phase 定义了一套状态机，这样并不方便处理，具体细节可以参考 [kubernetes/kubernetes#7856](https://github.com/kubernetes/kubernetes/issues/7856)。\n\n### 容错性\n\n由于目前 Kubernetes 对 CRD validation 的支持还存在一定的局限性，因此在 operator 得到的输入中仍有可能是非法的。而在非法的输入中，存在一些类型错误，比如在应该输入 string 的地方，得到的是一个 integer。这样的错误可以 crash\n整个 operator，使得 operator 的可用性降低。\n\n一个临时的解决方案是使用一个无类型的 informer，代替由 API 生成的 typed informer，随后再进行一次 convert 转为对应类型。具体可见 [kubeflow/tf-operator#561](https://github.com/kubeflow/tf-operator/issues/561#issuecomment-392412816)。\n\n### 架构\n\n因为 operator 本身其实是将对某种类型的应用的运维逻辑固化在了其中，所以很容易就会被实现成有状态的方式。比如在 [kubeflow/tf-operator] 的第一版实现中，会在 operator 的内存中维护一个 cache，来记录当前由 operator 处理的请求的状态，并且基于 cache，在新的请求到来时也会根据 cache 决定如何处理新的请求。这样的实现方式乍一看好像没什么问题，但是当 operator 遇到问题 crash 需要被重新运行时，因为内存中的 cache 是不会被持久化的，因此就会在处理新请求时出现问题。\n\n因此更推荐的方式是利用 informer，完全以事件驱动的方式处理请求。举例说明，如果不是以事件驱动的架构来处理一个新的请求“创建一个新的 TensorFlow 分布式训练任务”，operator 会在内存里维护从训练任务的名字到训练任务对象的映射，在检查过发现映射不存在后，会创建出对应的对象，并且在 Kubernetes 中创建对应的资源。当任务完成时，通过在内存中的映射找到对应的对象，更新它的状态为 completed。\n\n而如果是事件驱动的架构，在受到请求后，发现 Kubernetes 上并没有对应的资源，这时会创建出来，然后结束逻辑。随后对应的资源被创建时，会产生新的创建事件，operator 再基于新的事件，更新训练任务的状态。在训练完成后，从 Kubernetes 中获取资源并且更新它的状态。不知是否解释地够清楚，总结来说就是保证 operator 是无状态的，所有的状态都应该依赖 API Server。\n\n## 结语\n\n本篇文章介绍了在实现 Kubernetes CRD operator 时可能遇到的一些问题以及对应的解决方案，希望能够对各位有所帮助。\n\n## 关于作者\n\n[高策](http://gaocegege.com)，上海交通大学软件学院研究生，预计 2019 年毕业，Kubeflow Maintainer。如有问题还请不吝赐教。\n\n欢迎关注我们的 [GitHub](https://github.com/dyweb) 以及[博客](http://blog.dongyueweb.com/) :)\n\n## 参考资料\n\n- [The evolution of cluster scheduler architectures](http://firmament.io/blog/scheduler-architectures.html)\n- [Container Camp AU 2017 - Building a Kubernetes Operators](https://github.com/lukebond/cc-au-k8s-operators-workshop)\n- 唐瑞. 基于Kubernetes的容器云平台资源调度策略研究[D].电子科技大学,2017.\n- [kubeflow/tf-operator][]\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n","cover":"","link":"kubernetes_crd_operator_实现指南.html","preview":"\u003cp\u003e8012 年了，Kubernetes 已经成为了集群调度领域最炙手可热的开源项目之一。而多工作负载支持，是讨论到集群调度时不得不谈的一个话题。CRD 是 Kubernetes 的一个特性，通过它，集群可以支持自定义的资源类型，这是在 Kubernetes 集群上支持多工作负载的方式之一。本文希望讨论在实现一个 Kubernetes CRD Operator 时可能遇到的问题以及解决方案，抛砖引玉，探索实现的最佳实践。文章其余部分如下安排：首先在“导论”中，讨论了多工作负载的意义以及不同架构的调度系统的支持方式。其次在“预热”一节详细介绍了在 Kubernetes 上对多工作负载的不同支持方案，进一步划定本文的讨论范围。最后在“正文”一节介绍实现 CRD Operator 的注意事项。本文主要内容来自笔者在实现 kubeflow/tf-operator 时的经验教训。\u003c/p\u003e\n","title":"Kubernetes CRD Operator 实现指南"},{"content":"\n\n这篇文章的受众是想更加深入了解 Google Summer of Code 这一活动，或者有志于担任某一开源社区 mentor 的同学。由于有些背景知识没有介绍，因此配合 [Google Summer of Code 学生申请指南](https://zhuanlan.zhihu.com/p/27823910)阅读更佳。\n\n## 背景介绍\n\n[Google Summer of Code](https://developers.google.com/open-source/gsoc/)（下称作 GSoC）是谷歌组织并提供经费，面对全球（绝大多数国家）在读学生的在线编程项目。它的[官方介绍](http://write.flossmanuals.net/gsocstudentguide/what-is-google-summer-of-code/)是：\n\n\u003e Google Summer of Code (GSoC) is a global program that matches students up with open source, free software and technology-related organizations to write code and get paid to do it! The organizations provide mentors who act as guides through the entire process, from learning about the community to contributing code. The idea is to get students involved in and familiar with the open source community and help them to put their summer break to good use.\n\n即是：\n\n\u003e Google 编程之夏是一个全球性项目，旨在为学生们和开源、自由软件、技术相关的组织建立联系，让学生们贡献代码并获得报酬！组织会提供导师，在学生从熟悉社区到贡献代码的整个过程中提供指导。这个想法的目的是让学生们参与和熟悉开源社区，并帮助他们充分利用暑假时间去得到锻炼。\n\n具体可见 [Google Summer of Code 学生申请指南](https://zhuanlan.zhihu.com/p/27823910)，这里不再赘述了。这篇文章主要是从一个 mentor 的角度，来谈一谈对 Google Summer of Code 这个活动的理解。\n\n## GSoC 的流程 Extended\n\n在上一篇文章中，我介绍了一下从学生的角度看到的 Google Summer of Code 的流程，不过在 mentor 的角度来看，又有一些不同的地方。这里从头开始，重新介绍一下整个 GSoc 的 Timeline。\n\n一切的一切始于 organization 的申请。在 GSoC 中，不只是需要学生来申请 organization 的 slots，organization 也是要先向谷歌申请参加当年 GSoC 的。GSoC 的申请向来是一个黑盒的过程，我们看不到谷歌内部做了怎样的考量，会允许哪些组织参与今年的 GSoC。但是一般来说，老牌的组织很少有申请不中的情况，所以如果想尽早关注 GSoC 的话，建议还是从每年的 slots 数量比较多的组织下手，这些组织已经参加过多次 GSoC，再次入选的可能相对比较大。不过值得一提的是，像 The Processing Foundation 这样的老牌组织，在 2016 年的时候就被拒绝了。所以这只能保证概率，而不是绝对。\n\n一般对于 organization 申请的时间窗口大概是在 20 天左右，谷歌在申请结束后花 20 天左右的时间来确定入选的组织，因此这整个过程大概要花 40 天。接下来就是我们熟知的，学生申请的过程。在这个过程里学生会根据每个组织列出的 idea 来提交自己的 proposal，这个的时间窗口在 15 天左右。\n\n接下来 10 天的时间，是社区的 mentor 和 admin 们来 review 学生的 proposal 的阶段。在这一阶段，组织的 admin 需要在看过所有的 proposal 后，根据申请的质量来确定今年向谷歌申请的 slots 数量。这个数量是以区间的形式提交的，即提交一个最小值与一个最大值。谷歌不能保证给到你最小值的 slots 但可以保证最多不给你超过最大值的 slots。另外值得注意的是，一个新的 org，通常来讲只会有 1-2 个 slots。因为谷歌认为这些 org 可能还不太了解 GSoC 对组织的负担其实还是蛮大的。因此如果是申请的新 org 时，要格外注意这一点。\n\n接下来谷歌会花 1 天的时间来确定 org 真正可以拿到多少个 slots，这也是黑盒的操作，org 中的任何一个人在谷歌公布之前，都不能确定今年一共几个 slots。\n\n在确定了 slots 的数量后，org admin 会来分配 slots 给学生。这个过程大概持续 10 天的时间。在这个期间 mentor 起不到决定性的作用，因为 slots 的名额分配是 org admin 来做的。而 mentor 起到的作用是对其 mentor 的 idea 的 proposal 打分，给出参考性的意见。\n\n在确定了 slots 分配后，org admin 和 mentor 就会知道哪些学生可以参加今年的 GSoC，但是真正面向学生的公布会在 5-6 天后由谷歌在 GSoC 的网站上进行。在这段时间里谷歌会复查所有被选中的学生的 eligibility。\n\n随后在项目公布后，便会进入为期 20 天的 Community Bonding Process。在此期间，mentor 需要与学生建立联系，相互熟悉为后面的合作打好基础。除此之外学生也应积极地参与社区，如果 mentor 认为学生在这段时间并没有很好地进行交互，是可以而且被建议直接 fail 掉学生的。因为这段时间是 GSoC 项目的前期准备时间，如果这段时间里准备不是很充分，后面真正开始写代码的时候或多或少会有风险。\n\n在度过了 Community Bonding Process 后，就进入真正的 coding 环节。Coding 环节一共有三个 evaluation，将整个编程环节分成了三个 phase，每个 phase 大概有一个月的时间，所以整个编程的时间大概有三个月（五月到八月）。\n\n## Mentor 职责\n\nMentor 在 organization 开始申请之前，需要先写出自己想 mentor 的 idea 的 description，来说清楚 idea 的目标是什么，需要什么技术栈的同学来做，以及难度如何等等，例如 [coala Language Server](https://projects.coala.io/#/projects?project=coala_language_server\u0026lang=en) 这样。期间可能需要跟 org admin 交流来确定 idea 的工作量和重要程度等等。\n\n在开放学生申请后，Mentor 需要接受来自各位对 idea 的学生的询问和套瓷，解答他们的疑惑，review 他们的 proposal 草稿并且给出你的意见。这是一个非常艰难的阶段，一方面有一些申请者由于对社区和 idea 都不熟悉，会有很多在文档里都有记录的问题来问。另一方面，不同的学生对 idea 的解读不同，导致会花很多时间在 sync mentor 和学生之间的理解上。\n\n在学生申请 deadline 后，mentor 需要 review 所有的 proposal，随后给出自己的选择倾向。如果遇到了两个 candidate 都很强的情况，可能还要组织一次 video interview 或者类似的考核方式。不过前面也提到了，最终的选择是 org admin 进行的\u003cdel\u003e，当然mentor 的决定权也是很重要的\u003c/del\u003e。\n\n在 Community Bonding Process 中，mentor 需要和学生取得联系，并且与 org admin 沟通确定学生在这个期间需要做的事情，这是因社区而异的。同时，最好在这个时候确定下来与学生的常规交流时间，以及管理与交流工具的选型。在随后的 coding phase 中，mentor 主要起到的是进度管理，和解决学生对 code base 的疑惑等等这些问题。\n\n## 申请相关的建议\n\n最后谈一谈申请相关的事情吧。就我看过的申请情况来说，大体上申请人基本可以分为这么几类：\n\n首先就是在申请的社区里贡献了很久（甚至有的已经成为了 maintainer），与此同时有一些与申请的 idea 强相关的贡献的一类候选人。这种候选人基本来说如果 idea 不是特别边缘，都是板上钉钉的样子，这个就不多讨论了。\n\n其次是在申请的社区里贡献了很久，但是与申请的 idea 并没有什么交集的一类。这一类候选人，是稍微有点吃亏的。虽然在社区里做了一段时间，但是因为没有 PR 或者经历来证明其对申请的 idea 的了解。\n\n还有一类是虽然在社区里声明不显，但是在申请的 idea 有关的技术领域有一定的积累的候选人。这种就有点神经刀的感觉，看上去并无太大出众之处，但搞不好就可以交出一份方案详实，安排合理的 proposal。\n\n上面提到的三类申请者是比较可能申请到 GSoC 的候选人，而一个社区参与不多，对申请的 idea 上也无太多了解的同学就相对难度较大了，因为 GSoC 很少接受只是 ok 程度的 proposal。\n\n谈一下对第二类和第三类候选人的建议。如果你是第二类的候选人，在社区里属于活跃的贡献者，那你应该有近水楼台先得月的优势。所以我觉得需要做的是尽早跟感兴趣的 idea 的 mentor 联系，学习 idea 相关的知识，做出一些针对性的贡献。要是跟第三类候选人在同等条件下，肯定会倾向于选在社区已经有过贡献的第二类啦。\n\n对于第三类候选人，因为社区贡献不是一个可以短期速成的东西，因此建议可以多跟 mentor 沟通，在少量贡献的同时完成一份更高质量的 proposal，可能是更有竞争力的一种选择。\n\n## 结语\n\n这又是一篇摸鱼作，希望能够对各位有所帮助。如果你在申请 GSoC 的过程中有什么疑惑，欢迎加入 GSoC CN 的在线聊天频道 [Gsoc-cn/Lobby](https://gitter.im/Gsoc-cn/Lobby)。GSoC CN 是一个由国内的参加过 GSoC 的同学们创建的社区，我们维护有一些[社区的介绍](https://github.com/gsoc-cn/gsoc-cn/tree/master/resources/organizations)和往年的 proposal，可能会对你有些帮助。\n\n欢迎关注我们的 [GitHub](https://github.com/dyweb) 以及[博客](http://blog.dongyueweb.com/) :)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n","cover":"","link":"google_summer_of_code,_a_mentor's_perspective.html","preview":"\u003cp\u003e这篇文章的受众是想更加深入了解 Google Summer of Code 这一活动，或者有志于担任某一开源社区 mentor 的同学。由于有些背景知识没有介绍，因此配合 Google Summer of Code 学生申请指南阅读更佳。\u003c/p\u003e\n","title":"Google Summer of Code, A Mentor's Perspective"},{"content":"\n\n这篇文章主要介绍了 [Katib][katib]，一个由 NTT 贡献到 [Kubeflow][] 社区的超参数训练系统。面向人群为对在 Kubernetes 上运行机器学习负载感兴趣的同学。\n\n## 问题背景\n\n在[上一篇文章](http://gaocegege.com/Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kubeflow)中，我主要介绍了 Kubeflow 社区中关于 TensorFlow 模型训练的支持工作，也就是 [kubeflow/tf-operator][]。它使得用户能够在 Kubernetes 中运行 TensorFlow 的单机或者分布式训练任务，而它无法解决用户对于超参数训练的需求，于是 [Katib][] 应运而生。\n\n如果想了解 Katib，首先需要知道什么是超参数训练。[@tobegit3hub](https://github.com/tobegit3hub) 的[贝叶斯优化: 一种更好的超参数调优方式](https://zhuanlan.zhihu.com/p/29779000)一文对于其有一个比较好的介绍，这里引用一下：\n\n\u003e 首先，什么是超参数（Hyper-parameter）？这是相对于模型的参数而言（Parameter），我们知道机器学习其实就是机器通过某种算法学习数据的计算过程，通过学习得到的模型本质上是一些列数字，如树模型每个节点上判断属于左右子树的一个数，或者逻辑回归模型里的一维数组，这些都称为模型的参数。\n\u003e\n\u003e 那么定义模型属性或者定义训练过程的参数，我们称为超参数，例如我们定义一个神经网络模型有9527层网络并且都用RELU作为激活函数，这个9527层和RELU激活函数就是一组超参数，又例如我们定义这个模型使用RMSProp优化算法和learning rate为0.01，那么这两个控制训练过程的属性也是超参数。\n\u003e\n\u003e 显然，超参数的选择对模型最终的效果有极大的影响。如复杂的模型可能有更好的表达能力来处理不同类别的数据，但也可能因为层数太多导致梯度消失无法训练，又如learning rate过大可能导致收敛效果差，过小又可能收敛速度过慢。\n\u003e\n\u003e 那么如何选择合适的超参数呢，不同模型会有不同的最优超参数组合，找到这组最优超参数大家会根据经验、或者随机的方法来尝试，这也是为什么现在的深度学习工程师也被戏称为“调参工程师”。根据No Free Lunch原理，不存在一组完美的超参数适合所有模型，那么调参看起来是一个工程问题，有可能用数学或者机器学习模型来解决模型本身超参数的选择问题吗？答案显然是有的，而且通过一些数学证明，我们使用算法“很有可能”取得比常用方法更好的效果，为什么是“很有可能”，因为这里没有绝对只有概率分布，也就是后面会介绍到的贝叶斯优化。\n\n也就是说，机器学习的模型本身有一些参数，这些参数的选择通过训练的方式获得一个最优解或者次优解的过程，就是超参数训练的过程。这个过程是一个黑盒的优化过程，我们对模型本身一无所知，也不能做任何假设。而且训练模型往往是一个比较昂贵的任务，要么会占用相当长时间的 CPU，要么会占用 GPU，所以我们并没有很多的尝试机会。因此我们要做的就是在有限的尝试次数中，尽可能确定一组输入，能得到尽可能好的输出。\n\n## 相关工作\n\nGoogle Vizier 是 Google 内部的机器学习超参数训练系统的一个子系统，有一篇论文详细介绍了其抽象以及架构设计：[Google Vizier: A Service for Black-Box Optimization](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46180.pdf)。Google Vizier 对超参数训练这一需求进行了比较好的领域抽象，并且支持多个参数搜索的算法。\n\n[@tobegit3hub](https://github.com/tobegit3hub) 基于此实现了开源的实现 [advisor](https://github.com/tobegit3hub/advisor)，其采取了 clinet 与 server 的架构，并且提供了基础的 UI 界面。并且支持多个机器学习框架（如 TensorFlow, scikitlearn 等）。[Cisco AI](https://github.com/CiscoAI) 也基于 Google Vizier 实现了自己的超参数训练工具 [hyper-advisor-client](https://github.com/CiscoAI/hyper-advisor-client)。\n\n## Katib 介绍\n\nKatib 也是对 Google Vizier 的开源实现，因此也遵循其中对问题的抽象模型：Study，Trial 和 Suggestion。\n\nTrial 代表着一个由超参数的取值组成的列表，每个 Trial 都需要一次运行来得到这些超参数取值对应的结果。这里提到的运行就是一次训练的过程。Study 代表着在可行空间上运行的单个优化。每个 Study 都有一个配置，来描述可能取值的空间，超参数推荐算法等。此外，Study 包含一组 Trial，代表着算法在超参数集合中选取推荐值的多次尝试。如图所示，是创建一次 Study 并且进行 Trial 的验证的过程。\n\n![](/images/posts/katib/katib.png)\n\n首先，用户会通过 katib-cli 创建一个新的 Study，这需要用户提供一份配置，下面是一个样例配置。目前配置中主要有两类参数，一类是 Study 本身需要的参数，比如使用的 Suggestion 算法，在此例中就是 random。另一类是运行时需要的一些参数，比如其中的 scheduler，就是在使用 Kubernetes 运行 Trial 时的一个参数，它决定这一 Trial 会被哪个调度器调度。\n\n```yaml\nname: cifer10\nowner: root\noptimizationtype: 2\nsuggestalgorithm: random\nautostopalgorithm: median\nobjectivevaluename: Validation-accuracy\nscheduler: default-scheduler\nimage: mxnet/python\nsuggestionparameters:\n    -\n      name: SuggestionNum\n      value: 2\n    -\n      name: MaxParallel\n      value: 2\ncommand:\n        - python\n        - /mxnet/example/image-classification/train_cifar10.py\n        - --batch-size=512\nmetrics:\n    - accuracy\nparameterconfigs:\n    configs:\n      -\n        name: --lr\n        parametertype: 1\n        feasible:\n            min: 0.03\n            max: 0.07\n      -\n        name: --lr-factor\n        parametertype: 1\n        feasible:\n            min: 0.05\n            max: 0.2\n      -\n        name: --max-random-h\n        parametertype: 2\n        feasible:\n            min: 26\n            max: 46\n      -\n        name: --max-random-l\n        parametertype: 2\n        feasible:\n            min: 25\n            max: 75\n      -\n        name: --num-epochs\n        parametertype: 2\n        feasible:\n            min: 3\n            max: 3\n```\n\n随后，katib-manager 收到了来自 katib-cli 的创建 Study 请求后，会将 Study 的配置写入 SQL 数据库（目前为 MySQL），然后访问相应的 suggestion service 来初始化服务并且设置一些参数。接下来 katib-manager 会去对应的 worker 中检查已经运行的 Trial，以及成功了的 Trail，并且以此作为参数，通过 suggestion 服务来得到需要被运行 Trial，继而交给 worker 去运行。\n\n在整个的工作流中，涉及到 katib-cli，katib-manager，suggestion 和 worker 等多个不同的组件。其中 katib-cli 是一个 CLI 工具，通过它，用户可以与 katib-manager 交互。katib-manager 之于 katib 就相当于 kube-apiserver 之于 Kubernetes，它是 katib 中最核心的一个组件。它会负责与其他各个组件交互，并且将结果写入数据库。suggestion 则是一个逻辑上的概念。它包含着很多不同的算法，而每一个算法都是以一个容器的方式独立运行，但所有的算法都会暴露同一端口，使用 `suggestion` 同一前缀，以便于服务发现。worker 与此类似，也是一个逻辑上的概念。Katib 在架构上支持不同的 worker 以适配不同的环境。\n\nkatib-cli，katib-manager 以及不同的 suggestion 算法服务，都是运行在不同的容器中的。所有调用，都是通过 GRPC 的方式进行。这使得 Katib 具有极好的扩展性。在添加新的算法支持时，Katib 可以不停机地进行。而且 worker 的接口使得 Katib 也可以轻易地接入新的云环境。\n\n除此之外，Katib 支持的 Kubernetes worker 可以利用 Kubernetes 的集群能力将 Trail 的运行分布在不同的机器上，使得其真正地具有大规模超参数训练的可能，这也是目前其他同类型的系统支持地并不好的一点。\n\nKatib 使用 modeldb 存储模型，Trail 与模型一一对应。Katib 将所有从 Trail 训练得到的模型存储在 modeldb，以方便管理和查看不同超参数训练出来的模型效果。这里有一个 demo，可以一观。\n\n![](/images/posts/katib/katib.gif)\n\n## 未来的工作与计划\n\n在这篇文章中，我们介绍了 Katib 这一基于 Kubernetes 的超参数训练系统，并且阐述了其与其他系统相比的优劣。目前我们正在紧锣密鼓地开发新的特性与功能，这其中包括但不限于：\n\n**Early Stop 支持** 这一特性能够节省集群的资源，提高参数寻找的效率。目前这一工作由 [@YujiOshima](https://github.com/YujiOshima) 进行。\n\n**更多 Suggestion 算法支持** 目前我们只支持三种较为简单的算法，分别是随机搜索，网格搜索和 Hyperband。因此我们正在着手实现贝叶斯优化等等算法。这一工作由 [@libbyandhelen](https://github.com/libbyandhelen) 进行。\n\n**与 Kubeflow 的集成** Katib 支持原生的 Kubernetes，对于运行 Trial 的容器的配置较为复杂。我们希望能够利用社区现有的 [tf-operator][kubeflow/tf-operator] 和 [pytorch-operator][kubeflow/pytorch-operator] 来提供对不同机器学习框架的支持。这一工作由 [@gaocegege](https://github.com/gaocegege) 进行。\n\n**支持 Neural Architecture Search (NAS)** NAS 的支持是社区呼声比较高的需求，我们希望能够基于 Katib 的架构实现 NAS。目前这一工作由 [@YujiOshima](https://github.com/YujiOshima) 进行。\n\n**以 CRD 的方式支持 Study 资源** 目前 Katib 维护了自己的 CLI，以及配置文件格式。我们认为这可以被 Kubernetes CRD 替代。我们目前正在探索可否实现 katib controller 来替代 katib-cli 与 katib-manager。如此一来 Katib 的维护会更加简单。\n\n与此同时我们也欢迎更多对超参数训练，以及神经网络模型搜索的同学为 [Katib][] 贡献 :-) 欢迎加入我们的 [Slack Channel](https://kubeflow.slack.com/messages/C9ZLKR73L/)，在这之前需要先通过[这一邀请链接注册](https://join.slack.com/t/kubeflow/shared_invite/enQtMjgyMzMxNDgyMTQ5LWUwMTIxNmZlZTk2NGU0MmFiNDE4YWJiMzFiOGNkZGZjZmRlNTExNmUwMmQ2NzMwYzk5YzQxOWQyODBlZGY2OTg)。\n\n## 关于作者\n\n[@gaocegege](https://github.com/gaocegege)，上海交通大学软件学院研究生在读，Kubeflow core approver\n\n欢迎关注我们的 [GitHub](https://github.com/dyweb) 以及[博客](http://blog.dongyueweb.com/) :)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n\n[katib]: https://github.com/kubeflow/hp-tuning\n[Kubeflow]: https://github.com/kubeflow/kubeflow\n[kubeflow/tf-operator]: https://github.com/kubeflow/tf-operator\n[kubeflow/pytorch-operator]: https://github.com/kubeflow/pytorch-operator\n","cover":"","link":"katib:_kubernetes_native_的超参数训练系统.html","preview":"\u003cp\u003e这篇文章主要介绍了 Katib，一个由 NTT 贡献到 Kubeflow 社区的超参数训练系统。面向人群为对在 Kubernetes 上运行机器学习负载感兴趣的同学。\u003c/p\u003e\n","title":"Katib: Kubernetes Native 的超参数训练系统"},{"content":"\n\n这篇文章主要介绍了 [Kubeflow][] 的使用，以及未来的计划，面向人群为对在 Kubernetes 上运行机器学习负载感兴趣的同学。\n\n## 问题背景\n\nKubernetes 本来是一个用来管理无状态应用的容器平台，但是在近两年，有越来越多的公司用它来运行各种各样的工作负载，尤其是机器学习\u003cdel\u003e炼丹\u003c/del\u003e。各种 AI 公司或者互联网公司的 AI 部门都会尝试在 Kubernetes 上运行 TensorFlow，Caffe，MXNet 等等分布式学习的任务，这为 Kubernetes 带来了新的挑战。\n\n首先，分布式的机器学习任务一般会涉及参数服务器（以下称为 PS）和工作节点（以下成为 worker）两种不同的工作类型。而且不同领域的学习任务对 PS 和 worker 有不同的需求，这体现在 Kubernetes 中就是配置难的问题。以 TensorFlow 为例，[TensorFlow 的分布式学习任务](https://www.tensorflow.org/deploy/distributed)通常会启动多个 PS 和多个 worker，而且在 TensorFlow 提供的最佳实践中，每个 worker 和 PS 要求传入不同的命令行参数。举例说明：\n\n```bash\n# On ps0.example.com:\n$ python trainer.py \\\n     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \\\n     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \\\n     --job_name=ps --task_index=0\n# On ps1.example.com:\n$ python trainer.py \\\n     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \\\n     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \\\n     --job_name=ps --task_index=1\n# On worker0.example.com:\n$ python trainer.py \\\n     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \\\n     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \\\n     --job_name=worker --task_index=0\n# On worker1.example.com:\n$ python trainer.py \\\n     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \\\n     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \\\n     --job_name=worker --task_index=1\n```\n\n其中需要的参数有四个，一个是所有的 PS 的网络地址（主机名-端口），以及所有的 worker 的网络地址。另外是 job 的类型，分为 PS 与 worker 两种。最后是任务的 index，从 0 开始递增。因此在此例中，用户需要写至少四个 pod 的配置文件，以及四个 service 的配置文件，使得 PS 跟 worker 可以互相访问，况且这只是一个机器学习任务。如果大规模地在 Kubernetes 上运行 TensorFlow 分布式任务，可以预见繁杂的配置将成为机器学习工程师们新的负担。\n\n其次，Kubernetes 默认的调度器对于机器学习任务的调度并不友好。如果说之前的问题只是在应用与部署阶段比较麻烦，那调度引发的资源利用率低，或者机器学习任务效率下降的问题，就格外值得关注。机器学习任务对于计算和网络的要求相对较高，一般而言所有的 worker 都会使用 GPU 进行训练，而且为了能够得到一个较好的网络支持，尽可能地同一个机器学习任务的 PS 和 worker 放在同一台机器或者网络较好的相邻机器上会降低训练所需的时间。\n\n## Hello, Kubeflow\n\n针对这些问题，[Kubeflow][] 项目应运而生，它以 TensorFlow 作为第一个支持的框架，在 Kubernetes 上定义了一个新的资源类型：TFJob，即 TensorFlow Job 的缩写。通过这样一个资源类型，使用 TensorFlow 进行机器学习训练的工程师们不再需要编写繁杂的配置，只需要按照他们对业务的理解，确定 PS 与 worker 的个数以及数据与日志的输入输出，就可以进行一次训练任务。在本节中，我们将从零开始搭建一个 Kubernetes 集群，并且将 Kubeflow 运行在其上，最后利用其进行一次完整的学习任务运行。\n\n首先，我们需要有一个正在运行的 Kubernetes 集群，而且集群的版本要大于等于 1.8。在这一步里，个人推荐以下两种方式创建一个单节点的本地 Kubernetes 集群：\n\n- [使用 Kubernetes 里的 local-up-cluster.sh 脚本](https://github.com/kubernetes/kubernetes/blob/master/hack/local-up-cluster.sh)\n- [使用 minikube 项目](https://github.com/kubernetes/minikube)\n\n其中前者会在本地创建一个 native 的 Kubernetes 集群，而后者则会在本地的虚拟机里创建出 Kubernetes 集群。因为本文侧重点不在此，因此整个过程不再赘述。\n\n如果你已经成功地创建了一个 Kubernetes 集群，那么接下来就是在这一集群上创建 Kubeflow 所有的组件，这一步需要用到 [ksonnet][]，一个简化应用在 Kubernetes 上的分发与部署的命令行工具，它会帮助你创建 Kubeflow 所需组件。在安装了 [ksonnet][] 后，接下来就是一片坦途了，只需要运行下面的命令，就可以完成 Kubeflow 的部署。\n\n```bash\n# Initialize a ksonnet APP\nAPP_NAME=my-kubeflow\nks init ${APP_NAME}\ncd ${APP_NAME}\n\n# Install Kubeflow components\nks registry add kubeflow github.com/kubeflow/kubeflow/tree/master/kubeflow\nks pkg install kubeflow/core\nks pkg install kubeflow/tf-serving\nks pkg install kubeflow/tf-job\n\n# Deploy Kubeflow\nNAMESPACE=default\nkubectl create namespace ${NAMESPACE}\nks generate core kubeflow-core --name=kubeflow-core --namespace=${NAMESPACE}\nks apply default -c kubeflow-core\n```\n\nKubeflow 的部署会附带一个 JupyterHub 但笔者并不知道如何使用它，因此下面的操作是用 Docker 打包训练数据和代码，用 kubectl 在 Kubernetes 上启动一次训练任务的。\n\n示例代码可见 [tf_smoke.py](https://github.com/tensorflow/k8s/blob/master/examples/tf_sample/tf_sample/tf_smoke.py)，与正常的训练代码类似，只不过 clusterspec 的传递方式是遵循了 [Cloud ML](https://cloud.google.com/ml-engine/docs/trainer-considerations#use_tf_config) 的 TF_CONFIG 的方式。Kubeflow 已经根据这一训练文件打好了一个 Docker 镜像：`gcr.io/tf-on-k8s-dogfood/tf_sample:dc944ff`，在这里直接使用就好：\n\n```bash\nkubectl create -f https://raw.githubusercontent.com/tensorflow/k8s/master/examples/tf_job.yaml\n```\n\n\n\n## Kubeflow 实现介绍\n\n本部分主要涉及对 Kubeflow 内部实现的介绍和未来可能的开发计划，如果不感兴趣可以就此打住 :)\n\n### 对分布式训练任务的支持\n\n为了解决配置困难的问题，Kubeflow 以 TensorFlow 作为第一个支持的框架，为其实现了一个在 Kubernetes 上的 [operator](https://coreos.com/operators/)：[tensorflow/k8s][]。由于在 Kubernetes 上内置的资源类型，如 deployment，replicaset，或者是 pod 等，都很难能够简练而清晰地描述一个分布式机器学习的任务，因此我们利用 Kubernetes 的 [Custom Resource Definition](https://kubernetes.io/docs/concepts/api-extension/custom-resources/) 特性，定义了一个新的资源类型：TFJob，即 TensorFlow Job 的缩写。一个 TFJob 配置示例如下所示：\n\n```yaml\napiVersion: \"kubeflow.org/v1alpha1\"\nkind: \"TFJob\"\nmetadata:\n  name: \"example-job\"\nspec:\n  replicaSpecs:\n    - replicas: 1\n      tfReplicaType: MASTER\n      template:\n        spec:\n          containers:\n            - image: gcr.io/tf-on-k8s-dogfood/tf_sample:dc944ff\n              name: tensorflow\n          restartPolicy: OnFailure\n    - replicas: 1\n      tfReplicaType: WORKER\n      template:\n        spec:\n          containers:\n            - image: gcr.io/tf-on-k8s-dogfood/tf_sample:dc944ff\n              name: tensorflow\n          restartPolicy: OnFailure\n    - replicas: 2\n      tfReplicaType: PS\n      template:\n        spec:\n          containers:\n            - image: gcr.io/tf-on-k8s-dogfood/tf_sample:dc944ff\n              name: tensorflow\n          restartPolicy: OnFailure\n```\n\n其中每个字段就不多介绍了，这里主要是说一下实现。任何一个 PS 或者 worker，都由两个资源组成，分别是 job 和 service。其中 job 负责创建出 PS 或者 worker 的 pod，而 service 负责将其暴露出来。这里社区目前也在重新考虑选型，目前希望可以直接创建 pod 而非 job，而用 headless service 替代 service，因为 PS worker 不需要暴露给除了该分布式学习任务外的其他服务。\n\nTFJob operator 的实现早期是从 [etcd-operator](https://github.com/coreos/etcd-operator/) 复制来的，因此整体的架构在最初是完全仿照其改写而成。在最初的实现中，当有一个 TFJob 被创建时，在 operator 内都会有一个新的 goroutine，以轮询的方式获取 TFJob 的状态，然后基于此状态做出相应的操作，相当于是在 operator 内部维护了一个状态机。这样的方式会有一些缺点：\n\n- 这样的架构使得 operator 是有状态的，使得状态很难横向扩展\n- 维护基于 Phase 的状态机是 Kubernetes 社区不推崇的一种方式\n\n基于这些问题，operator 的架构正在往事件驱动重构，这部分工作由 [@caicloud][] 在推进。重构之后，operator 会在 Kubernetes 的一些资源上注册 informer 的事件回调，比较现在的状态与理想状态的不同而采取相应的操作。比如当有一个新的 TFJob 被创建时，理想状态是所有对应的 PS， worker 都被创建好，而当下的状态则是没有任何 pod 和 service 被创建，此时 operator 会创建出对应的 PS，worker 的 pod 和 service，以达到理想状态，这也是 Kubernetes 社区对于 operator/controller 的最佳实践。\n\n### 对分布式学习任务效率的关注\n\n目前社区还停留在如何对 AI 工程师更友好，更好地维护上面提到的 operator 这一步，在效率方面考虑地较少。目前有利用 [kube-arbitrator][] 来进行 gang scheduling 的探索，目前还没有尝试过因此不好评价。但是整体来说 Kubeflow 的性能提高还有很大的空间。\n\n因为机器学习任务根据模型的不同，其输入数据的规模，特征，模型的大小等等都有很大不同。比如 CV 领域与推荐领域的学习模型就有完全不同的特点，因此 TensorFlow 的分布式模型提供了极强的灵活性。而对于 Kubernetes 而言，如何能够在保持灵活性的基础上，同时也保证任务在较高的性能下运行，同时集群的利用率也相对较高，是一个值得研究的问题。\n\n### 对其他机器学习框架的支持\n\n目前 Kubeflow 主要关注 TensorFlow，而其他机器学习框架的支持将于之后展开，目前有一些第三方实现的 operator，比如 [MXNet operator](https://github.com/deepinsight/mxnet-operator)，但是质量难以保证。\n\n### 开发情况与未来展望\n\n目前 Kubeflow 有来自 Google，Caicloud，RedHat 等公司的积极参与，短期的目标有这么几个：\n\n- operator 方面\n    - 使用 pod 替换 job [tensorflow/k8s#325](https://github.com/tensorflow/k8s/issues/325)\n    - 使用 headless service 替换 service [tensorflow/k8s#40](https://github.com/tensorflow/k8s/issues/40)\n    - 由 etcd operator 主动轮询的方式改为事件驱动 [tensorflow/k8s#314](https://github.com/tensorflow/k8s/issues/314)\n    - 分离对 TensorBoard 的支持 [tensorflow/k8s#347](https://github.com/tensorflow/k8s/issues/347)\n    - 支持细粒度的任务状态 [tensorflow/k8s#333](https://github.com/tensorflow/k8s/issues/333)\n- 模型服务方面\n    - GPU 支持 [kubeflow/kubeflow#64](https://github.com/kubeflow/kubeflow/issues/64)\n    - 监控支持 [kubeflow/kubeflow#64](https://github.com/kubeflow/kubeflow/issues/64)\n    - 多框架支持下的统一 API 支持 [kubeflow/kubeflow#102](https://github.com/kubeflow/kubeflow/issues/102)\n- UI 方面\n    - 为各个部件支持统一的 UI [kubeflow/kubeflow#199](https://github.com/kubeflow/kubeflow/issues/199)\n\n目前 Kubeflow 在 GitHub 上有 2400 多个 star，有 40 个左右的贡献者。其长期的目标是成为 CNCF 的一个项目，目前实现仍存在很多问题，窃以为也并不是 production ready 的状态，但它仍然值得一试。\n\n## 关于作者\n\n[@gaocegege](https://github.com/gaocegege)，上海交通大学软件学院研究生在读，[Kubeflow][] Member，寻找 2018 暑期实习。\n\n[Kubeflow]: https://github.com/kubeflow/kubeflow\n[tensorflow/k8s]: https:github.com/tensorflow/k8s\n[ksonnet]: https://github.com/ksonnet/ksonnet\n[@caicloud]: https://github.com/caicloud\n[kube-arbitrator]: https://github.com/kubernetes-incubator/kube-arbitrator\n\n\n欢迎关注我们的 [GitHub](https://github.com/dyweb) 以及[博客](http://blog.dongyueweb.com/) :)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n","cover":"","link":"kubeflow_安利：在_kubernetes_上进行机器学习.html","preview":"\u003cp\u003e这篇文章主要介绍了 Kubeflow 的使用，以及未来的计划，面向人群为对在 Kubernetes 上运行机器学习负载感兴趣的同学。\u003c/p\u003e\n","title":"Kubeflow 安利：在 Kubernetes 上进行机器学习"},{"content":"\n\n[Netlify]: https://www.netlify.com/\n[Travis CI]: https://travis-ci.org/\n\n现在有越来越多的开发者选择把自己的博客以静态网站的方式托管在 GitHub 上, 这样的方式只需要一个域名就可以通过诸如 [Jekyll](https://jekyllrb.com/), [Hexo](https://hexo.io/), [纸小墨](http://www.chole.io/) 等等现有的静态博客生成工具, 非常便捷地搭建出一个样式美观的静态博客.\n\n目前传统的软件项目, 可以通过 [Travis CI][] 等等工具来进行编译, 测试等等持续集成任务, 但是对于一个静态网站来说, 其最主要的产物是 HTML 文件. 而主流的持续集成工具都不支持对静态的页面进行构建的预览. 这篇文章主要介绍了 [Netlify][], 一个可以用来做静态网站的持续集成与持续部署的工具. 通过 [Netlify][], 用户可以非常简单地为其静态网站项目引入持续集成, 并且允许其他成员对静态网站进行 UI 层面的 review.\n\n## 需求介绍\n\n[东岳团队博客](http://blog.dongyueweb.com/) 是一个使用[纸小墨](http://www.chole.io/)构建的静态博客网站, 其托管在 [github.com/dyweb/blog](https://github.com/dyweb/blog) 下. 因为我们的博客采取投稿制, 每个项目成员都可以为这个博客进行投稿, 所以对于投稿的审核是一个非常重要的需求.\n\n目前我们的投稿是以 Pull Request 的方式进行, 作者会首先提交自己的文章, 这是以 Markdown 的格式书写的. 然后会基于 Markdown 文件, 构建出对应的 HTML 文件并加入到 docs/ 目录下. 因此我们对于投稿的审核是在 Pull Request 下进行的.审核包括但不限于对博客样式是否被新的投稿破坏, 投稿内容是否贴合博客方向, 有无 typo 等等. \n\n而因为在 Pull Request 中, 我们只能看到所有文本文件的变动, 而看不到变动后的博客页面. 这在之前只能在文章被合并后, 再次访问[东岳团队博客](http://blog.dongyueweb.com/), 才可以进行效果的审核, 而这时文章已经是合并状态了, 再次修改需要提交新的 Pull Request 才能继续, 这样无疑延长了审核的周期.\n\n因此我们需要一个工具, 可以帮助我们为每个 Pull Request 构建出静态网站, 并且是允许所有成员都可以访问的.\n\n## 相关工作\n\n[Travis CI][] 是一个非常成熟的持续集成工具, 通过它, 用户可以执行自定义的脚本任务, 测试等等. 在之前我们使用它来确认我们的代码是可以完成构建出静态网站这一步操作的. 但是 [Travis CI][] 存在一些问题, 它不能为每个 Pull Request 构建出静态网站供我们审核, 而只能简单地返回构建的成功与否, 这个信息对我们而言是不充分的. 而其他此类的服务也往往具有这个问题.\n\n## 使用 Netlify 进行静态网站持续集成\n\n[Netlify][] 对自己的描述是:\n\n\u003e Netlify is a unified platform that automates your code to create high-performant, easily maintainable sites and web apps.\n\n[Netlify][] 与 [Travis CI][] 等等都是持续集成工具, 但是它更加关注前端, 或者说网站或者 web app 的持续集成与持续部署, 这也是它与其他持续集成工具最大的区别. 我们目前对于 [Netlify][] 的使用也非常简单, 但是这是其他持续集成工具没有的.\n\n为了能够在每个 Pull Request 中看到新的博客文章预览, 我们需要在 [Netlify][] 中为我们的 repository 做一些设置. 在我们的使用中, 设置过程非常地简单:\n\n- 为 repository 启用 Netlify\n- 设置发布目录为 docs/\n\n首先最重要的是为 repository 启用 [Netlify][], 这一部分与其他持续集成工具并无二致. 这一环节最主要的是让 [Netlify][] 在 GitHub 中建立 web hook, 使得它可以监听到整个 repository 的事件. 当然这是自动的, 对于用户而言是不感知的. 随后 [Netlify][] 会要求用户去设置构建的命令以及发布的目录.\n\n\u003cfigure\u003e\n\t\u003cimg src=\"http://gaocegege.com/Blog/images/netlify/start.png\" alt=\"在 Netlify 中启用功能\" width=\"500\"\u003e\n\u003c/figure\u003e\n\n对于我们的博客来说, 因为我们的博客会被编译好放置在 docs/ 目录下, 因此不需要 [Netlify][] 为我们进行构建, 如果你的项目并不是这样操作的, 比如只是有源文件而没有提交构建后的静态网站, 那你可以利用它的这一功能进行远端的构建. [Netlify][] 默认支持 [Jekyll](https://jekyllrb.com/) 等等静态博客生成工具的命令, 因此可以满足绝大多数的应用场景.\n\n\u003cfigure\u003e\n\t\u003cimg src=\"http://gaocegege.com/Blog/images/netlify/netlify.png\" alt=\"使用\" width=\"500\"\u003e\n\u003c/figure\u003e\n\n在配置结束后, 就可以利用 [Netlify][] 来进行持续集成了. 在大多数情况下, 用户甚至不需要像 [Travis CI][] 那样在 repository 里放置配置文件, 直接通过网页操作来搭建起对一个 repository 的持续集成.\n\n每当一个新的 Pull Request 被创建的时候, [Netlify][] 会为这个请求运行一个构建任务, 这个任务最终会生成一个预览, 通过 URL 可以访问到基于这一 Pull Request 的构建结果.\n\n\u003cfigure\u003e\n\t\u003cimg src=\"http://gaocegege.com/Blog/images/netlify/github.png\" alt=\"使用\" width=\"500\"\u003e\n\u003c/figure\u003e\n\n而 URL 是可以自定义的, 比如 [https://deploy-preview-26--blog-dongyueweb.netlify.com](https://deploy-preview-26--blog-dongyueweb.netlify.com/%E5%B0%8F%E8%AE%AE%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B). 不同 Pull Request 会有不同的 URL, 因此基于此还可以去做 Split Testing. 目前 [Netlify][] 支持两个 branch 之间的 Split Testing, 但还是 beta 阶段, 我们没有进行过尝试.\n\n如此以来, 我们可以利用 [Netlify][] 的预览功能, 来对 Pull Request 进行内容和格式的审核. [Netlify][] 本身有更多的功能, 它的愿景是为前端引入持续集成. 限于篇幅, 停笔于此.\n\n## 附录\n\n目前我们知道的使用 [Netlify][] 服务的网站有:\n\n- [统计之都](https://github.com/cosname/cosx.org)\n- [东岳团队博客](http://blog.dongyueweb.com/)\n- [Processing.R 网站](https://github.com/processing-r/processing-r.github.io)\n- [headlesscms.org](https://github.com/netlify/headlesscms.org)\n\nPS: 这是一篇免费的安利文, 我们与 [Netlify][] 利益不相关\n\n欢迎关注我们的 [GitHub](https://github.com/dyweb) 以及[博客](http://blog.dongyueweb.com/) :)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n","cover":"","link":"case_study:_使用_netlify_持续集成你的静态网站.html","preview":"\u003cp\u003e这篇文章主要介绍了 Netlify, 一个可以用来做静态网站的持续集成与持续部署的工具. 通过 Netlify, 用户可以非常简单地为其静态网站项目引入持续集成, 并且允许其他成员对静态网站进行 UI 层面的 review.\u003c/p\u003e\n","title":"Case Study: 使用 Netlify 持续集成你的静态网站"},{"content":"\n\n一致性 (Consistency) 一直是分布式系统里一个很重要的话题, 如果要了解一致性, 要从系统模式开始说起.\n\n## 系统模型\n\n系统模型 (System Model), 是描述系统的特性的一些假设, 基于这些假设才可以设计出各种各样的分布式系统. 这些假设包括但不限于:\n\n- 每个节点的计算能力以及它们的失效模式\n- 节点间通信的能力以及是否可能失效\n- 整个系统的属性, 比如时序等等\n\n其中每一点下面都会讲到. 如果一个系统模型是基于最弱的假设的, 比如节点可能出现硬件错误、网络拥塞或断开以及可能遭到恶意攻击等等, 那基于这样的系统模型的分布式系统可以运行在各种各样的环境下, 因为它的容错极高.\n\n而如果我们做出了很强硬的假设, 比如节点不会 fail, 那基于此假设的系统不需要处理节点失效的问题, 但是因为假设太偏离现实, 所以很难在生产环境中真正去使用.\n\n### 系统模型中的节点问题\n\n节点, 可以理解为系统中的一个个虚拟机或者物理机, 它们是负责真正的计算和存储业务的. 它们会运行确定性的算法, 并且可以将数据写入易失性存储器 (内存) 或者持久性存储器 (磁盘). 不同的存储会受到不同失效模型的影响.\n\n节点的失效模型是指可能使得节点失效的途径. 在大多数实践中, 系统都是假设节点只有在 crash 的时候会失效, 并且会在 crash 之后的某个时间点恢复工作.\n\n这个模型并不是最弱的, 最弱的是著名的[拜占庭失效](https://en.wikipedia.org/wiki/Byzantine_fault_tolerance), 也就是可以以任何方式失效. 基于拜占庭失效模型的算法很少在生产环境中遇到, 这也很好想象, 因为实现的难度必然是很大的, 而且大多数时候也遇不到那么多失效的方式. 因此最常用的还是 crash-recovery 模型.\n\n### 系统模型中的网络问题\n\n在上个年代中, 很少有系统会考虑网络分区的问题, 但是在现在这个环境下, 随着系统的规模增长, 网络的失效也变得常见起来.\n\n一般来说网络的失效模型比较简单, 即直到网络恢复为止, 网络上的信息可能会丢失或者延迟.\n\n### 系统模型中的时序问题\n\n一般来说, 时序只有两种模式, 异步和同步. 同步是指\n\n\u003e Processes execute in lock-step; there is a known upper bound on message transmission delay; each process has an accurate clock\n\n异步是指\n\n\u003e No timing assumptions - e.g. processes execute at independent rates; there is no bound on message transmission delay; useful clocks do not exist\n\n也就是说在同步模型中, 信息一定会在一定延迟内送达, 整个系统是有时序的逻辑的. 而在异步模型中, 不存在系统全局的时序.\n\n在现实中, 大部分的场景是部分同步的时序模型, 它们偶尔可以正常工作, 并提供一些上界, 但在某些情况下，消息会被无限期地延迟，时钟也不同步。\n\n## 共识问题\n\n讨论完系统模型, 接下来就可以在设计好的系统模型的假设下, 讨论共识问题了. 所谓共识 (Consensus) 问题, 就是相互独立的节点之间如何达成一项决议的问题。通俗来说, 如果多个相互独立的节点都认同同一个结果, 那他们就达成了共识. 形式化的描述就是:\n\n- 认同 (agreement): 一个决议过程中所有 N 个节点都认同一个结果\n- 合法 (validity): 该结果必须由 N 个节点中的节点提出\n- 可结束 (termination): 决议过程在一定时间内结束，不会无休止地进行下去\n\n这个听上去很简单, 但是在比较弱的系统模型下, 有很多问题会导致共识是很难达成的. 比如如果网络中存在消息延时、丢失，节点间消息传递; 亦或者是节点存在 crash 的情况. 在这些假设下, 如何达成共识, 才是真正对现实中的分布式系统有价值的讨论.\n\n### FLP 定理\n\n这里就不得不提到一个定理: FLP 定理. 该定理的论文是由 Fischer, Lynch and Patterson 三位作者于1985年发表,之后该论文获得了 Dijkstra 奖。定理所依赖的系统模型很简单:\n\n- 节点只会因为 crash 而失效 (非拜占庭失效)\n- 网络是可靠的, 只要进程非失败，消息虽会被无限延迟，但最终会被送达；并且消息仅会被送达一次 (无重复)\n- 异步的时序模型, 与同步通信的最大区别是没有时钟、不能时间同步、不能使用超时、不能探测失败、消息可任意延迟、消息可乱序\n\n在现实中，一般网络会使用 TCP 协议 (保证了消息健壮、不重复、不乱序)，每个节点都有 NTP 时钟同步 (可以使用超时), 如 FLP 定理的系统模型这样的异步场景相对比较少. 但是也还是存在一定的场景.\n\nFLP 在这样的系统模型下给出了一个很吃鸡的结论: 在异步通信场景，即使只有一个进程失败，也没有任何 (确定性) 算法能保证非失败进程达到一致性.\n\n也就是说, 解决共识问题的算法必须在不存在消息传递边界的情况下放弃强一致 (safety) 或者可用性 (liveness)。\n\n### CAP 理论\n\nCAP 恐怕要比 FLP 定理更出名一些. 甚至国内某 NewSQL 数据库厂商 [PingCAP](https://pingcap.com/index) 把它加入了自己公司的名字中.\n\n这一理论是说\n\n- 强一致性 (Strong Consistency): 如果系统对一个写操作返回成功，那么之后的读请求都必须读到这个新数据；如果返回失败，那么所有读操作都不能读到这个数据，对调用者而言数据具有强一致性\n- 可用性 (Availability): 节点的失效不会导致其余节点的工作收到影响\n- 分区容错性 (Partition tolerance): 在网络分区的情况下，被分隔的节点仍能正常对外服务\n\nC、A、P 三者最多只能满足其中两个，和 FLP 定理一样，CAP 定理也指出了一个不可达的结果 (impossibility result)。\n\n\u003cfigure\u003e\n\t\u003cimg src=\"http://book.mixu.net/distsys/images/CAP.png\" alt=\"CAP\" height=\"500\" width=\"500\"\u003e\n\u003c/figure\u003e\n\n但是需要注意的是, 这里的 C 指的是强一致性, 而非一致性. 因此在现实中, 一致性的强弱与可用性是可以 trade-off 的. 那因此就存在一致性的模型.\n\n#### 一致性模型\n\n一致性模型是描述一致性强弱的. 大类可分为强一致性和弱一致性模型. 但是其中又有很多小的不同.\n\n其中强一致性模型主要是有两种, 一种是线性一致性 (Linearizable consistency), 另一种是顺序一致性 (Sequential consistency).\n\n关键的区别在于，线性化的一致性要求操作生效的顺序等于实际的实时操作排序。顺序一致性允许操作被重新排序，只要在每个节点上观察到的顺序保持一致。作为用户, 能区分两者的唯一方法是，观察系统的所有输入和时间. 从客户端与节点交互的角度来看，两者是等价的。\n\n而弱一致性则是有以客户为中心的一致性模型 (Client-centric consistency models), 因果一致性 (Causal consistency) 和最终一致性 (Eventual consistency).\n\n其中客户为中心的一致性模型可能保证客户永远不会看到数据项的旧版本。这通常是通过在客户端库中构建额外的缓存来实现的，因此，如果客户端移动到包含旧数据的副本节点，那么客户端库就会返回缓存的值，而不是从副本中返回旧值。\n\n最终一致性是系统保证如果没有对某个对象的新更新操作，最终所有的访问将返回这个对象的最后更新的值。这里就有比较多的取舍了, 比如最终是多久之类的.\n\n因果一致性可以理解为是最终一致性的变种, 如果进程 A 通知进程 B 它已经更新了一个数据项，那么进程 B 的后续访问将返回更新后的值，并且写操作将被保证取代前一次写入。和进程 A 没有因果关系的 C 的访问将遵循正常的最终一致性规则。\n\n## 参考文章\n\n- [分布式系统理论 - 从放弃到入门](https://www.cnblogs.com/bangerlee/p/6216997.html)\n- [Distributed systems: for fun and profit](https://github.com/mixu/distsysbook)\n- [FLP Impossibility](http://blog.csdn.net/chen77716/article/details/27963079)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n\n\n欢迎关注我们的 [GitHub](https://github.com/dyweb) 以及[博客](http://blog.dongyueweb.com/) :)\n","cover":"","link":"小议分布式系统的一致性模型.html","preview":"\u003cp\u003e一致性 (Consistency) 一直是分布式系统里一个很重要的话题, 如果要了解一致性, 要从系统模式开始说起.\u003c/p\u003e\n","title":"小议分布式系统的一致性模型"},{"content":"\n\nSuppose we would like to perform a task repeatedly at regular intervals with a goroutine.\n\nHere is the first try:\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"time\"\n)\n\nfunc main() {\n\tgo func() {\n\t\tfor {\n\t\t\tfmt.Println(\"hello, world!\")\n\t\t\ttime.Sleep(5 * time.Second)\n\t\t}\n\t}()\n}\n```\n\nOops, it does not work. The main function terminates immediately after starting the goroutine,\nand the program terminates with the termination of the main function.\n\nWe should let the main function wait for the goroutine. Therefore,\n[WaitGroup](https://golang.org/pkg/sync/#WaitGroup) is needed.\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n)\n\ntype WaitGroupWrapper struct {\n\twg sync.WaitGroup\n}\n\nfunc (w *WaitGroupWrapper) Spawn(f func()) {\n\tw.wg.Add(1)\n\tgo func() {\n\t\tf()\n\t\tw.wg.Done()\n\t}()\n}\n\nfunc (w *WaitGroupWrapper) Wait() {\n\tw.wg.Wait()\n}\n\nfunc main() {\n\tvar wg WaitGroupWrapper\n\twg.Spawn(func() {\n\t\tfor {\n\t\t\tfmt.Println(\"hello, world!\")\n\t\t\ttime.Sleep(5 * time.Second)\n\t\t}\n\t})\n\twg.Wait()\n}\n```\n\nNote that I used a wrapper to wrap the WaitGroup to make it easier to use.\nNow the program works, but we can improve it with tickers, which provide by\nGo for doing something repeatedly at regular intervals.\n\n```go\nticker := time.NewTicker(5 * time.Second)\nwg.Spawn(func() {\n    for {\n        \u003c-ticker.C\n        fmt.Println(\"hello, world!\")\n    }\n})\n```\n\nCurrently, our goroutine runs forever. We can stop it using another channel.\n\n```go\nfunc main() {\n\tvar wg WaitGroupWrapper\n\tdone := make(chan struct{})\n\tticker := time.NewTicker(5 * time.Second)\n\twg.Spawn(func() {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase \u003c-ticker.C:\n\t\t\t\tfmt.Println(\"hello, world!\")\n\t\t\tcase \u003c-done:\n\t\t\t\tgoto exit\n\t\t\t}\n\t\t}\n\texit:\n\t})\n\ttime.Sleep(16 * time.Second)\n\tclose(done)\n\twg.Wait()\n}\n```\n\n## License\n\n- This article is licensed under [CC BY-NC-SA 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/).\n- Please contact \u003cmarketing@dongyue.io\u003e for commerical use.\n","cover":"","link":"go:_tasks_repeated_at_intervals.html","preview":"\u003cp\u003eUse Go to run tasks at intervals.\u003c/p\u003e\n","title":"Go: tasks repeated at intervals"},{"content":"\n\n[@codeworm96]: https://github.com/codeworm96\n[@hawkingrei]: https://github.com/hawkingrei\n[@prism-river]: https://github.com/prism-river\n[Killy]: https://github.com/prism-river/killy\n\n2017 年 10 月 21 日，由 [Golang Foundation](http://golangfoundation.org/) 和 [PingCAP](https://pingcap.com/index) 联合举办的 [Go Hack 17](http://gohack2017.golangfoundation.org/) 在上海心动网络举行，东岳的小哥哥 [@gaocegege](https://github.com/gaocegege) 和 [@codeworm96][]，以及工作一年的 Go 工程师 [@hawkingrei][] 一起组成了队伍 ，参加了这次 hackathon，凭借 [killy: Play TiDB in Minecraft!](https://github.com/prism-river/killy) 取得了二等奖以及 PingCAP 赞助的专项奖。这篇文章是 [@gaocegege](https://github.com/gaocegege) 第一人称视角的 hackathon 记录。\n\n## 好名字是成功的一半\n\n先从开头说起，最开始，是从 TiDB Contributor 的微信群中，看到了 PingCAP 小狼发的链接，知道了这个 hackathon。因为之前 hackathon 中的一些不好的体验，所以当时对是否参加这个比赛还是持观望态度。但是考虑到自己马上就要毕业了，需要多多地接触公司为一年后的实习还有两年后的工作做一些铺垫，在赞助公司里有好多是自己比较感兴趣的公司，于是拉拢了与我同在软院的大四学弟 [@codeworm96][] 和之前在 [Processing.R](https://github.com/gaocegege/Processing.R) 项目上有过合作的 Go 工程师 [@hawkingrei][] 组队报名了。\n\n在报名的时候，组织方要我们提交项目名和队名，于是我们提交了两个特别二次元的名字。团队的名字“Prismriver”是 [@codeworm96][] 学弟起的，是 [东方 Project 里的三姐妹](https://zh.moegirl.org/%E8%8E%89%E8%8E%89%E5%8D%A1%C2%B7%E6%99%AE%E8%8E%89%E5%85%B9%E5%A7%86%E5%88%A9%E5%B7%B4)。项目的名字“Killy”是 [@hawkingrei][] 提出的，一部硬科幻的男主的名字，据他说男主挺帅的。当时想着反正之后做项目可以改，并没有什么关系。但是后来开始之后也就没那么多闲心去想名字了，索性一直沿用到最后。而因为在 GitHub 上创建 organization 的时候我忘了 Prismriver 是连在一起的还是分开的，错误地使用了 prism-river，Go 对项目的路径是有要求的，直接改动组织名会造成很多麻烦的事情，所以现在在 GitHub 上我们的名字是 [@prism-river][]。\n\n## Idea 是成功的一半的一半\n\n定下了名字，开始想 idea。在石墨文档上记录着我们当时所有想到的 idea，一共有十二个。[@hawkingrei][] 工作了一年多，想到的很多 idea 是跟他的日常工作息息相关的，都是一些可以解决他的痛点的小 idea。因为我之前做过容器和持续集成方面的工作，因此想到的多是跟 Docker 或者 CI 有关系，偶尔有一些跟 TiDB 搭点边，下面的截图是我们想到一部分 idea。\n\n\u003cfigure\u003e\n\t\u003cimg src=\"/images/posts/killy/idea.png\" alt=\"Ideas\"\u003e\n\u003c/figure\u003e\n\n在所有的 idea 中，我们确定了两个候选 idea，按照意愿排序是 TiDBcraft 和 Local Travis Runner Based on Docker。后者是我在实现 [caicloud/cyclone](https://github.com/caicloud/cyclone) 的过程中想到的一个工具。在日常的生活中，我比较常用的 CI 工具是 Travis CI，而因为在很多公司里用的多的还是 Jenkins。所以我在想怎么能把 Travis CI 的 build 放在 Jenkins 里跑，一种比较简单的做法，就是保留 Travis CI 的配置，根据配置运行一个容器，把容器放在 Jenkins 的 build 中去跑。Travis CI 自身的设计使得这样的实现变得非常简单，因为它们有专门的一个组件是做 `.travis.yml -\u003e build.sh` 的转换的，得到 bash 脚本之后，放到 Travis CI 对应语言的官方镜像里去跑就好了。通过这样的实现，只要 repo 里有 .travis.yml 配置文件，就可以在任何支持 Docker 的 CI 工具中去运行。\n\nTiDBcraft 就是我们后来决定实现的 [Killy][]，最初的 idea 来自于之前写的 [dronecraft](https://github.com/gaocegege/dronecraft)，而 [dronecraft](https://github.com/gaocegege/dronecraft) 是受 [dockercraft](https://github.com/docker/dockercraft) 启发。我们想在 Minecraft 中监视整个 TiDB 集群的状态，以及其中表的状态，等等，这可以理解为是一个 Minecraft 里的 TiDB Dashboard。这只是一个概念，在未来甚至可以把整个物理机房都建模在 Minecraft 里，再配合 HoloLens，就可以实现 VR 运维了 /w\\\n\n最后在这两个里投票选一个做的时候，是挺纠结的，最后是 [@codeworm96][] 更倾向于 Just for Fun，于是就拍定了做 TiDBcraft。\n\n\u003cfigure\u003e\n\t\u003cimg src=\"/images/posts/killy/record.jpg\" alt=\"Record\" height=\"500\"\u003e\n\u003c/figure\u003e\n\n## 好的分工是成功的一半的一半的一半\n\n在介绍分工之前，先向大家介绍一下我们的架构。\n\n\u003cfigure\u003e\n\t\u003cimg src=\"https://github.com/prism-river/killy/raw/master/presentation/images/arch.png\" alt=\"Arch\" width=\"500\"\u003e\n\u003c/figure\u003e\n\nKilly 一共有两部分，前端与后端。前端是一个在 [Cuberite](https://cuberite.org/) 服务器中的插件。[Cuberite](https://cuberite.org/) 是一个用 C++ 实现的 Minecraft 服务器，支持使用 Lua 语言实现插件来扩展服务器功能，相信玩 Minecraft 的同学一定都接触过。后端是用 Go 实现的服务器，它负责从 TiDB 集群或者是 Prometheus 中定期拿到整个 TiDB 集群状态，然后转发给前端的插件，前端插件会将其绘制在 Minecraft 世界中，前后端通过一个 TCP 连接通信。整体的架构非常简单。\n\n比赛开始的周六，在 5 分钟内我们三个人就敲定了分工。由我来负责前端插件的实现，因为相对来说我对 Minecraft 比较熟悉而且之前也写过几十行 Lua，尽管我更想写 Go =。= [@codeworm96][] 和 [@hawkingrei][] 负责实现后端的逻辑。更具体来说，[@codeworm96][] 负责与数据库本身的交互，[@hawkingrei][] 负责跟整个集群交互获得集群实时的状态。\n\n## 完整的实现是成功的一半的一半的一半的一半\n\n第一天下午是我们精力最充沛的时候。这个时候我在构思并实现如何在 Minecraft 中显示数据库的表，而他们两个人在调研如何订阅 TiDB 中数据的变更和感知到整个集群的状态变更。对于前者我个人觉得 binlog 可能是可以用的，但是这样有点麻烦，不是在这个 hackathon 中适合的解决方案，所以提出直接暴力地轮询。在 [@codeworm96][] 认可后他开始着手实现，而我快速地实现了第一版 UI。在第一版中，有很多局限性，比如字段最多只支持 4 个，因为 Minecraft 里一个告示板只能显示 4 行。后来在晚上的时候，[@codeworm96][] 实现了第一版逻辑，暴力轮询数据库表然后转发给前端，然后我们进行了集成。而 [@hawkingrei][] 这时也实现了第一版，可以拿到所有 TiDB 集群上实例的静态信息。在晚上 12 点左右，我跟女朋友先走了，回去后，[@codeworm96][] 告诉我接受 SQL 查询的功能也实现了，于是我对接了这部分逻辑，实现了在 Minecraft Console 中执行 SQL 查询的功能，并且做了一个小 trick，把查到的记录高亮显示，就是原本是蓝色羊毛，查询到会变成绿色羊毛。\n\n\u003cfigure\u003e\n\t\u003cimg src=\"https://raw.githubusercontent.com/prism-river/killy/master/presentation/images/table.png\" alt=\"Table UI\" width=\"500\"\u003e\n\u003c/figure\u003e\n\n这些结束后，就选择了睡觉，这个时候大概是 1 点半，睡前与 [@hawkingrei][] 约定好三点起来。[@hawkingrei][] 在彻夜工作，在 3 点半的时候微信告诉我他负责的监控部分已经准备好了，因为 Prometheus 不能获得机器的实际状态，而且查询语句比较弱，有些功能短时间内实现比较难，所以方案换成了直接通过 TiKV，PD 的 REST API 拿到状态数据。可是这个时候我还没起来 =。=\n\n4 点 50 分的时候，我终于意识到再睡下去要背锅了，于是艰难地爬了起来。在 5 点 40 分的时候到了心动网络，从旁边的全家里买了一份三明治，走进了 hackathon 场地，发现大多数人都醒着，还在继续写 `_(:з」∠)_` 随后我们进行了联调，并且修改了第一版与数据库表相关的 UI，变成了更像表的结构，这个时候项目的主要功能都已经实现结束了，这个时候大概是早上 9 点。\n\n随后，我开始着手写 PPT 与 README，进行 demo 的录制。在中午的时候与女朋友玩了一局王者荣耀，结束后就到了答辩的时间。\n\n## 良好的答辩是成功的一半的一半的一半的一半的一半\n\n我们组是第十一个答辩的，所以前面听了几组，感觉很多组的 idea 都非常有趣。轮到自己上场的时候，着重强调了功能，演示了 demo，介绍了架构，吹了一下前景（等于没有）。内容而言自己感觉还可以，就是形象不是特别好，人比较憔悴，因为熬夜头发比较油所以戴着帽子显得不是很尊重，而且也有点驼背。\n\n在听过所有人的答辩后，我觉得大概是已经没戏了。有很多组的 idea 是真的非常棒，要技术有技术要场景有场景。但是可能做游戏天生就在展示上比较有优势，而且做的东西也确实比较有趣，最后误打误撞地拿到了二等奖。这可以说是非常有成就感了，要是放在之前，我会觉得一个 hackathon 的二等奖也就只能说说，但是这次的质量让我觉得有种成就达成的感觉，于是决定把它放在我的简历里 =。=\n\n在整场比赛里，给我留下的印象最深刻的项目是 [xcbuild](https://github.com/setekhid/ketos)，他们希望通过实现一个 `docker build` 的替代品，希望能优化 CI 场景下镜像的构建方式。当然还有很多其他有趣的项目，可以去 [Go Hack 17 主页](gohack2017.golangfoundation.org) 里去看看。\n\n这篇文章就到此为止了 :)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n","cover":"","link":"go_hack_17_参赛日记.html","preview":"\u003cp\u003e2017 年 10 月 21 日，由 Golang Foundation 和 PingCAP 联合举办的 Go Hack 17 在上海心动网络举行，东岳的小哥哥 @gaocegege 和 @codeworm96，以及工作一年的的 Go 工程师 @hawkingrei 一起组成了队伍 ，参加了这次 hackathon，凭借 killy: Play TiDB in Minecraft! 取得了二等奖以及 PingCAP 赞助的专项奖。这篇文章是 @gaocegege 第一人称视角的 hackathon 记录。\u003c/p\u003e\n","title":"Go Hack 17 参赛日记"},{"content":"\n\nKubernetes 是由 Google 捐赠给 [CNCF](https://www.cncf.io/) 的一个容器编排框架，也是目前应用最为广泛的编排框架之一。这篇文章是对 Kubernetes 1.8 中的 Scheduler（以下称为 kube-scheduler）的介绍，如果要阅读本文，需要对 Kubernetes 的基本概念如 pod, node 等有所了解。\n\n## 调度过程\n\n在读代码之前，先对 Kubernetes 整体的调度过程做一个简单的介绍。如果你对 Kubernetes 的调度过程已经熟悉的话，可略过不读。\n\nKubernetes 的调度的目的是把一个 pod 放在它最合适的 node 上去运行，所以调度的过程可以被理解为找一个 node，把 pod 放上去的过程。整个过程可以类比为喜闻乐见的相亲活动，pod 是相亲者，node 是所有可选的相亲对象，而 kube-scheduler 就像是相亲网站，它会负责根据相亲者的要求，找到与其要求最接近的相亲对象。当然这里也有一些不同点，比如每个 node 可以运行多个 pod，但是相亲者与相亲对象绑定以后应该是不能再跟别人进行喜闻乐见的相亲活动了/w\\\n\n出于性能，可扩展性以及其他各个方面的考虑，Kubernetes 的调度分为两个过程，第一个过程叫做 Predicates，第二个过程叫做 Priorities。\n\n在 Predicates 过程中，kube-scheduler 会先执行一系列被称为 predicate 的函数，过滤掉不符合硬性条件的 node。这个环节可以对应相亲活动中的硬条件过滤，比如你想找一个程序员，那相亲网站会帮你过滤掉所有不是程序员的选择。而所有通过了 Predicates 过程的 node，都会进入下一个过程。\n\n在 Priorities 过程中，kube-scheduler 会将所有通过 Predicates 过程的 node 根据自己的标准打分，然后从中选择一个得分最高的 node，将其与 pod 绑定在一起，即在该 node 上运行此 pod。这就好比，相亲网站过滤好了潜在的相亲对象，会再帮你对他们做一个打分，然后推荐给你一个条件最好的给你。（不要问我为什么这么熟练）\n\n\u003cfigure\u003e\n\t\u003cimg src=\"/images/posts/kubernetes/initial-state.png\" alt=\"State\" height=\"300\"\u003e\n\u003c/figure\u003e\n\n文字性的叙述过于单调，这里用图来说明这个过程。在图中一共有 16 台服务器，有着不同的配置。\n\n\u003cfigure\u003e\n\t\u003cimg src=\"/images/posts/kubernetes/algorithm.png\" alt=\"State\" width=\"500\"\u003e\n\u003c/figure\u003e\n\n经过了两个 predicate 后，过滤掉了不满足条件的 node，剩下的 node 都足以运行 pod，这时候就需要 Priorities 过程来找到最适合的那个 node。经过两轮 priority 后，发现了一个最适合的 node，于是 pod 和 node 终于在一起了。在绝大多数情况下，调度就结束了。\n\n这里特此说明，这些图只是为了说明调度的过程，Kubernetes 支持的 predicate 和 priority 的维度并不是 CPU 和 Memory。\n\n## 代码编译\n\n编译 kube-scheduler 只需要运行 \n\n```bash\nmake all WHAT=plugin/cmd/kube-scheduler\n```\n\n就可以了，编译后的结果会被放置在 `${KUBERNETES_PATH}/_output/bin/` 下。\n\n## 浅入理解\n\n接下来就进入了最激动人心的代码阅读部分。kube-scheduler 的入口是在 [`plugin/cmd/kube-scheduler/scheduler.go`](https://github.com/kubernetes/kubernetes/blob/release-1.8/plugin/cmd/kube-scheduler/scheduler.go) 中，因此如果要阅读调度的代码，从这里开始就可以了。\n\n不过 `main` 函数只是告诉你他是怎么启动的，真正的逻辑是从 [`plugin/cmd/kube-scheduler/app/server.go#L68`](https://github.com/kubernetes/kubernetes/blob/release-1.8/plugin/cmd/kube-scheduler/app/server.go#L68) 开始的。在 `Run` 函数中，首先创建了一个 Kuberntes Clientset，这可以理解为是 kube-scheduler 跟 Kubernetes 进行交互的 client，通过它可以拿到集群上 node，pod 等等的信息。然后根据这个 client 创建出对 node 和 pod 等等资源的 informer，这里用了一点点的 trick，来规避 import cycle。以 node informer 的逻辑为例，其创建的逻辑在 [`staging/src/k8s.io/client-go/informers/core/v1/node.go#L47`](https://github.com/kubernetes/kubernetes/blob/release-1.8/staging/src/k8s.io/client-go/informers/core/v1/node.go#L47)。Informer 有点像是观察者模式的样子，会 watch 一种资源的变化，这里水很深，代码挺复杂的，好奇的话建议仔细看看。在创建 kube-scheduler 的过程中，几乎所有的 informer（除了 pod informer），都是通过一个 factory 来做的，这样可以防止频繁地创建。之后就是 scheduler 去获取 leader 的地位，然后执行 [`plugin/pkg/scheduler/scheduler.go#L159`](https://github.com/kubernetes/kubernetes/blob/release-1.8/plugin/pkg/scheduler/scheduler.go#L159) 中的 run 函数，真真正正地开始提供相亲服务了。\n\n而实际上，这个 scheduler 也只是一层抽象，真正的 scheduler 的算法，是由 [`plugin/pkg/scheduler/core/generic_scheduler.go`](https://github.com/kubernetes/kubernetes/blob/release-1.8/plugin/pkg/scheduler/core/generic_scheduler.go) 提供的。如果你想替换原本的调度算法，可以从这个地方入手，也可以自己重新写一个全新的 scheduler，然后在 [`pkg/scheduler/factory/factory.go#L708`](https://github.com/kubernetes/kubernetes/blob/release-1.8/plugin/pkg/scheduler/factory/factory.go#L708) 直接修改创建的逻辑。\n\n在真正的逻辑中，一共有这么两个比较重要的函数：`Schedule`，`Preempt`。分别对应着调度和抢占调度的逻辑。\n\n先看 `Schedule` 函数，之前讲的 Predicates 和 Priorities 过程，就是在这个函数里进行的。在有了之前的积淀，这个函数其实很容易读懂。不过值得注意的是，在 Predicates 过程中，因为数据是没有依赖的，就是说检查每个节点是不是合格的节点其实是独立的事件，所以可以被并发来处理，在目前的实现中是会有 16 个独立的 worker 来处理所有的 Predicates 检查的。在 Priorities 过程中，数据不是完全独立的，因此完全并发的做法是行不通的，于是被实现成了 Map Reduce 的模式。可以不依赖其他的节点进行的计算，在 Map phase 来做，不能的就在 Reduce phase 来做。但是由于之前是完全单线程的实现，为了兼容性，目前在代码里可以看到单线程和并发的实现都存在在代码中。Map Reduce 模式的新实现是从 [`generic_scheduler.go#L404`](https://github.com/kubernetes/kubernetes/blob/release-1.8/plugin/pkg/scheduler/core/generic_scheduler.go#L404) 开始的，Map phase 的做法跟之前的 Predicates 是一样的，而 Reduce 是单线程来做的。\n\n值得注意的是，有一些 priority 函数不太适合用 Map Reduce 模式来进行处理，如果你感兴趣的话可以看看 [issues#51455](https://github.com/kubernetes/kubernetes/issues/51455)，这里有一些关于这些函数的讨论。\n\n再看看 `Preempt` 函数，这是一个比较新的 feature，支持 Pod 的抢占，具体的设计可以参见 [design proposal](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/pod-preemption.md)。大致的意思是说如果一个高优先级的 pod 没地方跑了，就杀掉一个在跑的低优先级的 pod，优先运行高优先级的 pod。在实现上，首先调度器会先去寻找那些可能运行这个 pod 的 node，如果 node 是因为一些诸如 selector 不 match 之类的问题被过滤掉了，那是无解的，但是如果是不是因为特别硬性的要求，而是因为资源不够运行这个 pod 之类的，那就列为潜在的合格 node。然后，调度器会在所有潜在的合格 node 上寻找可以被杀掉的 pod，如果不止一个 node 在杀死 pod 后可以满足要求，那就再经过一番选择，主要是选出优先级最低的 pod 所在的 node。选择的过程在 [`generic_scheduler.go#L501`](https://github.com/kubernetes/kubernetes/blob/release-1.8/plugin/pkg/scheduler/core/generic_scheduler.go#L501)，并不难懂。\n\n总体来说，我觉得读起来还是很简单的，本文就到此为止，系列下一篇应该是对 [Nomad](https://www.nomadproject.io/) 的介绍，大概应该就是在最近吧。\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n","cover":"","link":"浅入了解容器编排框架调度器之_kubernetes.html","preview":"\u003cp\u003eKubernetes 是由 Google 捐赠给 CNCF 的一个容器编排框架，也是目前应用最为广泛的编排框架之一。这篇文章是对 Kubernetes 1.8 中的 Scheduler（以下称为 kube-scheduler）的介绍，如果要阅读本文，需要对 Kubernetes 的基本概念如 pod, node 等有所了解。\u003c/p\u003e\n","title":"浅入了解容器编排框架调度器之 Kubernetes"},{"content":"\n\n[Processing.R](https://github.com/gaocegege/Processing.R) 是我在 [Jeremy Douglass](http://jeremydouglass.com/) 指导下，为 [Processing](https://processing.org/) 实现的一个 R 语言模式，这是一个 [Google Summer of Code 2017](https://summerofcode.withgoogle.com/projects/) 项目。这篇文章会讲一讲它的应用，以及实现。\n\n至于这篇文章的受众，我也不是很清楚 ┑(￣Д ￣)┍ 爱看就看看吧 =。=\n\n## Processing 是什么\n\n这里有一篇文章：[Processing是干嘛的？艺术家学编程能做什么？](https://zhuanlan.zhihu.com/p/25432507)，我个人觉得介绍的很到位。我这里就再稍微说一下，我对 Processing 的看法。\n\nProcessing 从功能上而言，是一个做 creative coding 的编程语言，Processing 的 IDE 也直接被称作 Processing Development Environment（缩写 PDE）。\n\n从一个软件工程师的角度来讲，Processing 跟传统的编程语言最大的不同在于，一个完整的 Processing 程序（在 Processing 的语境中，完整的程序被称作 Sketch），一定会有一个图形化的输出。这个输出可以是 2D 图形，3D 图形，也可以是动画，等等。Processing 本身是用 Java 实现了一个编译器，其本身的语法也是跟 Java 几乎一致，因此可以把它当做 Java 的一个 DSL（对语言学不是很懂啦）。Processing 为了使得用户能够更好地进行图形化编程，实现了很多简练的函数，使得寥寥数行就可以实现一个非常简单的图形化应用。\n\n从一个艺术家的角度来讲，因为我不是一个艺术家，我也不知道怎么讲，所以就随便讲讲。绝大多数艺术家，在我看来，在写代码的能力上可能稍微有所欠缺（希望没有冒犯到你）。因此，Processing 对于他们而言，最吸引人的点应该是在于其简单易用。\n\n### 一个简单的例子\n\n这里以一个非常简单的例子介绍 Processing 可以做的事情。\n\n```\nvoid setup() {\n  size(640, 360);\n  background(102);\n}\n\nvoid draw() {\n  // Call the variableEllipse() method and send it the\n  // parameters for the current mouse position\n  // and the previous mouse position\n  variableEllipse(mouseX, mouseY, pmouseX, pmouseY);\n}\n\n\n// The simple method variableEllipse() was created specifically \n// for this program. It calculates the speed of the mouse\n// and draws a small ellipse if the mouse is moving slowly\n// and draws a large ellipse if the mouse is moving quickly \n\nvoid variableEllipse(int x, int y, int px, int py) {\n  float speed = abs(x-px) + abs(y-py);\n  stroke(speed);\n  ellipse(x, y, speed, speed);\n}\n```\n\n先讲效果，这段代码会根据鼠标的位置和鼠标移动的速度在画布上不停的画圆。\n\n在代码中可以看到三个函数，其中 `setup` 和 `draw` 是内置函数，就像是传统编程语言中的 main 函数一样，是整个程序的入口。`setup` 会进行一些设定，比如画布的大小，以及背景颜色。而每当需要绘制新的一帧时，Processing 就会调用 `draw` 函数。因此如果 `draw` 函数每次调用结果都一样，那就是一个静态的图形，如果是不一样的，得到的就是动态的效果。`variableEllipse` 是负责绘制圆形的函数。其参数是鼠标的当前坐标和上一帧的坐标，它会根据坐标计算速度，随后去绘制圆形。如果你感兴趣的话，可以去 [Examples - Pattern](https://processing.org/examples/pattern.html) 亲自试试效果 :)\n\n## Processing.R 是什么\n\n之前提到 Processing 是基于 Java 的 DSL，而且是运行在 JVM 上的。而诸如 Python, Ruby, R 等等语言，也都有在 JVM 上的实现，因此 Processing 也可以通过切换模式的方式来使用其他语言来写 Sketch 的逻辑。\n\n而 [Processing.R](https://github.com/gaocegege/Processing.R)，就是利用 [renjin](http://www.renjin.org/) 实现的 Processing 在 R 语言上的支持。\n\n\u003cfigure\u003e\n\t\u003cimg src=\"https://github.com/gaocegege/Processing.R/raw/master/raw-docs/img/editor.png\" alt=\"Processing.R\" width=\"500\"\u003e\n\u003c/figure\u003e\n\n\u003cfigure\u003e\n\t\u003cimg src=\"https://github.com/gaocegege/Processing.R/raw/master/raw-docs/img/demo.gif\" alt=\"Processing.R Demo\" width=\"200\"\u003e\n\u003c/figure\u003e\n\nProcessing 在 R 语言上的实现，依赖了一个 JVM 上的 R 解释器，每当 Processing 需要调用 draw 等等函数时，都会转而执行 R 代码中相对的定义。目前，Processing.R 支持了绝大多数 Processing 的语法，与此同时支持 Processing 自身众多的库以及 R 语言的包（两者只测试了部分）。这使得 Processing.R 能够在拥有便捷的图形化能力的同时，使用 R 语言中各种方便的包。\n\n## 下载与安装\n\nProcessing 本身下载和安装都特别简单，而且是多平台的，在[此处](https://processing.org/download/)即可找到适合你的版本。而在 Processing 的 Contribution Manager 中的 Modes 一栏中，可以下载 Processing.R。随后在主界面右上角的下拉框中选择 R 即可。\n\n\u003cfigure\u003e\n\t\u003cimg src=\"https://user-images.githubusercontent.com/5100735/29493417-df2b614e-85c7-11e7-98c5-d9f20cf780a4.PNG\" alt=\"下载与使用\" width=\"500\"\u003e\n\u003c/figure\u003e\n\n目前 Processing.R 仍然会积极地进行维护，如果你感兴趣，可以与我联系，还有很多坑等着呢，而且也可以以这个项目为蓝本，拿去申请下一年的 Google Summer of Code，总而言之欢迎各种形式的贡献。原本想做个标题党，发现自己没有 UC 小编的能力，只好找了这么一个不明所以的标题，谢谢你还不辞辛劳地点进来看看 :)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n","cover":"","link":"安利时间:_processing_+_r.html","preview":"\u003cp\u003eProcessing.R 是我在 Jeremy Douglass 指导下，为 Processing 实现的一个 R 语言模式，这是一个 Google Summer of Code 2017 项目。这篇文章会讲一讲它的应用，以及实现。\u003c/p\u003e\n","title":"安利时间: Processing + R = ?"},{"content":"\n\n[Unikernels: Beyond Containers to the Next Generation of Cloud](http://www.oreilly.com/webops-perf/free/unikernels.csp) 是 [Russ Pavlicek](https://www.linkedin.com/in/rcpavlicek/) 的一本动物书（虽然是 O'Reilly 的，但是封面不是动物，是石榴），这本书对 Unikernel 有着比较全面的介绍，而且电子书是免费的，值得一读。\n\n## 啥是 Unikernel？\n\n从 2014 年以来，容器以一种不可逆转的态势席卷了全球，Unikernel 是很多人眼中的下一个容器。如果要了解什么是 Unikernel，首先需要了解什么是 kernel，kernel 是操作系统中的一个概念。应用要运行起来，是肯定要跟硬件打交道的，但是如果让应用都直接操作硬件，那一定是一场灾难。那内核就是在应用与硬件中间的一层抽象，内核提供了对底层硬件的抽象，比如把硬盘抽象成了文件，通过文件系统进行管理。传统的内核会将所有的硬件抽象都实现在其中，其中的代表就是 Linux，这样的内核被称为宏内核（Monolithic Kernel)。在宏内核中，所有的模块，诸如进程管理，内存管理，文件系统等等都是实现在内核中的。这样虽然不存在通信问题，但是任何一个模块的 bug 会使得整个内核崩溃。\n\n于是学者们提出了微内核（Micro Kernel）的概念，在内核中只保留必要的模块，比如IPC，内存管理，CPU调度等等。而其他，诸如文件系统，网络IO等等，都放在用户态来实现。这样会使得内核不那么容易崩溃，而且内核需要的内存小了。但是由于模块间的通信需要以 IPC 的方式进行，因此有一定的 overhead，效率不如很莽的宏内核。\n\n那后来又有了混合内核（Hybrid Kernel)，把一部分不常使用的内核模块，或者是原本需要的时间就很长，因此 IPC 的 overhead 看起来就不是那么夸张的功能，移出内核，而其他的就会放在内核里。\n\n再后来还有 Exokernel，但是太长了就不讲了，这部分内容在 [CSP 课堂笔记之 UniKernel](http://gaocegege.com/Blog/csp/unikernel) 一文中有更详细的解释。\n\n直接说 Unikernel，[Unikernel 的官方解释](http://unikernel.org/)是\n\n\u003eUnikernels are specialised, single-address-space machine images constructed by using library operating systems.\n\n翻译一下就是\n\n\u003eUnikernel 是专用的，单地址空间的，使用 library OS 构建出来的镜像\n\n其最大的卖点就是在，没有用户空间与内核空间之分，只有一个连续的地址空间。这样使得 Unikernel 中只能运行一个应用，而且对于运行的应用而言，没有硬件抽象可言，所有的逻辑，包括应用逻辑和操作硬件的逻辑，都在一个地址空间中。\n\n## 这样有啥好？\n\n哦，原来 Unikernel 就是一个单一内存空间的内核镜像，其中只能有一个应用在运行，那这样有啥好呢，为啥值得我放弃 Linux 而用你这么一个看上去像是阉割版的内核呢？好处就在，小，快，安♂全 /w\\\n\nUnikernel 镜像都很小，由 [MirageOS](https://mirage.io/) 实现的一个 DNS server 才 184KB，实现的一个 web server 674 KB，小到恐怖的程度。\n\n然后就是快，启动很快。因为镜像都很小，所以起停都在毫秒级别，比传统的 kernel 要快多了。\n\n最后是安全，一般来讲，小的东西相对而言比较安全。Unikernel 中没有 Shell 可用，没有密码文件，没有多余的设备驱动，这使得 Unikernel 更加安全。\n\n## 开发测试与传统有啥不同？\n\nUnikernel 在真正实践中，如何开发与测试是一个值得关注的问题。在开发过程中，开发者可以假定自己在传统的操作系统上进行开发，而所有内核相关的功能，暂且由开发机的操作系统提供。\n\n而在测试环境中，大部分 Unikernel 的实现会将应用代码与需要的内核模块构建成 Unikernel 后，再将其跑在一个传统的操作系统上，利用传统操作系统上的工具来测试 Unikernel。以 [Rumprun](https://github.com/rumpkernel/rumprun) 为例，它可以通过 KVM/QEMU 来运行一个 Rumprun Unikernel VM，随后用 Host OS 上的 GDB 来对其进行调试，具体细节可见[此处](https://github.com/rumpkernel/wiki/wiki/Howto:-Debugging-Rumprun-with-gdb)。关于调试就介绍到此，如果你想了解更多，[Hacker News 上的这个 post](https://news.ycombinator.com/item?id=10954132) 可能会给你一些启发。\n\n在发布阶段，这是 Unikernel 最简单的事情了。Unikernel 最后的产物就是一个 kernel image，可以在 Hypervisor，Bare Metal 等等各种环境上运行。\n\n所以可以看到，其中 Unikernel 在软件过程中与传统方式最大的不同就在于调试与测试。而在发布的阶段，传统的方式可能发布的是一个应用，时髦一点那一个容器镜像，而 Unikernel 则是一个高度定制化的 kernel。\n\n## 目前的 Unikernel 项目\n\n介绍完 Unikernel，接下来将介绍下目前比较成气候的 Unikernel 项目，Unikernel 的实现大部分都是语言特定的。因为涉及到具体语言的运行时，所以很难有一个项目可以适配所有的技术栈。\n\n[MirageOS](https://mirage.io) 应该是名气最大的一个 Unikernel 项目，它是使用 OCaml 进行开发的，也是要求开发者懂 OCaml 才行。与其他 Unikernel 相比，它非常成熟，而且有一些[论文](https://mirage.io/wiki/papers)，对钟爱论文的同学非常友好。\n\n[HaLVM](https://github.com/GaloisInc/HaLVM#readme) 也是一个比较早的 Unikernel 项目，它可以帮助 Haskell 程序员们把自己的 Haskell 程序构建成 Unikernel。如果你不会 Haskell，那就算了 =。=\n\n[ClickOS](http://cnp.neclab.eu/clickos) 是一个比较独特的项目，他也非常古老了，但是原本 Click 并不是以 ClickOS 的形式出现的，原本它只是一个支持定制的 router，后来就变成了 ClickOS，一个基于 Unikernel 的 router。它也有很多[论文](http://www.read.cs.ucla.edu/click/publications)，大部分都是关于 Click 本身，而不是 Unikernel 实现的。\n\n[Rumprun](https://github.com/rumpkernel/rumprun/) 也是一个非常独特的项目，其利用了 [Rump Kernel](http://rumpkernel.org/)，理论上 POSIX 兼容的程序，都可以用 Rumprun 来构建成 Unikernel。\n\n如果这些还不能满足你的好奇心，[Open source work on unikernels](http://unikernel.org/projects/) 上列出了众多的 Unikernel 项目，如有需要还请自行浏览。\n\n## Unikernel, Docker，Hyper 与 Linuxkit\n\n对 Unikernel 的介绍就是这些了，最后再谈谈自己对目前很火的一些概念的看法，以及它们之间的联系。\n\nUnikernel，在我看来，是另一种形式上的容器。在一个 Unikernel 中，只能运行一个应用，这与容器的哲学不谋而合。但现在容器最吸引人的特性并不是它的便捷，而是在它的分发。Docker 让我们看到了，原来应用的分发可以这么无痛。而 Unikernel 与容器相比，虽然可以做的更小更安全，而且也不需要有 Docker Daemon 这样的后台程序存在，甚至不需要 Host OS，或者 Hypervisor，但是它一是与传统的软件过程有较大的出入，二是在分发等等方面不能做到像容器那样方便。所以它目前肯定不会成为主流的应用分发方式，还需要进一步探索。\n\n为了能够让 Unikernel 尽快进入生产环境，有一项工作很值得关注。\n\n![](/images/posts/unikernel/unikernel.png)\n\n在 Unikernel 里运行一个 Docker Container，想法很美好，但是同样也有很多问题。这样其实并没有利用到容器便于分发的优势，也没有完全发挥 Unikernel 的优势，我觉得这不是未来。不过作为一种折中方案值得一看，可惜从 DockerCon 15 之后就没听见什么动静了。\n\nHyper Container 的技术特别独特，之前在 [Docker 与 Hyper](http://gaocegege.com/Blog/docker-rambles) 一文中介绍过，这里不再多说。他们的实现很完整，有对标 runc 的 runv，有扩展 Kubernetes 中 container runtime 的 frakti，虽然我没有尝试过，但是我觉得是比 Docker in Unikernel 更加可行的方案，讲道理很有前途。\n\nLinuxkit 是 Docker 改名 Moby 后随之发布的一个项目。Linuxkit 严格来说是一个构建操作系统的工具集，可以用来构建 Unikernel，但是也可以用来构建最小化的 Linux Kernel，目前还不知道要往什么方向发展。\n\n这些概念或多或少都有相互重叠的部分，也没有谁一定胜过谁的说法，但都有一个特点：有趣。它们都有自己不同的应用场景，本来嘛，Docker 也不是银弹。\n\nPS：本文都是纸上谈兵，作者本人并无对 Unikernel 在生产环境中的使用经验（应该暂时也没有人有），大家看看就好，如有疏漏还请不吝指教:)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n","cover":"","link":"unikernel:_从不入门到入门.html","preview":"\u003cp\u003eUnikernels: Beyond Containers to the Next Generation of Cloud 是 Russ Pavlicek 的一本动物书（虽然是 O\u0026rsquo;Reilly 的，但是封面不是动物，是石榴），这本书对 Unikernel 有着比较全面的介绍，而且电子书是免费的，值得一读。\u003c/p\u003e\n","title":"Unikernel: 从不入门到入门"},{"content":"\n\n本文的受众主要是想在暑假找点事情做，挣点外快的同学，亦或是想积累一下参与真实软件开发经验的同学。\n\n## 背景介绍\n\n[Google Summer of Code](https://developers.google.com/open-source/gsoc/)（下称作 GSoC）是谷歌组织并提供经费，面对全球（绝大多数国家）在读学生的在线编程项目。它的[官方介绍](http://write.flossmanuals.net/gsocstudentguide/what-is-google-summer-of-code/)是：\n\n\u003e Google Summer of Code (GSoC) is a global program that matches students up with open source, free software and technology-related organizations to write code and get paid to do it! The organizations provide mentors who act as guides through the entire process, from learning about the community to contributing code. The idea is to get students involved in and familiar with the open source community and help them to put their summer break to good use.\n\n即是：\n\n\u003e Google 编程之夏是一个全球性项目，旨在为学生们和开源、自由软件、技术相关的组织建立联系，让学生们贡献代码并获得报酬！组织会提供导师，在学生从熟悉社区到贡献代码的整个过程中提供指导。这个想法的目的是让学生们参与和熟悉开源社区，并帮助他们充分利用暑假时间去得到锻炼。\n\n整个活动的流程是这样的：在每年的二月末，GSoC 会公布一个 Mentoring organizations 的列表，比如 [2017 Organizations](https://summerofcode.withgoogle.com/organizations/)，这个列表是受到谷歌认可的开源社区或者组织。随后学生可以从列表中挑选出适合自己的 organization，并且在 organization 的 ideas list 中找出自己感兴趣，觉得可以胜任的 idea，提出申请。一个学生最多可以申请 5 个 idea，申请在 3 月末开始，4 月初结束。在 5 月份，Google 会公布所有入选的学生，社区会给每个学生分配一个或多个 mentor，mentor 负责指导学生的工作，并评估学生的工作是否满足了社区的要求。\n\n公布之后，会有持续一个月的 Community Bonding Period，在这个阶段学生需要尽快融入社区，跟自己的 mentor 建立联系，熟悉社区工具链，交流工具等等。6 月份开始正式的开发工作。开发工作一共有三个阶段，同时也会有三个 evaluation。每个阶段大约一个月，会有一个小目标，如果 mentor 认为你完成了阶段性的目标，Google 会在每次 evaluation 结束后发放奖金。三次一共的奖金在 1200 - 6600 美刀之间。具体的数额跟所在地的 [Purchasing Power Parity](https://developers.google.com/open-source/gsoc/help/student-stipends) 有关,中国是 3600 美刀。\n\n整个项目大概在 9 月份结束，但是社区的期望肯定是学生能够继续进行贡献，这也是他们获得新的 contributors 的一个重要途径。并且在持续贡献后，学生可以在来年的 GSoC 时申请成为 mentor，虽然 mentor 没有奖金，但是有一个 Google 组织的 Mentor Summit，据说就是公费旅游。\n\n## 名词解释\n\n因为在申请的时候不同的社区对于同一个对象的叫法都有所不同，所以这里列一些常见的名词的解释。\n\n| 名词        | 解释           |\n| ------------- |-------------|\n| Organization，组织，社区    | Google 公布的 Mentoring organizations 中的组织，可以接收学生参与 GSoC |\n| 学生，申请者    | 申请参加 Google Summer of Code 的学生 |\n| Slots         | 社区可以接收的学生数量，由 Google 决定 |\n| Mentor，导师   | 学生申请成功后社区指定的导师，指导学生的具体工作，以及负责评估完成度  |\n| Stipend，奖金，奖励   | 在学生完成阶段性目标后由 Google 发放的奖金，具体的数额跟所在地的 [Purchasing Power Parity](https://developers.google.com/open-source/gsoc/help/student-stipends) 有关  |\n| Evaluation    | 阶段性检查，mentor 会检查学生有没有达成阶段性成果，影响奖金发放 |\n\n## 申请之前的准备\n\n申请是一个比较漫长的过程，如果想更加稳妥一点，建议不要在谷歌公布 Mentoring organizations 列表后再进行准备，而是要提前选定一个或几个社区，进行持续的贡献和交流，尽可能混一个脸熟。这样与后期才开始准备申请的同学而言就有了很大的优势。除此之外，要日常性地多给开源项目做贡献。在申请的时候很多社区会要求学生提供其开源贡献的经历，无论是不是对自己社区的。这时如果你已经是其他社区的积极贡献者了，那无疑是会加分的。\n\n对于社区的选择，如果你偏向保守，可以多回顾往年的列表，有一些组织是雷打不动的，比如 Python Software Foundation, Apache 这些老牌开源社区，这些相对于其他组织，有更大的可能被谷歌选中。如果你喜欢高风险，可以事先问问社区是否有申请 GSoC organization 的打算，如果有，而且你也看好，可以选择这样的社区进行贡献。\n\n至于贡献的时间，窃以为比较理想的时间是前一年的 12 月份或者同年的 1 月份开始，就要尝试着去给社区做一些微小的工作。这些工作包括但不限于：\n\n* 贡献代码，无论什么社区，都喜欢高质量的 PR\n* Review PR，给别人的 Review 点赞\n* 提交 Bug\n* 添补更新文档\n* 在 IRC 里解决别人的问题\n\n在贡献的过程中，要注意交流，不要只是提交了就走人了，最好是可以时刻跟进，及时回复别人的信息也是一种表明你的热情的方式。\n\n## 申请\n\n### Organization 介绍\n\n申请真正开始于谷歌公布的 Mentoring organizations 列表，这里大致介绍下其内容。\n\n![](/images/posts/gsoc/processing-org.png)\n\n以 2017 年 GSoC 其中的一个开源组织 [The Processing Foundation](https://summerofcode.withgoogle.com/organizations/4962961559912448/) 为例，介绍一下 GSoC 主页上 organization 的页面布局。每个 organization 都会有一段介绍性的文字，这个不是很关键。右边的一栏是比较重要的，其中 Technologies 是方便大家在搜索 organization 的。上面的 VIEW IDEAS LIST 比较重要，一般来说每个社区会事先提出一些他们期望的 idea，学生可以就这些 idea 进行申请。当然社区也鼓励学生提出自己的 idea。其下的 Chat 和 Email 一般来说会写明该社区常用的交流工具，在申请的过程中往往需要频繁地与社区相关人员交流。\n\n### 正式申请\n\nProposal，是一个申请时很关键的材料。它是学生在申请时需要提交的一个设计文件，在其中，学生往往需要写明自己的背景（学术背景，开源贡献经历等），对 idea 的了解与认识，以及大致的实现思路和方法。Proposal 的书写是没有定式的，只要可以突出你的长处就好，这是社区对你了解的唯一途径，所以需要你把自己所有的优势都要写在其中。\n\n[Proposal for Processing.R](https://docs.google.com/document/d/1b0HhRVKtCJkDaxP9dfSwzthzX0FRv6Y_0Yk58r634TA/edit?usp=sharing) 是我在申请时的一份 Proposal，可以列为参考，介绍下常规的写法。\n\n首先是 Project Description，这个部分就是让社区知道你对 idea 没有理解错，你深刻地了解这个 idea 想做的是什么。三言两语就好了。\n\n然后是 Implementation，我个人觉得是比较重要的部分。要向社区证明你已经有了完整的实现思路，现在差的就是写代码实现而已了。\n\n其次，是 Development Process，社区肯定更喜欢那些风险低，feature 吸引人的申请。一个好的 schedule 可以让社区相信你是真的已经做好了准备。精确到天自然最好，但是基本来说比较难，周和月都是不错的选择。不过有一点需要注意，不要把所有时间都安排的满满的，还是需要有一些 buffer 的。不然看起来太假了 =。=\n\n最后是 About Me，因为我对于申请的项目而言，没有什么积累，而且没有相关领域的贡献，所以把这一项放在了最后。如果你是申请 Kubernetes，而日常是 Docker 的 contributor，那把 About Me 放在最前面是更好的选择，完全看申请而定。\n\n一般来说这些是都要有的，还有一些其他的，社区特定的要求，这个也是要注意的。这里还有一些我认为写的比较好的 proposal：\n\n* [Integrate Unikernel Runtime](https://docs.google.com/document/d/1Vld4j0B-wk1A1827gIc5fzWHJlzQVqcYQnCAKJwe_ZM/edit?usp=sharing)\n* [Optimization of Distance Between Methods in Single Java Class](https://docs.google.com/document/d/1lWXpWhUN6cE06sjQANjWxamc_X3ddbSphTRSofChLyk/edit?usp=sharing)\n\n感兴趣也可以看下。\n\n### Community Bonding Period\n\n走到这一步，离拿钱就不远了，因为 GSoC 申请比完成更难。在 Community Bonding Period，你需要跟自己的 mentor 建立联系，积极融入社区等等，但是没有量化的标准，这个就不再多说了。\n\n### 开始写码\n\n写码这个，不同的项目有不同的要求。有一部分项目是给开源的 repo 贡献代码，因此要走整个 review 的流程，这想必大家都比较熟悉，不再多说。但是还有一部分项目，是 standalone 的，就是自己开了个 repo，自己写，比如我申请到的 [Processing.R](https://github.com/gaocegege/Processing.R)。这就会有很多问题，这里也着重说一下对于这一类项目的建议。\n\n首先，要明确之前 proposal 里写的 schedule 只是为了给社区信心的，事实上在开始写码之前，mentor 会跟你重新制定计划。所以如果你在 Community Bonding Period 写了很多 feature，很可能没有用，因为 mentor 说不定会给你重新制定要求。\n\n关于 standalone 的项目，跟 mentor 以及社区其他成员的交流是很关键的。因为你的 repo 别人都是没有 watch 的，所有的变动，可能只有你和你的 mentor 知道。如何让社区里的其他人看到你的贡献，非常重要。所以尽可能多在 IRC 里跟大家分享你遇到的问题，或者你的项目中的新 feature，可能会让你感觉到自己不是玩单机游戏。\n\n其次就是要尽早引入 CI，并且所有变动都以 PR merge 的方式进行，以保证代码质量。一个人的项目，质量很容易滑坡，CI 和 PR 可以让 mentor 对你的代码有一个很好的 review 体验，他也会更加积极一点。\n\n最后是不要太肝。因为自己的项目，每个 PR 的生命周期都是由自己负责的，很容易就会进入疯狂开发的状态，但是记住上面说的，在开始写码之前，mentor 会跟你重新制定计划 :)\n\n### Evaluation 与奖金发放\n\nEvaluation 是一个双向的评估，mentor 会评估学生的工作完成度如何，学生会评估社区和 mentor 对自己的帮助是否到位，学生对社区的评估可能会影响社区明年能够参加 GSoC 以及 slots 的数量，mentor 的评估决定学生能否拿到奖金。\n\n如果通过了 evaluation，奖金会在几天内到账。\n\n## 注意事项与 Tips\n\n1. **只有**学生才可以申请 GSoC。\n1. 一般来说 GSoC 主页需要科学上网才能访问。\n1. 时差问题是申请的时候需要注意的问题，这个需要格外注意，每年都有人错过申请。\n1. 奖金的发放是通过 [Payoneer](https://www.payoneer.com/home/) 发放的，如果是非美元账户，需要支付 4% 左右的换汇费用。\n1. 第一次入选 Mentoring organizations 的组织原则上只有 1 个或者 2 个 slots。\n\n## 结语\n\n这是一篇摸鱼作，希望能够对各位有所帮助。其实大家在选择开源社区的时候可以多问问有经验的人，尽可能选择一个友好的社区作为开始，这样会在开源的路上走的远一点。。\n\n## 相关文章\n\n* [Google 编程之夏(GSoC)：海量优质项目，丰厚报酬，你竟然还不知道？](https://zhuanlan.zhihu.com/p/27330699)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n","cover":"","link":"google_summer_of_code_申请指南.html","preview":"\u003cp\u003e本文的受众主要是想在暑假找点事情做，挣点外快的同学，亦或是想积累一下参与真实软件开发经验的同学。\u003c/p\u003e\n","title":"Google Summer of Code 申请指南"},{"content":"\n\n这篇文章是写给[东岳网络工作室](https://github.com/dyweb)的小伙伴们的 (广告:欢迎在交大的同学加入)，适用于有一定数据库背景并且想要了解时间序列数据库的同学。\nPS: 中文版是在[英文版](/introduction_to_time_series_database.html)之后写的，很生硬，请见谅。\n\n目录\n\n- 什么是时间序列数据库 (TSDB)\n- 时间序列数据库数据模型\n- 时间序列数据库演变\n- 时间序列数据库类型\n  - KairosDB\n  - InfluxDB\n- 热点话题\n  - 低延迟\n  - 数据\n  - 元数据索引\n  - Tracing\n\n## 什么是时间序列数据库 (TSDB)\n\n时间序列数据库 `Time Series Database` (TSDB) 相对于关系型数据库 (RDBMS)，NoSQL，NewSQL 还很年轻。\n但是，随着系统监控以及物联网的发展，已经开始受到更多的关注。\n维基百科上对于时间序列的定义是‘一系列数据点按照时间顺序排列’，\n但是我个人的理解是**存储在服务端的客户端历史**。\n时间序列数据就是历史，它具有**不变性**, **唯一性**以及**可排序性**。\n比如`在2017年9月3日21点24分44秒，华东区的机器001的CPU使用率是10.02%`，\n这个值不会像银行存款一样随着时间发生变化，它一旦产生了就不会有更新。\n下一秒的使用率是一个新的数据点，其他机器的使用率在其他时间序列里。\n并且**数据到达服务器的顺序并不影响正确性**，根据数据本身可以直接进行排序和去重。\n客户端发送本地的历史到服务器端，即使服务器端挂掉了，客户端依旧继续他本来要做的事情而不受到影响。\n对于很多客户端来说，发送数据到 TSDB 跟它的本职工作并没有关联。\n比如一个静态文件服务器的主要职责是传送文件而不是上报 HTTP 状态码。\n关系型数据库则起着完全不一样的作用，它是客户端做决定的主要依据，\n这就导致时间序列数据库和关系型数据库的读写规律有很大的不同。\n比如你取钱之前，银行的程序必须从数据库里找到你的那条存款记录，读出你的余额，确认不会透支才能把钱给你，\n然后更新你的余额。\n然而大多数时间序列数据库的客户端是只读(监控系统)或者只写(被监控的系统)，\n并且读取数据是并不是读取特定的某条，而是读取某个时间区间内的大量数据，比如`最近1小时的CPU使用率`远比\n`2017年9月3日21点24分44秒的CPU使用率`有用，脱离上下文的时间序列数据并没有什么作用。\n\n时间序列数据跟关系型数据库有太多不同，但是很多公司并不想放弃关系型数据库。\n于是就产生了一些特殊的用法，比如用 [MySQL 的 VividCortex](https://www.vividcortex.com/blog/2014/12/16/in-case-you-missed-it-building-a-time-series-database-in-mysql/),\n用 [Postgres 的 Timescale](http://www.timescale.com/)。\n很多人觉得特殊的问题需要特殊的解决方法，于是很多时间序列数据库从头写起，不依赖任何现有的数据库,\n比如 [Graphite](https://graphiteapp.org/)，[InfluxDB](https://github.com/influxdata/influxdb)。\n\n## 时间序列数据库演变\n\n时间序列数据库有[很多](https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=All)，\n下面列出的是一些我个人认为具有里程碑意义的数据库。\n很多数据库主页上并没有最初版本的发布日期，因此以 GitHub 上最早的 tag 作为发布日期。\n\n- 1999/07/16 [RRDTool First release](https://en.wikipedia.org/wiki/RRDtool)\n- 2009/12/30 [Graphite 0.9.5](https://github.com/graphite-project/graphite-web/releases/tag/0.9.5)\n- 2011/12/23 [OpenTSDB 1.0.0](https://github.com/OpenTSDB/opentsdb/releases/tag/v1.0.0)\n- 2013/05/24 [KairosDB 1.0.0-beta](https://github.com/kairosdb/kairosdb/releases/tag/v1.0.0-beta2a)\n- 2013/10/24 [InfluxDB 0.0.1](https://github.com/influxdata/influxdb/releases/tag/v0.0.1)\n- 2014/08/25 [Heroic 0.3.0](https://github.com/spotify/heroic/releases/tag/0.3.0)\n- 2017/03/27 [TimescaleDB 0.0.1-beta](https://github.com/timescale/timescaledb/releases/tag/0.0.1-beta)\n\n[RRDTool](https://oss.oetiker.ch/rrdtool/) 是最早的时间序列数据库，它自带画图功能，现在大部分时间序列数据库都使用[Grafana](https://github.com/grafana/grafana)来画图。\n[Graphite](https://graphiteapp.org/) 是用 Python 写的 RRD 数据库，它的存储引擎 [Whisper](https://github.com/graphite-project/whisper) 也是 Python 写的，\n它画图和聚合能力都强了很多，但是很难水平扩展。\n来自雅虎的 [OpenTSDB](http://opentsdb.net/) 使用 HBase 解决了水平扩展的问题。\n[KairosDB](https://kairosdb.github.io/) 最初是基于OpenTSDB修改的，但是作者认为兼容HBase导致他们不能使用很多 Cassandra 独有的特性，\n于是就抛弃了HBase仅支持Cassandra。\n有趣的是，在[新发布的](http://opentsdb.net/docs/build/html/new.html) OpenTSDB 中也加入了对 Cassandra 的支持。\n故事还没完，Spotify 的人本来想使用 KairosDB，但是觉得[项目发展方向不对以及性能太差]((https://labs.spotify.com/2015/11/16/monitoring-at-spotify-the-story-so-far/))，就自己撸了一个 [Heroic](https://github.com/spotify/heroic)。\n[InfluxDB](https://github.com/influxdata/influxdb) 早期是完全开源的，后来为了维持公司运营，闭源了集群版本。\n在 Percona Live 上他们做了一个[开源数据库商业模型正面临危机](https://www.youtube.com/watch?v=Kvf5jWZjw0U)的演讲，里面调侃红帽的段子很不错。\n并且今年的 Percona Live 还有专门的[时间序列数据库单元](https://www.percona.com/live/17/program/schedule/time-series)。\n\n## 时间序列数据库数据模型\n\n时间序列数据可以分成两部分，**序列**和**数据点**。\n序列就是标识符，比如`华东区机器001的CPU使用率`。\n数据点是时间戳和数值构成的数组。\n\n对于序列，主要的目的是方便使用者进行搜索和筛选。\n比如你需要查询`华东区所有机器的CPU使用率`。\n序列 `华东区机器001的CPU使用率` 的标识符是 `name=cpu.usage machine=001 region=cn-east`，\n查询则是 `name=cpu.usage machine=* region=cn-east`。\n为了处理大量的序列，需要建立（倒排）索引来提高查询速度。\n一些时间序列数据库选择使用外部搜索引擎来解决这个问题，比如 [Heroic](https://github.com/spotify/heroic) 使用了 Elasticsearch,\n另一些则选择自己写索引，比如 [InfluxDB](https://github.com/influxdata/influxdb), [Prometheus](https://prometheus.io/)。\n\n对于数据点，有两种模型，一个数组的点  `[{t: 2017-09-03-21:24:44, v: 0.1002}, {t: 2017-09-03-21:24:45, v: 0.1012}]`\n或者两个数组，一个存时间戳，一个存数值。前者是行存，后者是列存（不是列簇）。\n大部分基于现有数据库( [Cassandra](https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=Cassandra), [HBase](https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=HBase) ) 的是第一种。\n对于新的时间序列数据库第二种更为普遍，TSDB 属于 OLAP 的一个子集，列存能有更好的压缩率和查询性能。\n\n## 时间序列数据库类型\n\n时间序列数据库可以分成两类，基于现有的数据库或者专门为时间序列数据写的数据库。\n我们以 [KairosDB](https://kairosdb.github.io/) 和 [InfluxDB](https://github.com/influxdata/influxdb) 为例来分析。\n有很多时间序列数据库是[基于 Cassandra](https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=Cassandra) 的，\n[KairosDB](https://kairosdb.github.io/) 是其中比较早的一个。\n[InfluxDB](https://github.com/influxdata/influxdb) 是专用于时间序列的数据库，他们尝试了很多存储引擎，最后写了自己的 `Time Structured Merge Tree`.\n\n### KairosDB\n\n在看 KairosDB 之前我们先用一个简化版本的预热一下。\n[Xephon-K](https://github.com/xephonhq/xephon-k) 是我写的一个有多种存储后端的时间序列数据库(专门用来对付各种课程大作业)。\n它有一个非常 naive 的基于 Cassandra 的实现。\n\n如果你对 Cassandra 不熟的话，这里有个简单的介绍。\nCassandra 是一个列簇数据库，是谷歌 BigTable 的开源实现。列簇又被称作宽列。\n实质上是一个多层嵌套的哈希表。它是一个行存储，不是列存储。\n一些 Cassandra 的名词可以跟关系型数据库中的对应起来。\nCassandra 中的 `Keyspace` 就是指的 `database`, 比如一个博客和一个网店虽然使用同一个 MySQL 服务器，但是各用一个数据库以进行隔离。\nCassandra 中的 `Table` 是一个哈希表，他的 `Partition Key` 是哈希表的键(也被叫做物理行键)，它的值也是一个哈希表，这个哈希表的键是 `Cluster Key`，\n它的值还是一个蛤希表。\n当使用 CQL 创建一个 `Table` 的时候，主键中的第一个列是 `Partition Key`，第二个列是 `Cluster Key`。\n比如在下面的 CQL 中， `Keyspace` 是 `naive`, `Table` 是 `metrics`，`Partition Key` 是 `metric_name`,\n`Cluster Key` 是 `metrics_timestamp`。\n最内层的哈希表是 `{value: 10.2}`, 如果需要我们可以存更多的值，比如 `{value: 10.2, annotation: '新 bug 上线啦'}`。\n\n````sql\nCREATE TABLE IF NOT EXISTS naive.metrics (\n    metric_name text, metric_timestamp timestamp, value int,\n    PRIMARY KEY (metric_name, metric_timestamp))\nINSERT INTO naive.metrics (metric_name, metric_timestamp, value) VALUES (cpu, 2017/03/17:13:24:00:20, 10.2)    \nINSERT INTO naive.metrics (metric_name, metric_timestamp, value) VALUES (mem, 2017/03/17:13:24:00:20, 80.3)   \n````\n\n![Cassandra Time Series Data model](images/posts/cassandra-tsdb-model.png)\n\n上图显示了使用 Cassandra 存储时间序列数据时 naive 的表结构，\n`Cluster Key` 存储时间戳，列的值存储实际的数值。\n它 naive 之处在于序列和 Cassandra 的物理行是一一对应的。\n当单一序列的数据点超过 Cassandra 的限制(20亿)时就会崩溃。\n\n一个更加成熟的表结构是把一个时间序列按时间范围分区，(KairosDB 按照 3 周来划分，但是可以根据数据量进行不定长的划分)。\n为了存储分区的信息，需要一张额外的表。\n同时在 naive 里序列的名称只是一个简单的字符串，如果需要按照多种条件进行筛选的话，需要存储更多的键值对，并且对于这些键值对需要建立索引以提高查询速度。\n\n下面是完整的 KairosDB 的表结构，`data_points` 表对应的是 naive 里的 `metrics` 表。\n它看上去不像人写的，因为它就是直接导出的，KairosDB 使用的旧版 Cassandra 的 Thrift API 创建表结构，没有 `.cql` 文件。\n\n````sql\nCREATE TABLE IF NOT EXISTS data_points (\n    key blob,\n    column1 blob,\n    value blob,\n    PRIMARY KEY ((key), column1)\n) WITH COMPACT STORAGE;\nCREATE TABLE IF NOT EXISTS row_key_index (\n    key blob,\n    column1 blob,\n    value blob,\n    PRIMARY KEY ((key), column1)\n) WITH COMPACT STORAGE;\nCREATE TABLE IF NOT EXISTS row_key_time_index (\n    metric text,\n    row_time timestamp,\n    value text,\n    PRIMARY KEY ((metric), row_time)\n)\nCREATE TABLE IF NOT EXISTS row_keys (\n    metric text,\n    row_time timestamp,\n    data_type text,\n    tags frozen\u003cmap\u003ctext, text\u003e\u003e,\n    value text,\n    PRIMARY KEY ((metric, row_time), data_type, tags)\n)\nCREATE TABLE IF NOT EXISTS string_index (\n    key blob,\n    column1 blob,\n    value blob,\n    PRIMARY KEY ((key), column1)\n) WITH COMPACT STORAGE\n````\n\n有很多基于 Cassandra 的时间序列数据库，他们的结构大多相同，你可以看[这个列表]((https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=Cassandra))。\n我最近正在写一个如何用 Cassandra 和 Golang 自己写个时间序列数据库的博客，写好之后会把地址更新在这里。\n\n### InfluxDB\n\n[InfluxDB](https://github.com/influxdata/influxdb) 在存储引擎上[纠结了很久](https://docs.influxdata.com/influxdb/v1.3/concepts/storage_engine/)，\nleveldb, rocksdb, boltdb 都玩了个遍，最后决定自己造个轮子叫 `Time Structured Merge Tree`。\n\n`Time Structured Merge Tree` (TSM) 和 `Log Structured Merge Tree` (LSM) 的名字都有点误导性，关键并不是树，也不是日志或者时间，而是 `Merge`。\n写入的时候，数据先写入到内存里，之后批量写入到硬盘。读的时候，同时读内存和硬盘然后合并结果。\n删除的时候，写入一个删除标记，被标记的数据在读取时不会被返回。\n后台会把小的块合并成大的块，此时被标记删除的数据才真正被删除，这个过程叫做 `Compaction`。\n相对于普通数据，有规律的时间序列数据在合并的过程中可以极大的提高压缩比。\n\n下图是一个简化版的 TSM，每个块包含序列标识符，一组时间戳，一组值。\n注意时间戳和值是分开存储的，而不是交替存储的，所以 InflxuDB 是一个列存储。\nInfluxDB 会根据数据来选择压缩的方法，如果可以使用行程编码是最好的，\n否则会使用 [Gorilla]((https://github.com/dgryski/go-tsz)) 中提到的浮点数压缩方法以及变长编码。\n时间戳和数值一般会使用不同的压缩方法，因为时间戳大多是非常大的整数而数值是非常小的浮点数。\n\n````\nchunk\n--------------------------------------------------\n| id | compressed timestamps | compressed values |\n--------------------------------------------------\ntsm file\n-------------------------------------------------------------------\n| header | chunk 0 | chunk 1 | ... | chunk 10086 | index | footer |\n-------------------------------------------------------------------\n````\n\n## 热点话题\n\n### 低延迟\n\n时间序列数据库主要是用来分析的，所以提高响应速度对于诊断生产环境的问题是十分重要的。\n\n最直接的提速方法就是把所有数据都放在内存，Facebook 写了叫 [Gorilla](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf) 的纯内存时间序列数据库发表在 VLDB 上，现在已经开源，改名为 [Beringei](https://github.com/facebookincubator/beringei)（都是猩猩...）。\n\n另一种提速的方法是提前聚合。因为查询中经常需要对一个很长的时间区间取一些粗粒度的值，比如`6月到8月每天的平均CPU使用率`。\n这些聚合值（均值，最大，最小) 都可以在存储数据的时候计算出来。\n[BtrDB](https://github.com/SoftwareDefinedBuildings/btrdb) 和 [Akumuli](https://github.com/akumuli/Akumuli)\n都在内部节点中存储聚合值，这样在很多查询中底层的节点不需要被访问就可以得到结果。\n\n同时一个好的数据传输格式也可以提高响应速度，虽然 JSON 被广泛使用，但是二进制的格式对于有大量数字的数据会显著的提升。\n[protobuf](https://github.com/golang/protobuf/) 可能会是一个更好的选择。\n\n### 处理旧数据\n\n很多时间序列数据都没有多大用处，特别是当系统长时间正常运行时，完整的历史数据意义并不大。\n所以有些数据库比如 [RDDTool](https://oss.oetiker.ch/rrdtool/) 和 [Graphite](https://graphiteapp.org/) 会自动删除高精度的数据，只保留低精度的。\n但是对于很多新的时间序列数据库，在聚合和删除大量旧数据的同时保证系统正常运行并不像删除一个本地文件那样简单。\n如果监控系统比被监控系统还不稳定就比较尴尬了。\n\n### 元数据索引\n\n时间序列的标识符是时间序列数据库里主要的元数据。\n[Heroic](https://github.com/spotify/heroic) 使用 Elasticsearch 来存储元数据，\n查询首先通过 Elasticsearch 来取得符合要求的序列标识符，之后从 Cassandra 根据标识符来读取对应的数据。\n但是维护一个完整的搜索引擎带来的运维压力和增加的通信时间都是不能忽视的。\n因此 InfluxDB 和 Prometheus 就[自己写了倒排索引]((https://fabxc.org/blog/2017-04-10-writing-a-tsdb/))来索引元数据。\n\n### Tracing\n\nInfluxDB 的人写了一篇博客 [Metrics are dead](https://www.influxdata.com/blog/metrics-are-dead/)，\n起因是在一个关于监控的会议 [Monitorama](http://monitorama.com/) 上有人说单纯的监控数据已经不能满足他们复杂的微服务架构了。\n于是 InfluxDB 的人反驳说并不是所有人都在使用大规模的分布式系统，对于很多简单的应用单纯的监控数据已经完全够用了。\n我的看法是**时间序列数据库是可以用来存 Trace 的**。\nTrace 是更加复杂的时间序列数据，把单纯的数值变成一个包含更多信息的对象，它就是一个 Trace。\n并且很多流行的 Tracer 的存储也是使用 Cassandra, 比如 [Zipkin](https://github.com/openzipkin/zipkin)，\nUber 的 [Jaeger](https://uber.github.io/jaeger/)。**更新:** InfluxDB 现在[已经支持存储 Trace 了](https://www.influxdata.com/blog/tracing-the-journey-of-a-transaction-as-it-propagates-through-a-distributed-system/)\n\n由于篇幅限制，有很多话题我们没有涉及，比如压缩，Pull vs Push, 写放大等，在以后的博客中会陆续介绍。\n\n## 参考\n\n- [Awesome Time Series Database](https://github.com/xephonhq/awesome-time-series-database)\n- [Akumuli](https://github.com/akumuli/Akumuli)\n- [Beringei](https://github.com/facebookincubator/beringei)\n- [BtrDB](https://github.com/SoftwareDefinedBuildings/btrdb)\n- [Gorilla](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf)\n- [Grafana](https://github.com/grafana/grafana)\n- [Graphite](https://graphiteapp.org/)\n- [Heroic](https://github.com/spotify/heroic)\n- [InfluxDB](https://github.com/influxdata/influxdb)\n- [Jaeger - Tracer](https://uber.github.io/jaeger/)\n- [KairosDB](https://kairosdb.github.io/)\n- [OpenTSDB](http://opentsdb.net/)\n- [Prometheus](https://prometheus.io/)\n- [RRDTool](https://oss.oetiker.ch/rrdtool/)\n- [Timescale - TSDB using Postgres](http://www.timescale.com/)\n- [VividCortex - TSDB using MySQL](https://www.vividcortex.com/blog/2014/12/16/in-case-you-missed-it-building-a-time-series-database-in-mysql/)\n- [Zipkin - Tracer](https://github.com/openzipkin/zipkin)\n\n## License\n\n- This article is licensed under [CC BY-NC-SA 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/).\n- Please contact \u003cmarketing@dongyue.io\u003e for commerical use.\n","cover":"","link":"时间序列数据库漫谈.html","preview":"\u003cp\u003e时间序列数据库基本概念和热点话题\u003c/p\u003e\n","title":"时间序列数据库漫谈"},{"content":"\n\nThis blog is written for fellow students at [dongyueweb](https://github.com/dyweb),\nso its targeted readers are people who have taken database class and want to know about time series database (TSDB).\n\nTable of content\n\n- What is time series database (TSDB)\n- Time series data model\n- Evolve of time series database\n- Types of time series database\n  - KairosDB\n  - InfluxDB\n- Hot topics in time series database\n  - Fast response\n  - Retention\n  - Meta data indexing\n  - Tracing\n\n## What is time series database (TSDB)\n\nTime series database (TSDB) is relative new compared with RDBMS, NoSQL, even NewSQL.\nHowever it is becoming trending with the growth of system monitoring and internet of things.\nThe [wiki](https://en.wikipedia.org/wiki/Time_series) definition of time series data is *a series of data points indexed (or listed or graphed) in time order*. When it comes to TSDB, I prefer my own definition: **store client history in server for analysis**.\nTime series data is history, it's **immutable**, **unique** and **sortable**. \nFor instance, `the CPU usage at 2017-09-03-21:24:44 is 10.02% for machine-01 in us-east region`, \nit won't change overtime like bank account balance, there will be no update once it's generated, \nthe CPU usage at next second, or from different machine are different data points. \nAnd **the order of data arriving at server does not effect correctness** because you can remove the duplicate and sort by client timestamp.\nClients of TSDB send their history to sever and is still functional when the server is down, \n**sending data to TSDB is not critical for many clients**;\nA http server's main job is serving content instead of reporting status code to TSDB.\nHowever, RDBMS is treated as single source of truth and effect client's critical decision making. \nThis lead to very different read and write pattern. \nFor instance, banking application need to query database for user's balance before proceed by reading and updating a single record.\nBut most TSDB clients are either write only (collectors) or read only (dashboard and alerting system). \nAnd when they read, they read in large batch, `show CPU usage of last 1h` is used more often than `show CPU usage at 2017-09-03-21:24:44` \nbecause time series data is not that useful without its context.\n\nTime series data is so different from what popular DBMS used to deal with that people are forced to use their favorite DB in very different ways (i.e. [VividCortex with MySQL](https://www.vividcortex.com/blog/2014/12/16/in-case-you-missed-it-building-a-time-series-database-in-mysql/), [Timescale with Postgres](http://www.timescale.com/)). \nSome decided for special problem special solution is needed, so many TSDBs are written from scratch ([Graphite](https://graphiteapp.org/), \n[InfluxDB](https://github.com/influxdata/influxdb) etc.) without dependencies to existing databases.\n\n## Evolve of time series database\n\nThere are [too many time series databases](https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=All),\nso I just list databases that I personally considered as milestone in the evolving of time series database, \nfeel free to comment the pieces I missed, I can't find the real initial release of many databases so I just use the oldest on github.\n\n- 1999/07/16 [RRDTool First release](https://en.wikipedia.org/wiki/RRDtool)\n- 2009/12/30 [Graphite 0.9.5](https://github.com/graphite-project/graphite-web/releases/tag/0.9.5)\n- 2011/12/23 [OpenTSDB 1.0.0](https://github.com/OpenTSDB/opentsdb/releases/tag/v1.0.0)\n- 2013/05/24 [KairosDB 1.0.0-beta](https://github.com/kairosdb/kairosdb/releases/tag/v1.0.0-beta2a)\n- 2013/10/24 [InfluxDB 0.0.1](https://github.com/influxdata/influxdb/releases/tag/v0.0.1)\n- 2014/08/25 [Heroic 0.3.0](https://github.com/spotify/heroic/releases/tag/0.3.0)\n- 2017/03/27 [TimescaleDB 0.0.1-beta](https://github.com/timescale/timescaledb/releases/tag/0.0.1-beta)\n\n[RRDTool](https://oss.oetiker.ch/rrdtool/) was created to graph network traffic, it ships with graphing tool while modern TSDB normally depends on [Grafana](https://github.com/grafana/grafana) for graphing. \n[Graphite](https://graphiteapp.org/) was created later using python instead of C like RRDTool, its storage engine is called [Whisper](https://github.com/graphite-project/whisper), it's much powerful when it comes to data processing and query, however it does not scale well.\n[OpenTSDB](http://opentsdb.net/) from Yahoo! solves the scale problem by using HBase.\n[KairosDB](https://kairosdb.github.io/) was a fork for OpenTSDB to support Cassandra as an alternative backend, but then they found being compatible with HBase limit the potential of Cassandra, so they dropped HBase and use Cassandra only. \nIronically, [recent release of OpenTSDB](http://opentsdb.net/docs/build/html/new.html) added support for Cassandra.\nThen [Heroic](https://github.com/spotify/heroic) came out because they are [not satisfied with KairosDB's performance and direction](https://labs.spotify.com/2015/11/16/monitoring-at-spotify-the-story-so-far/).\n[InfluxDB](https://github.com/influxdata/influxdb) started with full open source, \nbut then close sourced their cluster version because they need to keep the company running, there is a interesting talk called [The Open Source Database Business Model is Under Siege](https://www.youtube.com/watch?v=Kvf5jWZjw0U) during Percona Live which features [a time series session](https://www.percona.com/live/17/program/schedule/time-series).\n[TimeScaleDB](http://www.timescale.com/) is based on PostgreSQL with a plugin instead of special schema. \n\n## Time series data model\n\nTime series data can be split into two parts, **series** and **data points**.\nSeries is the identifier, like `CPU usage for machine-01 in us-east region`, \ndata points are an array of points where each point is a timestamp and value.\n\nFor series, the main goal is the extensibility for post processing (searching, filtering etc.).\ni.e. If you want `CPU usage of all machines in us-east region`,\nthe identifier of series `CPU usage for machine-01 in us-east region` is `name=cpu.usage machine=machine-01 region=us-east`, \nand the query becomes `name=cpu.usage machine=* region=us-east`.\nIt order to deal with large amount of series and wildcard matching, (inverted) index is needed,\nsome chose to use external search engine like [Heroic](https://github.com/spotify/heroic) is using Elasticsearch.\nSome chose to write their own like [InfluxDB](https://github.com/influxdata/influxdb), [Prometheus](https://prometheus.io/).\n\nFor data points there are two models, an array of points `[{t: 2017-09-03-21:24:44, v: 0.1002}, {t: 2017-09-03-21:24:45, v: 0.1012}]` \nor two arrays for timestamp and values respectively `[2017-09-03-21:24:44, 2017-09-03-21:24:45], [0.1002, 0.1012]`.\nThe former is row store, the latter is column store (not to be confused with column family).\nWhen building TSDB on top of existing databases ([Cassandra](https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=Cassandra), [HBase](https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=HBase) etc.), the former is used more,\nwhile for TSDB written from scratch, the latter is more popular, TSDB is actually a subset of OLAP and columnar format brings higher compression ratio and query speed.\n\n## Types of time series databases\n\nTime series databases can be split into two types, existing databases with time series specific special schema or databases built for time series data from scratch. \nWe use KairosDB and InfluxDB as example for following discussion. \nA lot of TSDB are [built on top of Cassandra](https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=Cassandra), \n[KairosDB](https://kairosdb.github.io/) is the pioneer of them.\n[InfluxDB](https://github.com/influxdata/influxdb) has tried many backends until they came up with their `Time Structured Merge Tree`.\n\n### KairosDB\n\nBefore dive into KairosDB, let's warm up using a simplified version called [Xephon-K](https://github.com/xephonhq/xephon-k).\n[Xephon-K](https://github.com/xephonhq/xephon-k) is a multi backend time series database I wrote for testing out different mechanism of building TSDB. Its immature Cassandra backend is simple and modeled after [KairosDB](https://kairosdb.github.io/).\n\nIf you are not familiar with Cassandra, here is a brief introduction.\nCassandra (C*) is a column family NoSQL database modeled after BigTable, people sometimes call it **wide column**. \nYou can think column family as a map of map of map. It's a row store, not a column store.\nWe can match some concept of Cassandra with RDBMS's.\n`Keyspace` in C* is database in RDBMS, i.e. your blog and ecommerce application use same MySQL Server but create different database for isolation.\n`Table` in C* is a map and `Partition Key` is its key, also known as (physical) row key, which is used to partition data to different nodes.\nThe value of the top level map is also map, and its key is the `Cluster key` (column), its value is also a map.\nWhen creating a table in CQL, the first column in primary key is partition key and the second is cluster key. i.e. In the following CQL, \n`Keyspace` is `naive`, `Table` is `metrics`, `Partition Key` is `metric_name`, `Cluster Key` is `metrics_timestamp`, \nthe inner most map is `{value: 10.2}`, we can have more than one keys for it if needed, i.e. `{value: 10.2, annotation: 'new app deployed'}`\n\n````sql\nCREATE TABLE IF NOT EXISTS naive.metrics (\n    metric_name text, metric_timestamp timestamp, value int, \n    PRIMARY KEY (metric_name, metric_timestamp))\nINSERT INTO naive.metrics (metric_name, metric_timestamp, value) VALUES (cpu, 2017/03/17:13:24:00:20, 10.2)    \nINSERT INTO naive.metrics (metric_name, metric_timestamp, value) VALUES (mem, 2017/03/17:13:24:00:20, 80.3)   \n````\n\n![Cassandra Time Series Data model](images/posts/cassandra-tsdb-model.png)\n\nThe figure above shows a naive schema when using Cassandra to store time series data, \n`Cluster key` is used to store timestamp and column value is the actual value.\nIt is naive because series and Cassandra's physical row is a one-to-one mapping, it won't scale when a single series grows larger than the hard limit of Cassandra (2 billion columns). \n \nA more mature schema would partition a single series by time range (might not be fixed, KairosDB use fixed 3 week time range) into several physical rows, an extra table is needed to keep this partition info. \nAlso the series name in naive schema is just a simple string, in order to filter series by different criteria, attributes (tags) need to be stored, and another table as index is needed to avoid iterate all the series. \n\nKairosDB's schema is listed below, the `data_points` table is same as `metrics` table in naive schema except `key` is \nnot for human like `metric_name` does. The naming of schema looks strange because it is dumped from Cassandra's shell (cqlsh), \nKairosDB didn't use a cql file to create schema like many other does because it was using the old thrift API.\n\n````sql\nCREATE TABLE IF NOT EXISTS data_points (\n    key blob,\n    column1 blob,\n    value blob,\n    PRIMARY KEY ((key), column1)\n) WITH COMPACT STORAGE;\nCREATE TABLE IF NOT EXISTS row_key_index (\n    key blob,\n    column1 blob,\n    value blob,\n    PRIMARY KEY ((key), column1)\n) WITH COMPACT STORAGE;\nCREATE TABLE IF NOT EXISTS row_key_time_index (\n    metric text,\n    row_time timestamp,\n    value text,\n    PRIMARY KEY ((metric), row_time)\n)\nCREATE TABLE IF NOT EXISTS row_keys (\n    metric text,\n    row_time timestamp,\n    data_type text,\n    tags frozen\u003cmap\u003ctext, text\u003e\u003e,\n    value text,\n    PRIMARY KEY ((metric, row_time), data_type, tags)\n)\nCREATE TABLE IF NOT EXISTS string_index (\n    key blob,\n    column1 blob,\n    value blob,\n    PRIMARY KEY ((key), column1)\n) WITH COMPACT STORAGE\n````\n\nThere are many more Cassandra based time series databases, they share very similar schema, you can find in [awesome time series database](https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=Cassandra). I am writing a new blog for more detailed survey on TSDB using Cassandra and how to write your own in Golang, I will update the link here once it's finished.\n\n### InfluxDB\n\n[InfluxDB](https://github.com/influxdata/influxdb) has [struggled a long time for their storage engine](https://docs.influxdata.com/influxdb/v1.3/concepts/storage_engine/) (leveldb, rocksdb, boltdb) before they settled with their time structured merge tree (TSM Tree). It can be separate into two parts, index for series identifiers and store for data points, we only focus on data points.\n\nTime structure merge tree (TSM), is a little bit misleading as log structured merge tree (LSM). \nThe key concept for both TSM and LSM is nor log or tree or time,\nit's **merge**. When write, data is stored in memory and then flushed to disk in large batch. When read, first read from memory, then read from disk and merge the result. When delete, a tombstone is added, and data with tombstone is not returned when read. In background, small chunks are merged into big chunks and items marked as deleted are truly removed to save disk space and speed up future query, this background procedure is called compaction. For time series data, compaction may increase compression ratio a lot for very regular data.\n\nA simplified version of TSM file is illustrated below, each chunk contains the series identifier, timestamps and values. \nNote that timestamps and values are stored separately instead of interleaved, which is why InfluxDB say they are using column format.\nInfluxDB use adaptive compression for data, it will loop through the data to see if it can be run length encoded, otherwise fallback to \n[Gorilla's](https://github.com/dgryski/go-tsz) or variable length encoding. Timestamps and value use different compression codec because\ntimestamps are normally very big integers (unix timestamp in millisecond or nanosecond) while value are normally small integer or float.\n\n````\nchunk\n--------------------------------------------------\n| id | compressed timestamps | compressed values |\n--------------------------------------------------\ntsm file\n-------------------------------------------------------------------\n| header | chunk 0 | chunk 1 | ... | chunk 10086 | index | footer |\n-------------------------------------------------------------------\n````\n\n## Hot topics in Time series databases\n\n### Fast response\n\nTime series database is used for analysis, and people don't want to wait in front of dashboard when production system is failing and \nuser's complain phone coming in, so fast response is a base requirement for any production ready time series database.\n\nThe most straight forward way is to put data into memory as much as possible.\nFacebook built [Gorilla](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf), now open sourced as [Beringei](https://github.com/facebookincubator/beringei), \nand its main contribution is using time series specific compression to store more data in memory.\n\nAnother way for speed up is pre-aggregation, also known as roll up. Because query often involve a long time range with coarse granularity, like\n`average daily cpu usage from June 1 to Aug 1`, those aggregations (average, min, max) can be computed when ingesting data, [BtrDB](https://github.com/SoftwareDefinedBuildings/btrdb) and [Akumuli](https://github.com/akumuli/Akumuli) store aggregation in upper level tree nodes so fine grained data won't be loaded when query is coarse grained.\n\nA proper ingest format could also reduce response time for both read and write, JSON is widely used, but Binary format is much better than textual format when a lot of number is involved, [protobuf](https://github.com/golang/protobuf/) could be a good choice.\n\n\u003c!-- ### Compression --\u003e\n\n### Retention\n\nNot all time series data is useful all time, if the system has been working well for the last two month, fine grained data can be dropped and only coarse grained is kept. \nThis is the default behavior of [RDDTool](https://oss.oetiker.ch/rrdtool/) and [Graphite](https://graphiteapp.org/), \nbut not the case for many newer scaled TSDB.\nDelete a file on local disk is easy but update a large amount of data in a distributed environment requires more caution to keep the system up all time, you don't want your monitoring systems failed before the system it is monitoring failed.\n\n\u003c!-- Also should the aggregated data get computed when data comes in or  --\u003e\n\n### Meta data indexing\n\nSeries identifier in general is the only meta data in time series database.\nDatabases like [Heroic](https://github.com/spotify/heroic) use ElasticSearch to store meta data, \nquery first goes to elasticsearch to retrieve the id for series, then data is loaded from Cassandra using id.\nA full search engine as Elasticsearch is powerful for sure, but the overhead of maintain another system and time spent\ncoordinating and communicating between two system can't be ignored. \nAlso some TSDB specific optimization may not be available when you don't have full control over metadata index building and storage.\nSo InfluxDB and Prometheus [wrote their own inverted index for indexing meta data](https://fabxc.org/blog/2017-04-10-writing-a-tsdb/).\n\n\u003c!-- ### Reduce write amplification --\u003e\n\u003c!-- ### Streaming --\u003e\n\u003c!-- Streaming has been hot for a long time, you must have heard Storm, Spark Streaming, Kafka etc. --\u003e\n\n### Tracing\n\nFolks from InfluxDB wrote a blog called [Metrics are dead](https://www.influxdata.com/blog/metrics-are-dead/) \nbecause during a conference for monitoring called [Monitorama](http://monitorama.com/) people say metrics can't provide enough insight as tracing does. \n(You can go to [OpenTracing](http://opentracing.io/) if you want to know more about tracing, and take a look at google's [Dapper paper](https://research.google.com/pubs/pub36356.html))\nTheir argument is tracing is for large scale distributed system, but there are many monolithic applications where metrics is enough (so metrics is not dead and you should use InfluxDB).\nI agree with them on the over emphasis of micro services, however my argument is **many time series database can be transfered into a tracing database**. \nTrace is a complex version of time series data points, \nif your value in a point is no longer a float value but a json payload with fields like parent span id, duration, it is a trace. \nOf course schema design, compression all need a lot of change, but many popular tracing solution like [Zipkin](https://github.com/openzipkin/zipkin)\n, Uber's [Jaeger](https://uber.github.io/jaeger/) is also using Cassandra like many TSDB do, there could be a middle ground.\n**Update:** InfluxDB already [tried to integrate Zipkin with their TICK stack](https://www.influxdata.com/blog/tracing-the-journey-of-a-transaction-as-it-propagates-through-a-distributed-system/)\nI spent too much time writing this blog.\n\nBecause the length of the blog we can't cover other hot topics like Compression, Pull vs Push, Streaming, Reduce write amplification etc,\nthey will be covered in future blogs.\n\n## Reference\n\n- [Awesome Time Series Database](https://github.com/xephonhq/awesome-time-series-database)\n- [Akumuli](https://github.com/akumuli/Akumuli)\n- [Beringei](https://github.com/facebookincubator/beringei)\n- [BtrDB](https://github.com/SoftwareDefinedBuildings/btrdb)\n- [Gorilla](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf)\n- [Grafana](https://github.com/grafana/grafana)\n- [Graphite](https://graphiteapp.org/)\n- [Heroic](https://github.com/spotify/heroic)\n- [InfluxDB](https://github.com/influxdata/influxdb)\n- [Jaeger - Tracer](https://uber.github.io/jaeger/)\n- [KairosDB](https://kairosdb.github.io/)\n- [OpenTSDB](http://opentsdb.net/)\n- [Prometheus](https://prometheus.io/)\n- [RRDTool](https://oss.oetiker.ch/rrdtool/)\n- [Timescale - TSDB using Postgres](http://www.timescale.com/)\n- [VividCortex - TSDB using MySQL](https://www.vividcortex.com/blog/2014/12/16/in-case-you-missed-it-building-a-time-series-database-in-mysql/)\n- [Zipkin - Tracer](https://github.com/openzipkin/zipkin)\n\n## License\n\n- This article is licensed under [CC BY-NC-SA 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/).\n- Please contact \u003cmarketing@dongyue.io\u003e for commerical use.\n","cover":"","link":"introduction_to_time_series_database.html","preview":"\u003cp\u003eAn introduction to time series database basic concepts and hot topics\u003c/p\u003e\n","title":"Introduction to Time Series Database"},{"content":"\n\n#### 转载自 [gaocegege 的博客](http://gaocegege.com/Blog/%E9%98%85%E8%AF%BB/sre-0)\n\n## SRE 介绍\n\nSRE，全称是 Site Reilability Engineer，是一个类似于运维，但是跟传统运维不一样的职业，更加偏向于 DevOps。谷歌在 [SRE-谷歌运维解密](https://book.douban.com/subject/26875239/) 一书中分享了 SRE 的工作职责，以及谷歌在自己的运维工作中的一些经验。\n\n## 本文介绍\n\n这篇博客是系列文章中的第一篇，主要分享在阅读这本书时的一些感想。这本书在我看来更加适合在分布式领域或者在运维领域工作的工程师阅读，对于一个还在念书，没有完整接触过分布式系统实现的新手来说，有些过早了。因此就当是抛砖引玉，随便写写吧。\n\n这次关注的是书中的第六章，分布式系统的监控。\n\n## 关于作者\n\n第六章的作者是 [Rob Ewaschuk](https://www.linkedin.com/in/robewaschuk)。作者主要工作的领域是分布式存储，而且在自我介绍中写道自己在谷歌干的很过瘾，16-17年是不打算换工作的。O'Reilly 摘录了他在 SRE 一书中关于分布式监控的部分，做了一本电子书 [Monitoring Distributed Systems](http://www.oreilly.com/webops-perf/free/monitoring-distributed-systems.csp)。\n\n## 阅读之前\n\n在读文章之前，我对监控的了解非常浅薄。因为无论是在学校还是在之前实习，都没有涉及到对生产系统进行监控的工作。在念了研究生之后，稍微了解了一些关于分布式监控的知识。[Dapper, a Large-Scale Distributed Systems Tracing Infrastructure](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/36356.pdf) 是谷歌在 2010 年发表的论文，是关于其内部的分布式 tracing 系统的一个介绍性的论文。Tracing 在我的理解是细粒度监控中很关键的一部分。点评开源了一套这样的系统 [CAT](https://github.com/dianping/cat)。这些系统的作用就是跟踪系统相互之间的调用。比如 Web 前端调用了后端，就会生成一个从前端指向后端的 trace 记录。业界比较常见的实现是埋点或者修改字节码，前者更加可行。在阅读之前，对于分布式监控的了解也就仅限于此了。\n\n虽说读过相关论文，但是并没有真实使用过，最多就是去过点评，见过点评的 CAT 的 dashboard 是长什么样子的。\n\n## 正文\n\n全文中提到的一些东西让我非常感兴趣。其中有一句话：\n\n\u003e我们会避免任何『魔法』系统--例如视图自动学习阈值或者自动检测故障原因的系统。\n\n之前在去大众点评学习 CAT 系统时，听他们说下一步发展规划中，就有利用机器学习来学习阈值和原因的想法。我认为谷歌在为什么要保持监控系统简单时没有说清楚，这可能是跟他们的监控规模和信奉的哲学有关。他们把这类复杂的有各种特性的系统称为『魔法』系统，因为我也没有什么发言权。但是在我看来，随着复杂性的上升，引入机器学习等等是自动化的新阶段。现在可能人工的方式或者硬编码等等方式还是可以操作的，可能谷歌考虑到监控系统要尽可能稳定吧。但是机器学习可以更好地取代人工，就像在容量规划方面，我始终认为机器学习会比经验估计的更准。\n\n书中写了四个谷歌认为的黄金监控指标，分别是延迟、流量、错误和饱和度。对于延迟，他们提到的一点对我来说特别具有启发性，那就是要区分成功请求和错误请求的延迟。这两类请求有着不同的模式，是不能混为一谈的。之前用过的少数几个监控的工具都没有区分正确与错误请求的能力。这一点是在看了这本书后才学到的。\n\n还有一个比较有趣的指标，是饱和度。饱和度是指服务容量有多满，一般是用瓶颈资源的使用率来衡量。这样衡量饱和度的方式很取巧，之前没有过工程经验，都是各种指标全看一遍，最后看哪个资源不够用了，就断定服务满载了。如果事先判断好是 Memory-bound 还是 CPU-bound 类型的服务，然后每次只需要看对应的瓶颈资源就好了。\n\n关于长尾问题，谷歌给出了一种监控的方法，使用直方分布图而不是平均值来进行展示。因为可能一小部分请求导致了长尾，但是平均值是看不出这个问题的。\n\n在监控系统构建后，有一个值得考虑的问题，是短期可用性与长期可用性的冲突。短期的可用性体现在对问题的及时修复上，而长期的可用性在于对系统造成问题的根源的消除上。看起来这两者是统一的，但是其实是冲突的。人的精力是有限的，如果一直在处理 On-Call 的问题，那必然会导致缺少时间投入到根源性问题的解决上，这时需要权衡，放弃一些 On-Call 非核心的问题，去优化系统，提高长期预期的可用性。\n\n## 下文预告\n\n下一篇文章将会谈谈有关发布工程（Releasing Engineering）的事情。\n\n## 系列文章\n\n* [Google SRE 阅读笔记(1)-监控](http://blog.dongyueweb.com/google_sre_%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%281%29-%E7%9B%91%E6%8E%A7.html)\n\n## License\n\n- This article is licensed under [CC BY-NC-SA 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/).\n- Please contact \u003cmarketing@dongyue.io\u003e for commerical use.\n","cover":"","link":"google_sre_阅读笔记(1)-监控.html","preview":"\u003cp\u003eSRE，全称是 Site Reilability Engineer，是一个类似于运维，但是跟传统运维不一样的职业，更加偏向于 DevOps。谷歌在 \u003ca href=\"https://book.douban.com/subject/26875239/\"\u003eSRE-谷歌运维解密\u003c/a\u003e 一书中分享了 SRE 的工作职责，以及谷歌在自己的运维工作中的一些经验。\u003c/p\u003e\n","title":"Google SRE 阅读笔记(1)-监控"},{"content":"\n\n今天为东岳搭建了一个饥荒的服务器，并不是特别复杂。饥荒对于服务器的要求是：\n\n```text\nInternet(Upload) = 8Kbytes/player/s\nRam = around 65Mbytes/player\nCPU = N/A\nVCRedist_2008 (x86)\n```\n\n因此选定配置的时候要计算下，服务器的最低配置要求。因为考虑到我们的玩家数最多也就20人左右，长期在线人数能在3-4人就不错了，因此一台1核2G内存的机器就可以满足我们的要求了。\n\n我们中的绝大多数玩家，都是在华东地区的，而只有一个美帝玩家。因此在服务器的选择上，华东节点是最合适的。在考察了包括阿里云、美团云、青云、腾讯云、Hyper.sh 在内的众多云服务提供商后，选择了最便宜的腾讯云。就流量来说，基本所有的服务商都是一个价钱，但是服务器的价格从 85 到 125 不等。Hyper.sh 因为没有华东节点，就没有关注价格。因为 steam 的 cmd 运行需要 32 位的环境，而且服务器的内存没有超过 4G，因此选择了 32位 Ubuntu 16.04.1 LTS。因为选择的云服务提供商和系统都很大众，因此在过程中并没有遇到什么坑。\n\n## 安装 steam 和 饥荒\n\n按照官方的文章，没什么好说的，不过为了简单，在搭建的过程中省略了创建用户的过程，直接在默认的用户目录下进行的。还有就是需要安装两个在官方教程中没有写到的东西：xfonts-75dpi 和 xfonts-100dpi，不然在运行 steamcmd.sh 的时候会报错 `Steam needs to be online to update`。\n\n```bash\nsudo apt-get install libgcc1\nsudo apt-get install xfonts-75dpi xfonts-100dpi\nmkdir ~/steamcmd\ncd ~/steamcmd\nwget https://steamcdn-a.akamaihd.net/client/installer/steamcmd_linux.tar.gz\ntar -xvzf steamcmd_linux.tar.gz\n./steamcmd.sh\nlogin anonymous\n# replace \u003cuser\u003e with your current user. if you use qcloud, ubuntu is the default username.\nforce_install_dir /home/\u003cuser\u003e/steamapps/DST\napp_update 343050 validate\nquit\ncd /home/steam/steamapps/DST/bin/\n```\n\n## 添加配置文件\n\n至此游戏服务器的所有二进制和依赖都安装好了，接下来需要进行配置。在 `/home/\u003cuser\u003e/.klei/DoNotStarveTogether/Cluster_1` 目录下需要建立两个文件，cluster.ini 和 cluster_token.txt。前者是对服务器的配置，后者是在饥荒的客户端游戏中生成的一个 token，猜测会用来校验玩家是否在使用正版游戏，等等。\n\ncluster.ini 文件内容很简单：\n\n```text\n[network]\ncluster_name = \u003ccluster_name\u003e\ncluster_intention = cooperative\ncluster_description = \u003ccluster_description\u003e\ncluster_port = 10999\ncluster_password = \u003cpasswd\u003e\n\n[misc]\nconsole_enabled = true\n\n[gameplay]\nmax_players = \u003cmax_players_num\u003e\npvp = false\ngame_mode = endless\npause_when_empty = true\n```\n\ncluster_token.txt 文件的内容需要用饥荒的客户端来生成，输入 `~` 打开游戏内置的 console，输入 `TheNet:GenerateClusterToken()`，不同系统会在不同位置生成一个 token：\n\n```text\nWindows:\n/My Documents/Klei/DoNotStarveTogether/cluster_token.txt\n\nLinux:\n ~/.klei/DoNotStarveTogether/cluster_token.txt\n\nMac OS X:\n~/Documents/Klei/DoNotStarveTogether/cluster_token.txt\n```\n\n然后将文件内容拷贝到 `/home/\u003cuser\u003e/.klei/DoNotStarveTogether/Cluster_1/cluster_token.txt` 中就行。\n\n## 运行\n\n```bash\n/home/\u003cuser\u003e/steamapps/DST/bin/dontstarve_dedicated_server_nullrenderer\n```\n\n官方推荐使用 screen 来维持服务器在退出 ssh 连接后依然在运行，但你喜欢怎么做就随便了。\n\n## Reference\n\n* [Don’t Starve Together（饥荒）服务器搭建](https://www.nevermoe.com/?p=695)\n\n## License\n\n- This article is licensed under [CC BY-NC-SA 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/).\n- Please contact \u003cmarketing@dongyue.io\u003e for commerical use.\n","cover":"","link":"在32位_ubuntu_16.04.1_lts_上安装饥荒服务器.html","preview":"\u003cp\u003e今天为东岳搭建了一个饥荒的服务器，并不是特别复杂。\u003c/p\u003e\n","title":"在32位 Ubuntu 16.04.1 LTS 上安装饥荒服务器"},{"content":"\n\n## 简介\n\n[Ayi](https://github.com/dyweb/Ayi) 是一个跨平台的命令行工具，类似于[busybox](https://busybox.net/about.html)。\n开始于 2015 年 7 月。主要目的是为了方便配置环境和解决各种由于配置环境导致的问题，比如:\n\n\u003e - 我这里跑的好好的，怎么到了你那(服务器上)就挂了\n\u003e - 我用 Mac 自带的 PHP 和 Apache 就挺好，我不用 Vagrant 和 Nginx\n\u003e - 我就想用 Windows 下的一键安装包\n\n考虑到没钱给大家每人配个 Mac，以及东岳的男女比例。\n我们需要一个跨平台的配置环境和收集环境信息的工具，用于**快速**的解决上述问题。\n\n## 技术选型\n\n在选择 Ayi 使用的技术时主要考虑的是以下几个问题\n\n- 跨平台\n- 可维护性\n- 对于东岳其他项目的帮助\n\n\u003c!-- TODO:找不到是哪个 issue 了，倒是找到了 commit https://github.com/dyweb/Ayi/commit/3a96921ccb6b5edb7c294e2a1eab2b9e63cc130b --\u003e\n最开始和咩的考虑是使用 shell 来进行操作， 但是 shell 的问题在于很难维护，基本不可能测试。\n东岳 shell 用的很少，并且 shell 对于其他项目帮助十分有限。\n\n之后考虑到 PHP, python, java 都需要运行时，C/C++ 写起来太累， Rust 没人会 (那会还没有 Ivan 和 Codeworm)，\n就选择了 Golang，当时版本是 1.5。\n\nGolang 的主要优点是\n\n- 跨平台 \u0026 交叉编译\n- 简洁的包管理\n- 性能好，可以用来改进东岳现有的纯 PHP 服务端体系\n- 一个活跃的社区，PHP 沉浸在 CMS 和抄 Rails 中不能自拔，JS 日新月异\n- Google 老爹\n\n## 主要问题\n\n- 人太少，基本只有 @at15 (我) 一个人\n- 需求不是很明确\n- 对 Golang 语言本身很不熟悉\n- Golang 的一些工具链不是很成熟，比如不支持依赖的 vendor 。\n\n但是由于项目拖了很长时间，后面三个问题基本都解决了\n\n- 主要需求是\n  - 生成器\n  - 环境检查\n  - [git 操作的简化](https://github.com/dyweb/Ayi/tree/master/app/git)\n  - [makefile 类似的自动化工具](https://github.com/dyweb/Ayi/tree/master/util/runner)\n  - [静态 web 服务器](https://github.com/dyweb/Ayi/tree/master/app/web)\n  - [进程管理](https://github.com/dyweb/Ayi/pull/64)\n  - waka time 服务器\n  - 文件传输\n- go 的版本从 1.5 跳到了 1.7。原生支持 vendor 并且有了很多更好的依赖管理工具，比如 [glide](https://github.com/Masterminds/glide)\n\n第一个问题的话，基本无解，目前东岳经常写 Golang 的人好像只有我和策策。策策有空就要去陪妹子，自然不可能陪我来填坑。\n(要有妹子的话我还会去填坑么?)\n\n## 实现的功能\n\n### Git 操作的简化\n\n前提是：你习惯使用 Golang 的 workspace，有关 workspace 我在以前东岳的讲座中[有提到](http://dongyueweb.com/course/web/2016_Spring/environment/slide.html#/4) (btw: 按方向键`下`而不是`右`)。我个人的工作区是这样的 (`cd ~/workspace \u0026\u0026 tree -L 4`)。\n\n````\n├── bin\n│   ├── Ayi\n│   ├── glide\n│   └── ink\n├── pkg\n│   └── linux_amd64\n│       └── github.com\n│           └── dyweb\n└── src\n    └── github.com\n        ├── at15\n        │   └── at15.github.io\n        ├── dyweb\n        │   ├── Ayi\n        │   └── blog\n        └── xephonhq\n            └── xephon-b\n````\n\n当使用 `git clone` 时后面必须跟完整的 remote 地址，并且默认 clone 到当前文件夹下，而使用\n`Ayi git clone` 地址可以是浏览器地址，并且根据配置文件，可以支持非默认端口的 ssh，比如东岳的 GitLab。\n从下面的输出可以看到 `Ayi git clone github.com/at15/at15.gihub.io` 被展开成了\n`git clone git@github.com:at15/at15.github.io.git /home/at15/workspace/src/github.com/at15/at15.github.io`。\n\n````\nat15@pc4038:~/workspace|⇒  Ayi git clone github.com/at15/at15.github.io\nINFO[0000] git clone git@github.com:at15/at15.github.io.git /home/at15/workspace/src/github.com/at15/at15.github.io pkg=a.a.git\nCloning into '/home/at15/workspace/src/github.com/at15/at15.github.io'...\nremote: Counting objects: 435, done.\nremote: Total 435 (delta 0), reused 0 (delta 0), pack-reused 435\nReceiving objects: 100% (435/435), 3.56 MiB | 1.64 MiB/s, done.\nResolving deltas: 100% (234/234), done.\nChecking connectivity... done.\nINFO[0002] Sucessfully cloned to: /home/at15/workspace/src/github.com/at15/at15.github.io pkg=a.cmd\n````\n\nbtw: `Ayi` 的 log 组件看上去很像 [logrus](https://github.com/sirupsen/logrus)，但其实是[自己的轮子](https://github.com/dyweb/Ayi/pull/60)\n\n### 自动化\n\n自动化部分很类似 `npm run`，但是主要有以下区别\n\n- 使用 yaml 而不是 json, json 不支持注释，而且即使使用支持注释的 parser，编辑器也会有提示\n- 支持一个指令对应一系列命令, 类似 Travis 等 CI 的配置文件\n- 目前[新的重构](https://github.com/dyweb/Ayi/pull/64)可能会把它改成类似 + 的工具\n\n````\ndebug: true\ndep-install:\n    - go get github.com/at15/go.rice/rice\n    - go get github.com/mitchellh/gox\n    - glide install\ninstall:\n    - go build -o Ayi\n    - rice append -i github.com/dyweb/Ayi/app/web --exec Ayi\n    - sh -c \"mv Ayi $GOPATH/bin/Ayi\"\ntest:\n    - go install\n    - sh -c \"go test -v -cover $(glide novendor)\"\nscripts:\n    build: gox -output=\"build/Ayi_{{.OS}}_{{.Arch}}\"\n````\n\n内置指令如`install`, `test` 跟 `Ayi run \u003cscript-name\u003e` 都是使用 `util/runner`。\n目前准备把 runner 做成一个通用的 package，\n因此[又在重构](https://github.com/dyweb/Ayi/pull/64)来增加如下的功能\n\n- 类似 [Ansible](https://www.ansible.com/) 的更丰富的配置\n- 类似[ PM2](http://pm2.keymetrics.io/) 和 [Foreman](https://github.com/ddollar/foreman) 的进程管理\n\n### 静态服务器\n\n双击一个 html 文件多半会看不了，经典的解决方案是 `python -m SimpleHTTPServer \u003cport\u003e`，\n然而 windows 并不预装 py，而且有时候我想侧边栏显示文件树，markdown 高亮，\n遇到学习文件夹自动播放并且在没有插耳机的情况下静音。\n以前自己挖了一个坑 [doc-viewer](https://github.com/at15/doc-viewer) 。\nAyi 里目前只实现了基本的静态服务器 `Ayi web static`（不要被 help 骗了，根本没有 highlight)。\n\n````\n⇒  Ayi web static -h\nserve static file like python's SimpleHTTPServer, support highlight and markdown render inspired by https://github.com/at15/doc-viewer\n\nUsage:\n  Ayi web static [flags]\n\nGlobal Flags:\n      --config string   config file (default is $HOME/.ayi.yaml)\n  -n, --dry-run         show commands to execute\n  -p, --port int        port to listen on (default 3000)\n      --root string     server root folder\n  -v, --verbose         verbose output\n````\n\n## 使用开源库中遇到的问题\n\n虽然我们要站在巨人的肩膀上，但是站的久了就会发现有些巨人其实也有点 low，比如\n\n- 不支持 windows 的 [overall](https://github.com/go-playground/overalls)，[fork](https://github.com/at15/overalls)\n- 不支持 ignore 的 [go.rice](https://github.com/GeertJohan/go.rice), [fork](https://github.com/at15/go.rice/tree/feature/ignore) 和 [issue](https://github.com/GeertJohan/go.rice/issues/83)\n- 不支持 filter 的 [logrus](https://github.com/sirupsen/logrus)，还自带[统计运行时间的 bug](https://github.com/sirupsen/logrus/issues/457)\n\n一些库虽然 star 很高，但是其实如果仔细看代码的话会发现很多问题，同时看别人的代码可以学到一些自己以前忽略的问题，比如 Golang 里 struct 的方法的 thread safe。\n相关的 issue [dyweb/Ayi#59](https://github.com/dyweb/Ayi/issues/59) [at15/go-learning#3](https://github.com/at15/go-learning/issues/3)。\nlogrus 里对应的代码如下，作为**读者的练习**。\n\n\u003c!-- TODO: no highlight --\u003e\n````golang\n// This function is not declared with a pointer value because otherwise\n// race conditions will occur when using multiple goroutines\nfunc (entry Entry) log(level Level, msg string) {\n        var buffer *bytes.Buffer\n\tentry.Time = time.Now()\n\tentry.Level = level\n\tentry.Message = msg\n````\n\n一些(很多)开源库都维护状态都是很不乐观的，上面提到的几个开 PR 和 Feature Request 的 issue\n都是没人鸟的，既然已经看了那么多了，为什么不自己写呢？ 所以就开始造轮子了(其实还是想造轮子)。\n\nbtw: 在使用开源项目的过程中完全没有必要去埋怨作者无视你的各种请求和贡献，换位思考一下，\n你是愿意陪妹子玩一晚上呢，还是愿意改 Gayhub 上某个不认识的人反馈的 bug 呢 (没有妹子的人表示思考不出来，我选择去改 bug)。\n\n## 通用库 (轮子)\n\n自己造轮子有以下几个优点:\n\n- 方便维护\n- 代码风格一致，比如 [spf13](https://github.com/spf13/) 的 [viper](https://github.com/spf13/viper) 和 [cobra](https://github.com/spf13/cobra/)\n- 可以共用很多 code base\n\n当然关键还是程序员的天性，上面的都是借口。\n\nAyi 里抽出来的库有以下几个\n\n### Log\n\nhttps://github.com/dyweb/Ayi/tree/master/common/log 仿照 [logrus](https://github.com/sirupsen/logrus) 实现,\n目标功能类似 log4j ([logback](http://logback.qos.ch/))\n\n有以下几个特点\n\n- 支持类似 log4j 的按照 package 进行 filter，避免了:\n  - 开启 debug 之后大量输出淹没了需要的信息\n  - 为了 debug，把代码里的 debug 改成 info，忘记改回去\n- 支持更多的 Level (你想加个 Hearbreak 什么的 Level 也可以 `log.Hearbreak(\"got a good man card on New Year's Eve\")`)\n- 减少了 lock (不过没做 benchmark)\n- 移除了 logger 上与 logEntry 重复的接口\n\n之后计划\n\n- 改用 generator 生成代码，`Debugf` 和其他所有 `*f` 都只差一个单词，为什么要人写呢 (我就不说我拼写错误然后 painc 了)。\n- 支持 log4j 的 appender, transformer, xml etc.\n\n### Runner\n\n之前在自动化的部分已经基本说过了，所以就不说了(就是想加个标题)。\n\n### Structure\n\nGolang 内置的数据结构少的可怜，作为一个用了3天 python 的人当然要加一点数据结构。\n\n目前实现的有\n\n- [Set](https://github.com/dyweb/Ayi/tree/common-util/runner/common/structure)\n(一开始只有 Contains 没有 Add 用了才发现这个 Set 是 immutable 的)。\n- 没有然后了\n\n### Requests\n\n`net/http` 很好用，但是 `python` 的 `requests` 更简洁，不过这个轮子目前在[另一项目(xephon-b)里](https://github.com/xephonhq/xephon-b/tree/master/pkg/util/requests)\n\nBefore\n\n````golang\nfunc (client *KairosDBHTTPClient) Ping() error {\n\tres, err := http.Get(client.Config.Host.HostURL() + \"/api/v1/version\")\n\tif err != nil {\n\t\tlog.Warn(\"can't get kairosdb version\")\n\t\tlog.Debug(err.Error())\n\t\treturn err\n\t}\n\tdefer res.Body.Close()\n\tresContent, err := ioutil.ReadAll(res.Body)\n\tif err != nil {\n\t\tlog.Warn(\"can't read response body\")\n\t\tlog.Debug(err.Error())\n\t\treturn err\n\t}\n\tvar resData map[string]string\n\tif err := json.Unmarshal(resContent, \u0026resData); err != nil {\n\t\tlog.Warn(\"can't parse json\")\n\t\tlog.Debug(err.Error())\n\t\treturn err\n\t}\n\tlog.Info(\"KairosDB version is \" + resData[\"version\"])\n\treturn nil\n}\n````\n\nAfter\n\n````golang\nfunc (client *KairosDBHTTPClient) Ping() error {\n\tversionURL := client.Config.Host.HostURL() + \"/api/v1/version\"\n\tres, err := requests.GetJSON(versionURL)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"can't reach KairosDB via %s\", versionURL)\n\t}\n\tlog.Info(\"KairosDB version is \" + res[\"version\"])\n\treturn nil\n}\n````\n\n## 开发计划\n\n上面说了那么多，一半都是画饼，可以从 issue 里看最近的进度\n\n- [正在开发的部分](https://github.com/dyweb/Ayi/issues?q=is%3Aopen+is%3Aissue+label%3Aworking)\n- [想做但是被搁置了的 issue](https://github.com/dyweb/Ayi/issues?utf8=%E2%9C%93\u0026q=is%3Aissue%20label%3Abacklog)\n\n~~欢迎感兴趣的女同学联系我! 我的微信是 `uictor`~~\n\n预计等到国内寒假的时候很多坑可以填完了，到时候欢迎假期想了解一下 Golang 的小伙伴来玩，我会加 `help wanted` 和难度的 label。\n\n## 开发人员\n\n[GitHub 传送门](https://github.com/dyweb/Ayi/graphs/contributors)\n\n- 咩在项目开始时提交了一些 shell 脚本，但是由于转到了 Golang 以及咩一向很忙，遂弃婶\n- @kdplus (丘) 参与过 `Ayi check` 的开发，不过那时我 Golang 菜的抠脚，导致丘也在划水。\n- @gaocegege (策策) 因为周报的功能，参与过一段时间的开发，\n引入了`Godep` 交叉编译，不过最后周报的功能并没有投入实用。\n\n## 总结\n\n- 等有钱了，给大家都配 MBP\n- 自己开的坑，不能让别人填 (我去开个找妹子的坑先)\n\n## 杂项\n\n- 使用 `git log -reverse` 可以反过来看 log, 可以用来找第一个提交。\n- shell 在 windows 下基本不会有问题，因为为了使用 git，东岳所有的 windows 用户都安装了\nmsysgit (现在叫 git for windows)，它自带了 bash 和一些基本的工具。\n- 周报的功能作为 MOS 的一个项目交给了 @codeworm96, 进度见[这个issue](https://github.com/dyweb/mos/issues/1)\n- [所有带 `backlog` 标签的 issue](https://github.com/dyweb/Ayi/issues?q=is%3Aissue+label%3Abacklog+is%3Aclosed)\n\n第一个提交\n````\ncommit 19858fe3958317da08dc512116c58acbd82b2a35\nAuthor: At15 \u003cat15@outlook.com\u003e\nDate:   Sun Jul 26 13:24:38 2015 +0800\n\n    Initial commit\n````\n\n## 更新\n\n## 引用\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n","cover":"","link":"ayi.html","preview":"\u003cp\u003eAyi 跨平台的命令行工具(库)\u003c/p\u003e\n","title":"Ayi"}]
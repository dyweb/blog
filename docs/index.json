[{"content":"\n\n这篇文章的受众是想更加深入了解 Google Summer of Code 这一活动，或者有志于担任某一开源社区 mentor 的同学。由于有些背景知识没有介绍，因此配合 [Google Summer of Code 学生申请指南](https://zhuanlan.zhihu.com/p/27823910)阅读更佳。\n\n## 背景介绍\n\n[Google Summer of Code](https://developers.google.com/open-source/gsoc/)（下称作 GSoC）是谷歌组织并提供经费，面对全球（绝大多数国家）在读学生的在线编程项目。它的[官方介绍](http://write.flossmanuals.net/gsocstudentguide/what-is-google-summer-of-code/)是：\n\n\u003e Google Summer of Code (GSoC) is a global program that matches students up with open source, free software and technology-related organizations to write code and get paid to do it! The organizations provide mentors who act as guides through the entire process, from learning about the community to contributing code. The idea is to get students involved in and familiar with the open source community and help them to put their summer break to good use.\n\n即是：\n\n\u003e Google 编程之夏是一个全球性项目，旨在为学生们和开源、自由软件、技术相关的组织建立联系，让学生们贡献代码并获得报酬！组织会提供导师，在学生从熟悉社区到贡献代码的整个过程中提供指导。这个想法的目的是让学生们参与和熟悉开源社区，并帮助他们充分利用暑假时间去得到锻炼。\n\n具体可见 [Google Summer of Code 学生申请指南](https://zhuanlan.zhihu.com/p/27823910)，这里不再赘述了。这篇文章主要是从一个 mentor 的角度，来谈一谈对 Google Summer of Code 这个活动的理解。\n\n## GSoC 的流程 Extended\n\n在上一篇文章中，我介绍了一下从学生的角度看到的 Google Summer of Code 的流程，不过在 mentor 的角度来看，又有一些不同的地方。这里从头开始，重新介绍一下整个 GSoc 的 Timeline。\n\n一切的一切始于 organization 的申请。在 GSoC 中，不只是需要学生来申请 organization 的 slots，organization 也是要先向谷歌申请参加当年 GSoC 的。GSoC 的申请向来是一个黑盒的过程，我们看不到谷歌内部做了怎样的考量，会允许哪些组织参与今年的 GSoC。但是一般来说，老牌的组织很少有申请不中的情况，所以如果想尽早关注 GSoC 的话，建议还是从每年的 slots 数量比较多的组织下手，这些组织已经参加过多次 GSoC，再次入选的可能相对比较大。不过值得一提的是，像 The Processing Foundation 这样的老牌组织，在 2016 年的时候就被拒绝了。所以这只能保证概率，而不是绝对。\n\n一般对于 organization 申请的时间窗口大概是在 20 天左右，谷歌在申请结束后花 20 天左右的时间来确定入选的组织，因此这整个过程大概要花 40 天。接下来就是我们熟知的，学生申请的过程。在这个过程里学生会根据每个组织列出的 idea 来提交自己的 proposal，这个的时间窗口在 15 天左右。\n\n接下来 10 天的时间，是社区的 mentor 和 admin 们来 review 学生的 proposal 的阶段。在这一阶段，组织的 admin 需要在看过所有的 proposal 后，根据申请的质量来确定今年向谷歌申请的 slots 数量。这个数量是以区间的形式提交的，即提交一个最小值与一个最大值。谷歌不能保证给到你最小值的 slots 但可以保证最多不给你超过最大值的 slots。另外值得注意的是，一个新的 org，通常来讲只会有 1-2 个 slots。因为谷歌认为这些 org 可能还不太了解 GSoC 对组织的负担其实还是蛮大的。因此如果是申请的新 org 时，要格外注意这一点。\n\n接下来谷歌会花 1 天的时间来确定 org 真正可以拿到多少个 slots，这也是黑盒的操作，org 中的任何一个人在谷歌公布之前，都不能确定今年一共几个 slots。\n\n在确定了 slots 的数量后，org admin 会来分配 slots 给学生。这个过程大概持续 10 天的时间。在这个期间 mentor 起不到决定性的作用，因为 slots 的名额分配是 org admin 来做的。而 mentor 起到的作用是对其 mentor 的 idea 的 proposal 打分，给出参考性的意见。\n\n在确定了 slots 分配后，org admin 和 mentor 就会知道哪些学生可以参加今年的 GSoC，但是真正面向学生的公布会在 5-6 天后由谷歌在 GSoC 的网站上进行。在这段时间里谷歌会复查所有被选中的学生的 eligibility。\n\n随后在项目公布后，便会进入为期 20 天的 Community Bonding Process。在此期间，mentor 需要与学生建立联系，相互熟悉为后面的合作打好基础。除此之外学生也应积极地参与社区，如果 mentor 认为学生在这段时间并没有很好地进行交互，是可以而且被建议直接 fail 掉学生的。因为这段时间是 GSoC 项目的前期准备时间，如果这段时间里准备不是很充分，后面真正开始写代码的时候或多或少会有风险。\n\n在度过了 Community Bonding Process 后，就进入真正的 coding 环节。Coding 环节一共有三个 evaluation，将整个编程环节分成了三个 phase，每个 phase 大概有一个月的时间，所以整个编程的时间大概有三个月（五月到八月）。\n\n## Mentor 职责\n\nMentor 在 organization 开始申请之前，需要先写出自己想 mentor 的 idea 的 description，来说清楚 idea 的目标是什么，需要什么技术栈的同学来做，以及难度如何等等，例如 [coala Language Server](https://projects.coala.io/#/projects?project=coala_language_server\u0026lang=en) 这样。期间可能需要跟 org admin 交流来确定 idea 的工作量和重要程度等等。\n\n在开放学生申请后，Mentor 需要接受来自各位对 idea 的学生的询问和套瓷，解答他们的疑惑，review 他们的 proposal 草稿并且给出你的意见。这是一个非常艰难的阶段，一方面有一些申请者由于对社区和 idea 都不熟悉，会有很多在文档里都有记录的问题来问。另一方面，不同的学生对 idea 的解读不同，导致会花很多时间在 sync mentor 和学生之间的理解上。\n\n在学生申请 deadline 后，mentor 需要 review 所有的 proposal，随后给出自己的选择倾向。如果遇到了两个 candidate 都很强的情况，可能还要组织一次 video interview 或者类似的考核方式。不过前面也提到了，最终的选择是 org admin 进行的\u003cdel\u003e，当然mentor 的决定权也是很重要的\u003c/del\u003e。\n\n在 Community Bonding Process 中，mentor 需要和学生取得联系，并且与 org admin 沟通确定学生在这个期间需要做的事情，这是因社区而异的。同时，最好在这个时候确定下来与学生的常规交流时间，以及管理与交流工具的选型。在随后的 coding phase 中，mentor 主要起到的是进度管理，和解决学生对 code base 的疑惑等等这些问题。\n\n## 申请相关的建议\n\n最后谈一谈申请相关的事情吧。就我看过的申请情况来说，大体上申请人基本可以分为这么几类：\n\n首先就是在申请的社区里贡献了很久（甚至有的已经成为了 maintainer），与此同时有一些与申请的 idea 强相关的贡献的一类候选人。这种候选人基本来说如果 idea 不是特别边缘，都是板上钉钉的样子，这个就不多讨论了。\n\n其次是在申请的社区里贡献了很久，但是与申请的 idea 并没有什么交集的一类。这一类候选人，是稍微有点吃亏的。虽然在社区里做了一段时间，但是因为没有 PR 或者经历来证明其对申请的 idea 的了解。\n\n还有一类是虽然在社区里声明不显，但是在申请的 idea 有关的技术领域有一定的积累的候选人。这种就有点神经刀的感觉，看上去并无太大出众之处，但搞不好就可以交出一份方案详实，安排合理的 proposal。\n\n上面提到的三类申请者是比较可能申请到 GSoC 的候选人，而一个社区参与不多，对申请的 idea 上也无太多了解的同学就相对难度较大了，因为 GSoC 很少接受只是 ok 程度的 proposal。\n\n谈一下对第二类和第三类候选人的建议。如果你是第二类的候选人，在社区里属于活跃的贡献者，那你应该有近水楼台先得月的优势。所以我觉得需要做的是尽早跟感兴趣的 idea 的 mentor 联系，学习 idea 相关的知识，做出一些针对性的贡献。要是跟第三类候选人在同等条件下，肯定会倾向于选在社区已经有过贡献的第二类啦。\n\n对于第三类候选人，因为社区贡献不是一个可以短期速成的东西，因此建议可以多跟 mentor 沟通，在少量贡献的同时完成一份更高质量的 proposal，可能是更有竞争力的一种选择。\n\n## 结语\n\n这又是一篇摸鱼作，希望能够对各位有所帮助。如果你在申请 GSoC 的过程中有什么疑惑，欢迎加入 GSoC CN 的在线聊天频道 [Gsoc-cn/Lobby](https://gitter.im/Gsoc-cn/Lobby)。GSoC CN 是一个由国内的参加过 GSoC 的同学们创建的社区，我们维护有一些[社区的介绍](https://github.com/gsoc-cn/gsoc-cn/tree/master/resources/organizations)和往年的 proposal，可能会对你有些帮助。\n\n欢迎关注我们的 [GitHub](https://github.com/dyweb) 以及[博客](http://blog.dongyueweb.com/) :)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n","cover":"","link":"google_summer_of_code,_a_mentor's_perspective.html","preview":"\u003cp\u003eTODO: write your blog abstract here\u003c/p\u003e\n","title":"Google Summer of Code, A Mentor's Perspective"},{"content":"\n\n这篇文章主要介绍了 [Katib][katib]，一个由 NTT 贡献到 [Kubeflow][] 社区的超参数训练系统。面向人群为对在 Kubernetes 上运行机器学习负载感兴趣的同学。\n\n## 问题背景\n\n在[上一篇文章](http://gaocegege.com/Blog/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/kubeflow)中，我主要介绍了 Kubeflow 社区中关于 TensorFlow 模型训练的支持工作，也就是 [kubeflow/tf-operator][]。它使得用户能够在 Kubernetes 中运行 TensorFlow 的单机或者分布式训练任务，而它无法解决用户对于超参数训练的需求，于是 [Katib][] 应运而生。\n\n如果想了解 Katib，首先需要知道什么是超参数训练。[@tobegit3hub](https://github.com/tobegit3hub) 的[贝叶斯优化: 一种更好的超参数调优方式](https://zhuanlan.zhihu.com/p/29779000)一文对于其有一个比较好的介绍，这里引用一下：\n\n\u003e 首先，什么是超参数（Hyper-parameter）？这是相对于模型的参数而言（Parameter），我们知道机器学习其实就是机器通过某种算法学习数据的计算过程，通过学习得到的模型本质上是一些列数字，如树模型每个节点上判断属于左右子树的一个数，或者逻辑回归模型里的一维数组，这些都称为模型的参数。\n\u003e\n\u003e 那么定义模型属性或者定义训练过程的参数，我们称为超参数，例如我们定义一个神经网络模型有9527层网络并且都用RELU作为激活函数，这个9527层和RELU激活函数就是一组超参数，又例如我们定义这个模型使用RMSProp优化算法和learning rate为0.01，那么这两个控制训练过程的属性也是超参数。\n\u003e\n\u003e 显然，超参数的选择对模型最终的效果有极大的影响。如复杂的模型可能有更好的表达能力来处理不同类别的数据，但也可能因为层数太多导致梯度消失无法训练，又如learning rate过大可能导致收敛效果差，过小又可能收敛速度过慢。\n\u003e\n\u003e 那么如何选择合适的超参数呢，不同模型会有不同的最优超参数组合，找到这组最优超参数大家会根据经验、或者随机的方法来尝试，这也是为什么现在的深度学习工程师也被戏称为“调参工程师”。根据No Free Lunch原理，不存在一组完美的超参数适合所有模型，那么调参看起来是一个工程问题，有可能用数学或者机器学习模型来解决模型本身超参数的选择问题吗？答案显然是有的，而且通过一些数学证明，我们使用算法“很有可能”取得比常用方法更好的效果，为什么是“很有可能”，因为这里没有绝对只有概率分布，也就是后面会介绍到的贝叶斯优化。\n\n也就是说，机器学习的模型本身有一些参数，这些参数的选择通过训练的方式获得一个最优解或者次优解的过程，就是超参数训练的过程。这个过程是一个黑盒的优化过程，我们对模型本身一无所知，也不能做任何假设。而且训练模型往往是一个比较昂贵的任务，要么会占用相当长时间的 CPU，要么会占用 GPU，所以我们并没有很多的尝试机会。因此我们要做的就是在有限的尝试次数中，尽可能确定一组输入，能得到尽可能好的输出。\n\n## 相关工作\n\nGoogle Vizier 是 Google 内部的机器学习超参数训练系统的一个子系统，有一篇论文详细介绍了其抽象以及架构设计：[Google Vizier: A Service for Black-Box Optimization](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46180.pdf)。Google Vizier 对超参数训练这一需求进行了比较好的领域抽象，并且支持多个参数搜索的算法。\n\n[@tobegit3hub](https://github.com/tobegit3hub) 基于此实现了开源的实现 [advisor](https://github.com/tobegit3hub/advisor)，其采取了 clinet 与 server 的架构，并且提供了基础的 UI 界面。并且支持多个机器学习框架（如 TensorFlow, scikitlearn 等）。[Cisco AI](https://github.com/CiscoAI) 也基于 Google Vizier 实现了自己的超参数训练工具 [hyper-advisor-client](https://github.com/CiscoAI/hyper-advisor-client)。\n\n## Katib 介绍\n\nKatib 也是对 Google Vizier 的开源实现，因此也遵循其中对问题的抽象模型：Study，Trial 和 Suggestion。\n\nTrial 代表着一个由超参数的取值组成的列表，每个 Trial 都需要一次运行来得到这些超参数取值对应的结果。这里提到的运行就是一次训练的过程。Study 代表着在可行空间上运行的单个优化。每个 Study 都有一个配置，来描述可能取值的空间，超参数推荐算法等。此外，Study 包含一组 Trial，代表着算法在超参数集合中选取推荐值的多次尝试。如图所示，是创建一次 Study 并且进行 Trial 的验证的过程。\n\n![](/images/posts/katib/katib.png)\n\n首先，用户会通过 katib-cli 创建一个新的 Study，这需要用户提供一份配置，下面是一个样例配置。目前配置中主要有两类参数，一类是 Study 本身需要的参数，比如使用的 Suggestion 算法，在此例中就是 random。另一类是运行时需要的一些参数，比如其中的 scheduler，就是在使用 Kubernetes 运行 Trial 时的一个参数，它决定这一 Trial 会被哪个调度器调度。\n\n```yaml\nname: cifer10\nowner: root\noptimizationtype: 2\nsuggestalgorithm: random\nautostopalgorithm: median\nobjectivevaluename: Validation-accuracy\nscheduler: default-scheduler\nimage: mxnet/python\nsuggestionparameters:\n    -\n      name: SuggestionNum\n      value: 2\n    -\n      name: MaxParallel\n      value: 2\ncommand:\n        - python\n        - /mxnet/example/image-classification/train_cifar10.py\n        - --batch-size=512\nmetrics:\n    - accuracy\nparameterconfigs:\n    configs:\n      -\n        name: --lr\n        parametertype: 1\n        feasible:\n            min: 0.03\n            max: 0.07\n      -\n        name: --lr-factor\n        parametertype: 1\n        feasible:\n            min: 0.05\n            max: 0.2\n      -\n        name: --max-random-h\n        parametertype: 2\n        feasible:\n            min: 26\n            max: 46\n      -\n        name: --max-random-l\n        parametertype: 2\n        feasible:\n            min: 25\n            max: 75\n      -\n        name: --num-epochs\n        parametertype: 2\n        feasible:\n            min: 3\n            max: 3\n```\n\n随后，katib-manager 收到了来自 katib-cli 的创建 Study 请求后，会将 Study 的配置写入 SQL 数据库（目前为 MySQL），然后访问相应的 suggestion service 来初始化服务并且设置一些参数。接下来 katib-manager 会去对应的 worker 中检查已经运行的 Trial，以及成功了的 Trail，并且以此作为参数，通过 suggestion 服务来得到需要被运行 Trial，继而交给 worker 去运行。\n\n在整个的工作流中，涉及到 katib-cli，katib-manager，suggestion 和 worker 等多个不同的组件。其中 katib-cli 是一个 CLI 工具，通过它，用户可以与 katib-manager 交互。katib-manager 之于 katib 就相当于 kube-apiserver 之于 Kubernetes，它是 katib 中最核心的一个组件。它会负责与其他各个组件交互，并且将结果写入数据库。suggestion 则是一个逻辑上的概念。它包含着很多不同的算法，而每一个算法都是以一个容器的方式独立运行，但所有的算法都会暴露同一端口，使用 `suggestion` 同一前缀，以便于服务发现。worker 与此类似，也是一个逻辑上的概念。Katib 在架构上支持不同的 worker 以适配不同的环境。\n\nkatib-cli，katib-manager 以及不同的 suggestion 算法服务，都是运行在不同的容器中的。所有调用，都是通过 GRPC 的方式进行。这使得 Katib 具有极好的扩展性。在添加新的算法支持时，Katib 可以不停机地进行。而且 worker 的接口使得 Katib 也可以轻易地接入新的云环境。\n\n除此之外，Katib 支持的 Kubernetes worker 可以利用 Kubernetes 的集群能力将 Trail 的运行分布在不同的机器上，使得其真正地具有大规模超参数训练的可能，这也是目前其他同类型的系统支持地并不好的一点。\n\nKatib 使用 modeldb 存储模型，Trail 与模型一一对应。Katib 将所有从 Trail 训练得到的模型存储在 modeldb，以方便管理和查看不同超参数训练出来的模型效果。这里有一个 demo，可以一观。\n\n![](/images/posts/katib/katib.gif)\n\n## 未来的工作与计划\n\n在这篇文章中，我们介绍了 Katib 这一基于 Kubernetes 的超参数训练系统，并且阐述了其与其他系统相比的优劣。目前我们正在紧锣密鼓地开发新的特性与功能，这其中包括但不限于：\n\n**Early Stop 支持** 这一特性能够节省集群的资源，提高参数寻找的效率。目前这一工作由 [@YujiOshima](https://github.com/YujiOshima) 进行。\n\n**更多 Suggestion 算法支持** 目前我们只支持三种较为简单的算法，分别是随机搜索，网格搜索和 Hyperband。因此我们正在着手实现贝叶斯优化等等算法。这一工作由 [@libbyandhelen](https://github.com/libbyandhelen) 进行。\n\n**与 Kubeflow 的集成** Katib 支持原生的 Kubernetes，对于运行 Trial 的容器的配置较为复杂。我们希望能够利用社区现有的 [tf-operator][kubeflow/tf-operator] 和 [pytorch-operator][kubeflow/pytorch-operator] 来提供对不同机器学习框架的支持。这一工作由 [@gaocegege](https://github.com/gaocegege) 进行。\n\n**支持 Neural Architecture Search (NAS)** NAS 的支持是社区呼声比较高的需求，我们希望能够基于 Katib 的架构实现 NAS。目前这一工作由 [@YujiOshima](https://github.com/YujiOshima) 进行。\n\n**以 CRD 的方式支持 Study 资源** 目前 Katib 维护了自己的 CLI，以及配置文件格式。我们认为这可以被 Kubernetes CRD 替代。我们目前正在探索可否实现 katib controller 来替代 katib-cli 与 katib-manager。如此一来 Katib 的维护会更加简单。\n\n与此同时我们也欢迎更多对超参数训练，以及神经网络模型搜索的同学为 [Katib][] 贡献 :-) 欢迎加入我们的 [Slack Channel](https://kubeflow.slack.com/messages/C9ZLKR73L/)，在这之前需要先通过[这一邀请链接注册](https://join.slack.com/t/kubeflow/shared_invite/enQtMjgyMzMxNDgyMTQ5LWUwMTIxNmZlZTk2NGU0MmFiNDE4YWJiMzFiOGNkZGZjZmRlNTExNmUwMmQ2NzMwYzk5YzQxOWQyODBlZGY2OTg)。\n\n## 关于作者\n\n[@gaocegege](https://github.com/gaocegege)，上海交通大学软件学院研究生在读，Kubeflow core approver\n\n欢迎关注我们的 [GitHub](https://github.com/dyweb) 以及[博客](http://blog.dongyueweb.com/) :)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n\n[katib]: https://github.com/kubeflow/hp-tuning\n[Kubeflow]: https://github.com/kubeflow/kubeflow\n[kubeflow/tf-operator]: https://github.com/kubeflow/tf-operator\n[kubeflow/pytorch-operator]: https://github.com/kubeflow/pytorch-operator\n","cover":"","link":"katib:_kubernetes_native_的超参数训练系统.html","preview":"\u003cp\u003e这篇文章主要介绍了 Katib，一个由 NTT 贡献到 Kubeflow 社区的超参数训练系统。面向人群为对在 Kubernetes 上运行机器学习负载感兴趣的同学。\u003c/p\u003e\n","title":"Katib: Kubernetes Native 的超参数训练系统"},{"content":"\n\n这篇文章主要介绍了 [Kubeflow][] 的使用，以及未来的计划，面向人群为对在 Kubernetes 上运行机器学习负载感兴趣的同学。\n\n## 问题背景\n\nKubernetes 本来是一个用来管理无状态应用的容器平台，但是在近两年，有越来越多的公司用它来运行各种各样的工作负载，尤其是机器学习\u003cdel\u003e炼丹\u003c/del\u003e。各种 AI 公司或者互联网公司的 AI 部门都会尝试在 Kubernetes 上运行 TensorFlow，Caffe，MXNet 等等分布式学习的任务，这为 Kubernetes 带来了新的挑战。\n\n首先，分布式的机器学习任务一般会涉及参数服务器（以下称为 PS）和工作节点（以下成为 worker）两种不同的工作类型。而且不同领域的学习任务对 PS 和 worker 有不同的需求，这体现在 Kubernetes 中就是配置难的问题。以 TensorFlow 为例，[TensorFlow 的分布式学习任务](https://www.tensorflow.org/deploy/distributed)通常会启动多个 PS 和多个 worker，而且在 TensorFlow 提供的最佳实践中，每个 worker 和 PS 要求传入不同的命令行参数。举例说明：\n\n```bash\n# On ps0.example.com:\n$ python trainer.py \\\n     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \\\n     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \\\n     --job_name=ps --task_index=0\n# On ps1.example.com:\n$ python trainer.py \\\n     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \\\n     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \\\n     --job_name=ps --task_index=1\n# On worker0.example.com:\n$ python trainer.py \\\n     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \\\n     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \\\n     --job_name=worker --task_index=0\n# On worker1.example.com:\n$ python trainer.py \\\n     --ps_hosts=ps0.example.com:2222,ps1.example.com:2222 \\\n     --worker_hosts=worker0.example.com:2222,worker1.example.com:2222 \\\n     --job_name=worker --task_index=1\n```\n\n其中需要的参数有四个，一个是所有的 PS 的网络地址（主机名-端口），以及所有的 worker 的网络地址。另外是 job 的类型，分为 PS 与 worker 两种。最后是任务的 index，从 0 开始递增。因此在此例中，用户需要写至少四个 pod 的配置文件，以及四个 service 的配置文件，使得 PS 跟 worker 可以互相访问，况且这只是一个机器学习任务。如果大规模地在 Kubernetes 上运行 TensorFlow 分布式任务，可以预见繁杂的配置将成为机器学习工程师们新的负担。\n\n其次，Kubernetes 默认的调度器对于机器学习任务的调度并不友好。如果说之前的问题只是在应用与部署阶段比较麻烦，那调度引发的资源利用率低，或者机器学习任务效率下降的问题，就格外值得关注。机器学习任务对于计算和网络的要求相对较高，一般而言所有的 worker 都会使用 GPU 进行训练，而且为了能够得到一个较好的网络支持，尽可能地同一个机器学习任务的 PS 和 worker 放在同一台机器或者网络较好的相邻机器上会降低训练所需的时间。\n\n## Hello, Kubeflow\n\n针对这些问题，[Kubeflow][] 项目应运而生，它以 TensorFlow 作为第一个支持的框架，在 Kubernetes 上定义了一个新的资源类型：TFJob，即 TensorFlow Job 的缩写。通过这样一个资源类型，使用 TensorFlow 进行机器学习训练的工程师们不再需要编写繁杂的配置，只需要按照他们对业务的理解，确定 PS 与 worker 的个数以及数据与日志的输入输出，就可以进行一次训练任务。在本节中，我们将从零开始搭建一个 Kubernetes 集群，并且将 Kubeflow 运行在其上，最后利用其进行一次完整的学习任务运行。\n\n首先，我们需要有一个正在运行的 Kubernetes 集群，而且集群的版本要大于等于 1.8。在这一步里，个人推荐以下两种方式创建一个单节点的本地 Kubernetes 集群：\n\n- [使用 Kubernetes 里的 local-up-cluster.sh 脚本](https://github.com/kubernetes/kubernetes/blob/master/hack/local-up-cluster.sh)\n- [使用 minikube 项目](https://github.com/kubernetes/minikube)\n\n其中前者会在本地创建一个 native 的 Kubernetes 集群，而后者则会在本地的虚拟机里创建出 Kubernetes 集群。因为本文侧重点不在此，因此整个过程不再赘述。\n\n如果你已经成功地创建了一个 Kubernetes 集群，那么接下来就是在这一集群上创建 Kubeflow 所有的组件，这一步需要用到 [ksonnet][]，一个简化应用在 Kubernetes 上的分发与部署的命令行工具，它会帮助你创建 Kubeflow 所需组件。在安装了 [ksonnet][] 后，接下来就是一片坦途了，只需要运行下面的命令，就可以完成 Kubeflow 的部署。\n\n```bash\n# Initialize a ksonnet APP\nAPP_NAME=my-kubeflow\nks init ${APP_NAME}\ncd ${APP_NAME}\n\n# Install Kubeflow components\nks registry add kubeflow github.com/kubeflow/kubeflow/tree/master/kubeflow\nks pkg install kubeflow/core\nks pkg install kubeflow/tf-serving\nks pkg install kubeflow/tf-job\n\n# Deploy Kubeflow\nNAMESPACE=default\nkubectl create namespace ${NAMESPACE}\nks generate core kubeflow-core --name=kubeflow-core --namespace=${NAMESPACE}\nks apply default -c kubeflow-core\n```\n\nKubeflow 的部署会附带一个 JupyterHub 但笔者并不知道如何使用它，因此下面的操作是用 Docker 打包训练数据和代码，用 kubectl 在 Kubernetes 上启动一次训练任务的。\n\n示例代码可见 [tf_smoke.py](https://github.com/tensorflow/k8s/blob/master/examples/tf_sample/tf_sample/tf_smoke.py)，与正常的训练代码类似，只不过 clusterspec 的传递方式是遵循了 [Cloud ML](https://cloud.google.com/ml-engine/docs/trainer-considerations#use_tf_config) 的 TF_CONFIG 的方式。Kubeflow 已经根据这一训练文件打好了一个 Docker 镜像：`gcr.io/tf-on-k8s-dogfood/tf_sample:dc944ff`，在这里直接使用就好：\n\n```bash\nkubectl create -f https://raw.githubusercontent.com/tensorflow/k8s/master/examples/tf_job.yaml\n```\n\n\n\n## Kubeflow 实现介绍\n\n本部分主要涉及对 Kubeflow 内部实现的介绍和未来可能的开发计划，如果不感兴趣可以就此打住 :)\n\n### 对分布式训练任务的支持\n\n为了解决配置困难的问题，Kubeflow 以 TensorFlow 作为第一个支持的框架，为其实现了一个在 Kubernetes 上的 [operator](https://coreos.com/operators/)：[tensorflow/k8s][]。由于在 Kubernetes 上内置的资源类型，如 deployment，replicaset，或者是 pod 等，都很难能够简练而清晰地描述一个分布式机器学习的任务，因此我们利用 Kubernetes 的 [Custom Resource Definition](https://kubernetes.io/docs/concepts/api-extension/custom-resources/) 特性，定义了一个新的资源类型：TFJob，即 TensorFlow Job 的缩写。一个 TFJob 配置示例如下所示：\n\n```yaml\napiVersion: \"kubeflow.org/v1alpha1\"\nkind: \"TFJob\"\nmetadata:\n  name: \"example-job\"\nspec:\n  replicaSpecs:\n    - replicas: 1\n      tfReplicaType: MASTER\n      template:\n        spec:\n          containers:\n            - image: gcr.io/tf-on-k8s-dogfood/tf_sample:dc944ff\n              name: tensorflow\n          restartPolicy: OnFailure\n    - replicas: 1\n      tfReplicaType: WORKER\n      template:\n        spec:\n          containers:\n            - image: gcr.io/tf-on-k8s-dogfood/tf_sample:dc944ff\n              name: tensorflow\n          restartPolicy: OnFailure\n    - replicas: 2\n      tfReplicaType: PS\n      template:\n        spec:\n          containers:\n            - image: gcr.io/tf-on-k8s-dogfood/tf_sample:dc944ff\n              name: tensorflow\n          restartPolicy: OnFailure\n```\n\n其中每个字段就不多介绍了，这里主要是说一下实现。任何一个 PS 或者 worker，都由两个资源组成，分别是 job 和 service。其中 job 负责创建出 PS 或者 worker 的 pod，而 service 负责将其暴露出来。这里社区目前也在重新考虑选型，目前希望可以直接创建 pod 而非 job，而用 headless service 替代 service，因为 PS worker 不需要暴露给除了该分布式学习任务外的其他服务。\n\nTFJob operator 的实现早期是从 [etcd-operator](https://github.com/coreos/etcd-operator/) 复制来的，因此整体的架构在最初是完全仿照其改写而成。在最初的实现中，当有一个 TFJob 被创建时，在 operator 内都会有一个新的 goroutine，以轮询的方式获取 TFJob 的状态，然后基于此状态做出相应的操作，相当于是在 operator 内部维护了一个状态机。这样的方式会有一些缺点：\n\n- 这样的架构使得 operator 是有状态的，使得状态很难横向扩展\n- 维护基于 Phase 的状态机是 Kubernetes 社区不推崇的一种方式\n\n基于这些问题，operator 的架构正在往事件驱动重构，这部分工作由 [@caicloud][] 在推进。重构之后，operator 会在 Kubernetes 的一些资源上注册 informer 的事件回调，比较现在的状态与理想状态的不同而采取相应的操作。比如当有一个新的 TFJob 被创建时，理想状态是所有对应的 PS， worker 都被创建好，而当下的状态则是没有任何 pod 和 service 被创建，此时 operator 会创建出对应的 PS，worker 的 pod 和 service，以达到理想状态，这也是 Kubernetes 社区对于 operator/controller 的最佳实践。\n\n### 对分布式学习任务效率的关注\n\n目前社区还停留在如何对 AI 工程师更友好，更好地维护上面提到的 operator 这一步，在效率方面考虑地较少。目前有利用 [kube-arbitrator][] 来进行 gang scheduling 的探索，目前还没有尝试过因此不好评价。但是整体来说 Kubeflow 的性能提高还有很大的空间。\n\n因为机器学习任务根据模型的不同，其输入数据的规模，特征，模型的大小等等都有很大不同。比如 CV 领域与推荐领域的学习模型就有完全不同的特点，因此 TensorFlow 的分布式模型提供了极强的灵活性。而对于 Kubernetes 而言，如何能够在保持灵活性的基础上，同时也保证任务在较高的性能下运行，同时集群的利用率也相对较高，是一个值得研究的问题。\n\n### 对其他机器学习框架的支持\n\n目前 Kubeflow 主要关注 TensorFlow，而其他机器学习框架的支持将于之后展开，目前有一些第三方实现的 operator，比如 [MXNet operator](https://github.com/deepinsight/mxnet-operator)，但是质量难以保证。\n\n### 开发情况与未来展望\n\n目前 Kubeflow 有来自 Google，Caicloud，RedHat 等公司的积极参与，短期的目标有这么几个：\n\n- operator 方面\n    - 使用 pod 替换 job [tensorflow/k8s#325](https://github.com/tensorflow/k8s/issues/325)\n    - 使用 headless service 替换 service [tensorflow/k8s#40](https://github.com/tensorflow/k8s/issues/40)\n    - 由 etcd operator 主动轮询的方式改为事件驱动 [tensorflow/k8s#314](https://github.com/tensorflow/k8s/issues/314)\n    - 分离对 TensorBoard 的支持 [tensorflow/k8s#347](https://github.com/tensorflow/k8s/issues/347)\n    - 支持细粒度的任务状态 [tensorflow/k8s#333](https://github.com/tensorflow/k8s/issues/333)\n- 模型服务方面\n    - GPU 支持 [kubeflow/kubeflow#64](https://github.com/kubeflow/kubeflow/issues/64)\n    - 监控支持 [kubeflow/kubeflow#64](https://github.com/kubeflow/kubeflow/issues/64)\n    - 多框架支持下的统一 API 支持 [kubeflow/kubeflow#102](https://github.com/kubeflow/kubeflow/issues/102)\n- UI 方面\n    - 为各个部件支持统一的 UI [kubeflow/kubeflow#199](https://github.com/kubeflow/kubeflow/issues/199)\n\n目前 Kubeflow 在 GitHub 上有 2400 多个 star，有 40 个左右的贡献者。其长期的目标是成为 CNCF 的一个项目，目前实现仍存在很多问题，窃以为也并不是 production ready 的状态，但它仍然值得一试。\n\n## 关于作者\n\n[@gaocegege](https://github.com/gaocegege)，上海交通大学软件学院研究生在读，[Kubeflow][] Member，寻找 2018 暑期实习。\n\n[Kubeflow]: https://github.com/kubeflow/kubeflow\n[tensorflow/k8s]: https:github.com/tensorflow/k8s\n[ksonnet]: https://github.com/ksonnet/ksonnet\n[@caicloud]: https://github.com/caicloud\n[kube-arbitrator]: https://github.com/kubernetes-incubator/kube-arbitrator\n\n\n欢迎关注我们的 [GitHub](https://github.com/dyweb) 以及[博客](http://blog.dongyueweb.com/) :)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n","cover":"","link":"kubeflow_安利：在_kubernetes_上进行机器学习.html","preview":"\u003cp\u003e这篇文章主要介绍了 Kubeflow 的使用，以及未来的计划，面向人群为对在 Kubernetes 上运行机器学习负载感兴趣的同学。\u003c/p\u003e\n","title":"Kubeflow 安利：在 Kubernetes 上进行机器学习"},{"content":"\n\n[Netlify]: https://www.netlify.com/\n[Travis CI]: https://travis-ci.org/\n\n现在有越来越多的开发者选择把自己的博客以静态网站的方式托管在 GitHub 上, 这样的方式只需要一个域名就可以通过诸如 [Jekyll](https://jekyllrb.com/), [Hexo](https://hexo.io/), [纸小墨](http://www.chole.io/) 等等现有的静态博客生成工具, 非常便捷地搭建出一个样式美观的静态博客.\n\n目前传统的软件项目, 可以通过 [Travis CI][] 等等工具来进行编译, 测试等等持续集成任务, 但是对于一个静态网站来说, 其最主要的产物是 HTML 文件. 而主流的持续集成工具都不支持对静态的页面进行构建的预览. 这篇文章主要介绍了 [Netlify][], 一个可以用来做静态网站的持续集成与持续部署的工具. 通过 [Netlify][], 用户可以非常简单地为其静态网站项目引入持续集成, 并且允许其他成员对静态网站进行 UI 层面的 review.\n\n## 需求介绍\n\n[东岳团队博客](http://blog.dongyueweb.com/) 是一个使用[纸小墨](http://www.chole.io/)构建的静态博客网站, 其托管在 [github.com/dyweb/blog](https://github.com/dyweb/blog) 下. 因为我们的博客采取投稿制, 每个项目成员都可以为这个博客进行投稿, 所以对于投稿的审核是一个非常重要的需求.\n\n目前我们的投稿是以 Pull Request 的方式进行, 作者会首先提交自己的文章, 这是以 Markdown 的格式书写的. 然后会基于 Markdown 文件, 构建出对应的 HTML 文件并加入到 docs/ 目录下. 因此我们对于投稿的审核是在 Pull Request 下进行的.审核包括但不限于对博客样式是否被新的投稿破坏, 投稿内容是否贴合博客方向, 有无 typo 等等. \n\n而因为在 Pull Request 中, 我们只能看到所有文本文件的变动, 而看不到变动后的博客页面. 这在之前只能在文章被合并后, 再次访问[东岳团队博客](http://blog.dongyueweb.com/), 才可以进行效果的审核, 而这时文章已经是合并状态了, 再次修改需要提交新的 Pull Request 才能继续, 这样无疑延长了审核的周期.\n\n因此我们需要一个工具, 可以帮助我们为每个 Pull Request 构建出静态网站, 并且是允许所有成员都可以访问的.\n\n## 相关工作\n\n[Travis CI][] 是一个非常成熟的持续集成工具, 通过它, 用户可以执行自定义的脚本任务, 测试等等. 在之前我们使用它来确认我们的代码是可以完成构建出静态网站这一步操作的. 但是 [Travis CI][] 存在一些问题, 它不能为每个 Pull Request 构建出静态网站供我们审核, 而只能简单地返回构建的成功与否, 这个信息对我们而言是不充分的. 而其他此类的服务也往往具有这个问题.\n\n## 使用 Netlify 进行静态网站持续集成\n\n[Netlify][] 对自己的描述是:\n\n\u003e Netlify is a unified platform that automates your code to create high-performant, easily maintainable sites and web apps.\n\n[Netlify][] 与 [Travis CI][] 等等都是持续集成工具, 但是它更加关注前端, 或者说网站或者 web app 的持续集成与持续部署, 这也是它与其他持续集成工具最大的区别. 我们目前对于 [Netlify][] 的使用也非常简单, 但是这是其他持续集成工具没有的.\n\n为了能够在每个 Pull Request 中看到新的博客文章预览, 我们需要在 [Netlify][] 中为我们的 repository 做一些设置. 在我们的使用中, 设置过程非常地简单:\n\n- 为 repository 启用 Netlify\n- 设置发布目录为 docs/\n\n首先最重要的是为 repository 启用 [Netlify][], 这一部分与其他持续集成工具并无二致. 这一环节最主要的是让 [Netlify][] 在 GitHub 中建立 web hook, 使得它可以监听到整个 repository 的事件. 当然这是自动的, 对于用户而言是不感知的. 随后 [Netlify][] 会要求用户去设置构建的命令以及发布的目录.\n\n\u003cfigure\u003e\n\t\u003cimg src=\"http://gaocegege.com/Blog/images/netlify/start.png\" alt=\"在 Netlify 中启用功能\" width=\"500\"\u003e\n\u003c/figure\u003e\n\n对于我们的博客来说, 因为我们的博客会被编译好放置在 docs/ 目录下, 因此不需要 [Netlify][] 为我们进行构建, 如果你的项目并不是这样操作的, 比如只是有源文件而没有提交构建后的静态网站, 那你可以利用它的这一功能进行远端的构建. [Netlify][] 默认支持 [Jekyll](https://jekyllrb.com/) 等等静态博客生成工具的命令, 因此可以满足绝大多数的应用场景.\n\n\u003cfigure\u003e\n\t\u003cimg src=\"http://gaocegege.com/Blog/images/netlify/netlify.png\" alt=\"使用\" width=\"500\"\u003e\n\u003c/figure\u003e\n\n在配置结束后, 就可以利用 [Netlify][] 来进行持续集成了. 在大多数情况下, 用户甚至不需要像 [Travis CI][] 那样在 repository 里放置配置文件, 直接通过网页操作来搭建起对一个 repository 的持续集成.\n\n每当一个新的 Pull Request 被创建的时候, [Netlify][] 会为这个请求运行一个构建任务, 这个任务最终会生成一个预览, 通过 URL 可以访问到基于这一 Pull Request 的构建结果.\n\n\u003cfigure\u003e\n\t\u003cimg src=\"http://gaocegege.com/Blog/images/netlify/github.png\" alt=\"使用\" width=\"500\"\u003e\n\u003c/figure\u003e\n\n而 URL 是可以自定义的, 比如 [https://deploy-preview-26--blog-dongyueweb.netlify.com](https://deploy-preview-26--blog-dongyueweb.netlify.com/%E5%B0%8F%E8%AE%AE%E5%88%86%E5%B8%83%E5%BC%8F%E7%B3%BB%E7%BB%9F%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E6%A8%A1%E5%9E%8B). 不同 Pull Request 会有不同的 URL, 因此基于此还可以去做 Split Testing. 目前 [Netlify][] 支持两个 branch 之间的 Split Testing, 但还是 beta 阶段, 我们没有进行过尝试.\n\n如此以来, 我们可以利用 [Netlify][] 的预览功能, 来对 Pull Request 进行内容和格式的审核. [Netlify][] 本身有更多的功能, 它的愿景是为前端引入持续集成. 限于篇幅, 停笔于此.\n\n## 附录\n\n目前我们知道的使用 [Netlify][] 服务的网站有:\n\n- [统计之都](https://github.com/cosname/cosx.org)\n- [东岳团队博客](http://blog.dongyueweb.com/)\n- [Processing.R 网站](https://github.com/processing-r/processing-r.github.io)\n- [headlesscms.org](https://github.com/netlify/headlesscms.org)\n\nPS: 这是一篇免费的安利文, 我们与 [Netlify][] 利益不相关\n\n欢迎关注我们的 [GitHub](https://github.com/dyweb) 以及[博客](http://blog.dongyueweb.com/) :)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n","cover":"","link":"case_study:_使用_netlify_持续集成你的静态网站.html","preview":"\u003cp\u003e这篇文章主要介绍了 Netlify, 一个可以用来做静态网站的持续集成与持续部署的工具. 通过 Netlify, 用户可以非常简单地为其静态网站项目引入持续集成, 并且允许其他成员对静态网站进行 UI 层面的 review.\u003c/p\u003e\n","title":"Case Study: 使用 Netlify 持续集成你的静态网站"},{"content":"\n\n一致性 (Consistency) 一直是分布式系统里一个很重要的话题, 如果要了解一致性, 要从系统模式开始说起.\n\n## 系统模型\n\n系统模型 (System Model), 是描述系统的特性的一些假设, 基于这些假设才可以设计出各种各样的分布式系统. 这些假设包括但不限于:\n\n- 每个节点的计算能力以及它们的失效模式\n- 节点间通信的能力以及是否可能失效\n- 整个系统的属性, 比如时序等等\n\n其中每一点下面都会讲到. 如果一个系统模型是基于最弱的假设的, 比如节点可能出现硬件错误、网络拥塞或断开以及可能遭到恶意攻击等等, 那基于这样的系统模型的分布式系统可以运行在各种各样的环境下, 因为它的容错极高.\n\n而如果我们做出了很强硬的假设, 比如节点不会 fail, 那基于此假设的系统不需要处理节点失效的问题, 但是因为假设太偏离现实, 所以很难在生产环境中真正去使用.\n\n### 系统模型中的节点问题\n\n节点, 可以理解为系统中的一个个虚拟机或者物理机, 它们是负责真正的计算和存储业务的. 它们会运行确定性的算法, 并且可以将数据写入易失性存储器 (内存) 或者持久性存储器 (磁盘). 不同的存储会受到不同失效模型的影响.\n\n节点的失效模型是指可能使得节点失效的途径. 在大多数实践中, 系统都是假设节点只有在 crash 的时候会失效, 并且会在 crash 之后的某个时间点恢复工作.\n\n这个模型并不是最弱的, 最弱的是著名的[拜占庭失效](https://en.wikipedia.org/wiki/Byzantine_fault_tolerance), 也就是可以以任何方式失效. 基于拜占庭失效模型的算法很少在生产环境中遇到, 这也很好想象, 因为实现的难度必然是很大的, 而且大多数时候也遇不到那么多失效的方式. 因此最常用的还是 crash-recovery 模型.\n\n### 系统模型中的网络问题\n\n在上个年代中, 很少有系统会考虑网络分区的问题, 但是在现在这个环境下, 随着系统的规模增长, 网络的失效也变得常见起来.\n\n一般来说网络的失效模型比较简单, 即直到网络恢复为止, 网络上的信息可能会丢失或者延迟.\n\n### 系统模型中的时序问题\n\n一般来说, 时序只有两种模式, 异步和同步. 同步是指\n\n\u003e Processes execute in lock-step; there is a known upper bound on message transmission delay; each process has an accurate clock\n\n异步是指\n\n\u003e No timing assumptions - e.g. processes execute at independent rates; there is no bound on message transmission delay; useful clocks do not exist\n\n也就是说在同步模型中, 信息一定会在一定延迟内送达, 整个系统是有时序的逻辑的. 而在异步模型中, 不存在系统全局的时序.\n\n在现实中, 大部分的场景是部分同步的时序模型, 它们偶尔可以正常工作, 并提供一些上界, 但在某些情况下，消息会被无限期地延迟，时钟也不同步。\n\n## 共识问题\n\n讨论完系统模型, 接下来就可以在设计好的系统模型的假设下, 讨论共识问题了. 所谓共识 (Consensus) 问题, 就是相互独立的节点之间如何达成一项决议的问题。通俗来说, 如果多个相互独立的节点都认同同一个结果, 那他们就达成了共识. 形式化的描述就是:\n\n- 认同 (agreement): 一个决议过程中所有 N 个节点都认同一个结果\n- 合法 (validity): 该结果必须由 N 个节点中的节点提出\n- 可结束 (termination): 决议过程在一定时间内结束，不会无休止地进行下去\n\n这个听上去很简单, 但是在比较弱的系统模型下, 有很多问题会导致共识是很难达成的. 比如如果网络中存在消息延时、丢失，节点间消息传递; 亦或者是节点存在 crash 的情况. 在这些假设下, 如何达成共识, 才是真正对现实中的分布式系统有价值的讨论.\n\n### FLP 定理\n\n这里就不得不提到一个定理: FLP 定理. 该定理的论文是由 Fischer, Lynch and Patterson 三位作者于1985年发表,之后该论文获得了 Dijkstra 奖。定理所依赖的系统模型很简单:\n\n- 节点只会因为 crash 而失效 (非拜占庭失效)\n- 网络是可靠的, 只要进程非失败，消息虽会被无限延迟，但最终会被送达；并且消息仅会被送达一次 (无重复)\n- 异步的时序模型, 与同步通信的最大区别是没有时钟、不能时间同步、不能使用超时、不能探测失败、消息可任意延迟、消息可乱序\n\n在现实中，一般网络会使用 TCP 协议 (保证了消息健壮、不重复、不乱序)，每个节点都有 NTP 时钟同步 (可以使用超时), 如 FLP 定理的系统模型这样的异步场景相对比较少. 但是也还是存在一定的场景.\n\nFLP 在这样的系统模型下给出了一个很吃鸡的结论: 在异步通信场景，即使只有一个进程失败，也没有任何 (确定性) 算法能保证非失败进程达到一致性.\n\n也就是说, 解决共识问题的算法必须在不存在消息传递边界的情况下放弃强一致 (safety) 或者可用性 (liveness)。\n\n### CAP 理论\n\nCAP 恐怕要比 FLP 定理更出名一些. 甚至国内某 NewSQL 数据库厂商 [PingCAP](https://pingcap.com/index) 把它加入了自己公司的名字中.\n\n这一理论是说\n\n- 强一致性 (Strong Consistency): 如果系统对一个写操作返回成功，那么之后的读请求都必须读到这个新数据；如果返回失败，那么所有读操作都不能读到这个数据，对调用者而言数据具有强一致性\n- 可用性 (Availability): 节点的失效不会导致其余节点的工作收到影响\n- 分区容错性 (Partition tolerance): 在网络分区的情况下，被分隔的节点仍能正常对外服务\n\nC、A、P 三者最多只能满足其中两个，和 FLP 定理一样，CAP 定理也指出了一个不可达的结果 (impossibility result)。\n\n\u003cfigure\u003e\n\t\u003cimg src=\"http://book.mixu.net/distsys/images/CAP.png\" alt=\"CAP\" height=\"500\" width=\"500\"\u003e\n\u003c/figure\u003e\n\n但是需要注意的是, 这里的 C 指的是强一致性, 而非一致性. 因此在现实中, 一致性的强弱与可用性是可以 trade-off 的. 那因此就存在一致性的模型.\n\n#### 一致性模型\n\n一致性模型是描述一致性强弱的. 大类可分为强一致性和弱一致性模型. 但是其中又有很多小的不同.\n\n其中强一致性模型主要是有两种, 一种是线性一致性 (Linearizable consistency), 另一种是顺序一致性 (Sequential consistency).\n\n关键的区别在于，线性化的一致性要求操作生效的顺序等于实际的实时操作排序。顺序一致性允许操作被重新排序，只要在每个节点上观察到的顺序保持一致。作为用户, 能区分两者的唯一方法是，观察系统的所有输入和时间. 从客户端与节点交互的角度来看，两者是等价的。\n\n而弱一致性则是有以客户为中心的一致性模型 (Client-centric consistency models), 因果一致性 (Causal consistency) 和最终一致性 (Eventual consistency).\n\n其中客户为中心的一致性模型可能保证客户永远不会看到数据项的旧版本。这通常是通过在客户端库中构建额外的缓存来实现的，因此，如果客户端移动到包含旧数据的副本节点，那么客户端库就会返回缓存的值，而不是从副本中返回旧值。\n\n最终一致性是系统保证如果没有对某个对象的新更新操作，最终所有的访问将返回这个对象的最后更新的值。这里就有比较多的取舍了, 比如最终是多久之类的.\n\n因果一致性可以理解为是最终一致性的变种, 如果进程 A 通知进程 B 它已经更新了一个数据项，那么进程 B 的后续访问将返回更新后的值，并且写操作将被保证取代前一次写入。和进程 A 没有因果关系的 C 的访问将遵循正常的最终一致性规则。\n\n## 参考文章\n\n- [分布式系统理论 - 从放弃到入门](https://www.cnblogs.com/bangerlee/p/6216997.html)\n- [Distributed systems: for fun and profit](https://github.com/mixu/distsysbook)\n- [FLP Impossibility](http://blog.csdn.net/chen77716/article/details/27963079)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n\n\n欢迎关注我们的 [GitHub](https://github.com/dyweb) 以及[博客](http://blog.dongyueweb.com/) :)\n","cover":"","link":"小议分布式系统的一致性模型.html","preview":"\u003cp\u003e一致性 (Consistency) 一直是分布式系统里一个很重要的话题, 如果要了解一致性, 要从系统模式开始说起.\u003c/p\u003e\n","title":"小议分布式系统的一致性模型"},{"content":"\n\nSuppose we would like to perform a task repeatedly at regular intervals with a goroutine.\n\nHere is the first try:\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"time\"\n)\n\nfunc main() {\n\tgo func() {\n\t\tfor {\n\t\t\tfmt.Println(\"hello, world!\")\n\t\t\ttime.Sleep(5 * time.Second)\n\t\t}\n\t}()\n}\n```\n\nOops, it does not work. The main function terminates immediately after starting the goroutine,\nand the program terminates with the termination of the main function.\n\nWe should let the main function wait for the goroutine. Therefore,\n[WaitGroup](https://golang.org/pkg/sync/#WaitGroup) is needed.\n\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\t\"sync\"\n\t\"time\"\n)\n\ntype WaitGroupWrapper struct {\n\twg sync.WaitGroup\n}\n\nfunc (w *WaitGroupWrapper) Spawn(f func()) {\n\tw.wg.Add(1)\n\tgo func() {\n\t\tf()\n\t\tw.wg.Done()\n\t}()\n}\n\nfunc (w *WaitGroupWrapper) Wait() {\n\tw.wg.Wait()\n}\n\nfunc main() {\n\tvar wg WaitGroupWrapper\n\twg.Spawn(func() {\n\t\tfor {\n\t\t\tfmt.Println(\"hello, world!\")\n\t\t\ttime.Sleep(5 * time.Second)\n\t\t}\n\t})\n\twg.Wait()\n}\n```\n\nNote that I used a wrapper to wrap the WaitGroup to make it easier to use.\nNow the program works, but we can improve it with tickers, which provide by\nGo for doing something repeatedly at regular intervals.\n\n```go\nticker := time.NewTicker(5 * time.Second)\nwg.Spawn(func() {\n    for {\n        \u003c-ticker.C\n        fmt.Println(\"hello, world!\")\n    }\n})\n```\n\nCurrently, our goroutine runs forever. We can stop it using another channel.\n\n```go\nfunc main() {\n\tvar wg WaitGroupWrapper\n\tdone := make(chan struct{})\n\tticker := time.NewTicker(5 * time.Second)\n\twg.Spawn(func() {\n\t\tfor {\n\t\t\tselect {\n\t\t\tcase \u003c-ticker.C:\n\t\t\t\tfmt.Println(\"hello, world!\")\n\t\t\tcase \u003c-done:\n\t\t\t\tgoto exit\n\t\t\t}\n\t\t}\n\texit:\n\t})\n\ttime.Sleep(16 * time.Second)\n\tclose(done)\n\twg.Wait()\n}\n```\n\n## License\n\n- This article is licensed under [CC BY-NC-SA 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/).\n- Please contact \u003cmarketing@dongyue.io\u003e for commerical use.\n","cover":"","link":"go:_tasks_repeated_at_intervals.html","preview":"\u003cp\u003eUse Go to run tasks at intervals.\u003c/p\u003e\n","title":"Go: tasks repeated at intervals"},{"content":"\n\n[@codeworm96]: https://github.com/codeworm96\n[@hawkingrei]: https://github.com/hawkingrei\n[@prism-river]: https://github.com/prism-river\n[Killy]: https://github.com/prism-river/killy\n\n2017 年 10 月 21 日，由 [Golang Foundation](http://golangfoundation.org/) 和 [PingCAP](https://pingcap.com/index) 联合举办的 [Go Hack 17](http://gohack2017.golangfoundation.org/) 在上海心动网络举行，东岳的小哥哥 [@gaocegege](https://github.com/gaocegege) 和 [@codeworm96][]，以及工作一年的 Go 工程师 [@hawkingrei][] 一起组成了队伍 ，参加了这次 hackathon，凭借 [killy: Play TiDB in Minecraft!](https://github.com/prism-river/killy) 取得了二等奖以及 PingCAP 赞助的专项奖。这篇文章是 [@gaocegege](https://github.com/gaocegege) 第一人称视角的 hackathon 记录。\n\n## 好名字是成功的一半\n\n先从开头说起，最开始，是从 TiDB Contributor 的微信群中，看到了 PingCAP 小狼发的链接，知道了这个 hackathon。因为之前 hackathon 中的一些不好的体验，所以当时对是否参加这个比赛还是持观望态度。但是考虑到自己马上就要毕业了，需要多多地接触公司为一年后的实习还有两年后的工作做一些铺垫，在赞助公司里有好多是自己比较感兴趣的公司，于是拉拢了与我同在软院的大四学弟 [@codeworm96][] 和之前在 [Processing.R](https://github.com/gaocegege/Processing.R) 项目上有过合作的 Go 工程师 [@hawkingrei][] 组队报名了。\n\n在报名的时候，组织方要我们提交项目名和队名，于是我们提交了两个特别二次元的名字。团队的名字“Prismriver”是 [@codeworm96][] 学弟起的，是 [东方 Project 里的三姐妹](https://zh.moegirl.org/%E8%8E%89%E8%8E%89%E5%8D%A1%C2%B7%E6%99%AE%E8%8E%89%E5%85%B9%E5%A7%86%E5%88%A9%E5%B7%B4)。项目的名字“Killy”是 [@hawkingrei][] 提出的，一部硬科幻的男主的名字，据他说男主挺帅的。当时想着反正之后做项目可以改，并没有什么关系。但是后来开始之后也就没那么多闲心去想名字了，索性一直沿用到最后。而因为在 GitHub 上创建 organization 的时候我忘了 Prismriver 是连在一起的还是分开的，错误地使用了 prism-river，Go 对项目的路径是有要求的，直接改动组织名会造成很多麻烦的事情，所以现在在 GitHub 上我们的名字是 [@prism-river][]。\n\n## Idea 是成功的一半的一半\n\n定下了名字，开始想 idea。在石墨文档上记录着我们当时所有想到的 idea，一共有十二个。[@hawkingrei][] 工作了一年多，想到的很多 idea 是跟他的日常工作息息相关的，都是一些可以解决他的痛点的小 idea。因为我之前做过容器和持续集成方面的工作，因此想到的多是跟 Docker 或者 CI 有关系，偶尔有一些跟 TiDB 搭点边，下面的截图是我们想到一部分 idea。\n\n\u003cfigure\u003e\n\t\u003cimg src=\"/images/posts/killy/idea.png\" alt=\"Ideas\"\u003e\n\u003c/figure\u003e\n\n在所有的 idea 中，我们确定了两个候选 idea，按照意愿排序是 TiDBcraft 和 Local Travis Runner Based on Docker。后者是我在实现 [caicloud/cyclone](https://github.com/caicloud/cyclone) 的过程中想到的一个工具。在日常的生活中，我比较常用的 CI 工具是 Travis CI，而因为在很多公司里用的多的还是 Jenkins。所以我在想怎么能把 Travis CI 的 build 放在 Jenkins 里跑，一种比较简单的做法，就是保留 Travis CI 的配置，根据配置运行一个容器，把容器放在 Jenkins 的 build 中去跑。Travis CI 自身的设计使得这样的实现变得非常简单，因为它们有专门的一个组件是做 `.travis.yml -\u003e build.sh` 的转换的，得到 bash 脚本之后，放到 Travis CI 对应语言的官方镜像里去跑就好了。通过这样的实现，只要 repo 里有 .travis.yml 配置文件，就可以在任何支持 Docker 的 CI 工具中去运行。\n\nTiDBcraft 就是我们后来决定实现的 [Killy][]，最初的 idea 来自于之前写的 [dronecraft](https://github.com/gaocegege/dronecraft)，而 [dronecraft](https://github.com/gaocegege/dronecraft) 是受 [dockercraft](https://github.com/docker/dockercraft) 启发。我们想在 Minecraft 中监视整个 TiDB 集群的状态，以及其中表的状态，等等，这可以理解为是一个 Minecraft 里的 TiDB Dashboard。这只是一个概念，在未来甚至可以把整个物理机房都建模在 Minecraft 里，再配合 HoloLens，就可以实现 VR 运维了 /w\\\n\n最后在这两个里投票选一个做的时候，是挺纠结的，最后是 [@codeworm96][] 更倾向于 Just for Fun，于是就拍定了做 TiDBcraft。\n\n\u003cfigure\u003e\n\t\u003cimg src=\"/images/posts/killy/record.jpg\" alt=\"Record\" height=\"500\"\u003e\n\u003c/figure\u003e\n\n## 好的分工是成功的一半的一半的一半\n\n在介绍分工之前，先向大家介绍一下我们的架构。\n\n\u003cfigure\u003e\n\t\u003cimg src=\"https://github.com/prism-river/killy/raw/master/presentation/images/arch.png\" alt=\"Arch\" width=\"500\"\u003e\n\u003c/figure\u003e\n\nKilly 一共有两部分，前端与后端。前端是一个在 [Cuberite](https://cuberite.org/) 服务器中的插件。[Cuberite](https://cuberite.org/) 是一个用 C++ 实现的 Minecraft 服务器，支持使用 Lua 语言实现插件来扩展服务器功能，相信玩 Minecraft 的同学一定都接触过。后端是用 Go 实现的服务器，它负责从 TiDB 集群或者是 Prometheus 中定期拿到整个 TiDB 集群状态，然后转发给前端的插件，前端插件会将其绘制在 Minecraft 世界中，前后端通过一个 TCP 连接通信。整体的架构非常简单。\n\n比赛开始的周六，在 5 分钟内我们三个人就敲定了分工。由我来负责前端插件的实现，因为相对来说我对 Minecraft 比较熟悉而且之前也写过几十行 Lua，尽管我更想写 Go =。= [@codeworm96][] 和 [@hawkingrei][] 负责实现后端的逻辑。更具体来说，[@codeworm96][] 负责与数据库本身的交互，[@hawkingrei][] 负责跟整个集群交互获得集群实时的状态。\n\n## 完整的实现是成功的一半的一半的一半的一半\n\n第一天下午是我们精力最充沛的时候。这个时候我在构思并实现如何在 Minecraft 中显示数据库的表，而他们两个人在调研如何订阅 TiDB 中数据的变更和感知到整个集群的状态变更。对于前者我个人觉得 binlog 可能是可以用的，但是这样有点麻烦，不是在这个 hackathon 中适合的解决方案，所以提出直接暴力地轮询。在 [@codeworm96][] 认可后他开始着手实现，而我快速地实现了第一版 UI。在第一版中，有很多局限性，比如字段最多只支持 4 个，因为 Minecraft 里一个告示板只能显示 4 行。后来在晚上的时候，[@codeworm96][] 实现了第一版逻辑，暴力轮询数据库表然后转发给前端，然后我们进行了集成。而 [@hawkingrei][] 这时也实现了第一版，可以拿到所有 TiDB 集群上实例的静态信息。在晚上 12 点左右，我跟女朋友先走了，回去后，[@codeworm96][] 告诉我接受 SQL 查询的功能也实现了，于是我对接了这部分逻辑，实现了在 Minecraft Console 中执行 SQL 查询的功能，并且做了一个小 trick，把查到的记录高亮显示，就是原本是蓝色羊毛，查询到会变成绿色羊毛。\n\n\u003cfigure\u003e\n\t\u003cimg src=\"https://raw.githubusercontent.com/prism-river/killy/master/presentation/images/table.png\" alt=\"Table UI\" width=\"500\"\u003e\n\u003c/figure\u003e\n\n这些结束后，就选择了睡觉，这个时候大概是 1 点半，睡前与 [@hawkingrei][] 约定好三点起来。[@hawkingrei][] 在彻夜工作，在 3 点半的时候微信告诉我他负责的监控部分已经准备好了，因为 Prometheus 不能获得机器的实际状态，而且查询语句比较弱，有些功能短时间内实现比较难，所以方案换成了直接通过 TiKV，PD 的 REST API 拿到状态数据。可是这个时候我还没起来 =。=\n\n4 点 50 分的时候，我终于意识到再睡下去要背锅了，于是艰难地爬了起来。在 5 点 40 分的时候到了心动网络，从旁边的全家里买了一份三明治，走进了 hackathon 场地，发现大多数人都醒着，还在继续写 `_(:з」∠)_` 随后我们进行了联调，并且修改了第一版与数据库表相关的 UI，变成了更像表的结构，这个时候项目的主要功能都已经实现结束了，这个时候大概是早上 9 点。\n\n随后，我开始着手写 PPT 与 README，进行 demo 的录制。在中午的时候与女朋友玩了一局王者荣耀，结束后就到了答辩的时间。\n\n## 良好的答辩是成功的一半的一半的一半的一半的一半\n\n我们组是第十一个答辩的，所以前面听了几组，感觉很多组的 idea 都非常有趣。轮到自己上场的时候，着重强调了功能，演示了 demo，介绍了架构，吹了一下前景（等于没有）。内容而言自己感觉还可以，就是形象不是特别好，人比较憔悴，因为熬夜头发比较油所以戴着帽子显得不是很尊重，而且也有点驼背。\n\n在听过所有人的答辩后，我觉得大概是已经没戏了。有很多组的 idea 是真的非常棒，要技术有技术要场景有场景。但是可能做游戏天生就在展示上比较有优势，而且做的东西也确实比较有趣，最后误打误撞地拿到了二等奖。这可以说是非常有成就感了，要是放在之前，我会觉得一个 hackathon 的二等奖也就只能说说，但是这次的质量让我觉得有种成就达成的感觉，于是决定把它放在我的简历里 =。=\n\n在整场比赛里，给我留下的印象最深刻的项目是 [xcbuild](https://github.com/setekhid/ketos)，他们希望通过实现一个 `docker build` 的替代品，希望能优化 CI 场景下镜像的构建方式。当然还有很多其他有趣的项目，可以去 [Go Hack 17 主页](gohack2017.golangfoundation.org) 里去看看。\n\n这篇文章就到此为止了 :)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n","cover":"","link":"go_hack_17_参赛日记.html","preview":"\u003cp\u003e2017 年 10 月 21 日，由 Golang Foundation 和 PingCAP 联合举办的 Go Hack 17 在上海心动网络举行，东岳的小哥哥 @gaocegege 和 @codeworm96，以及工作一年的的 Go 工程师 @hawkingrei 一起组成了队伍 ，参加了这次 hackathon，凭借 killy: Play TiDB in Minecraft! 取得了二等奖以及 PingCAP 赞助的专项奖。这篇文章是 @gaocegege 第一人称视角的 hackathon 记录。\u003c/p\u003e\n","title":"Go Hack 17 参赛日记"},{"content":"\n\nKubernetes 是由 Google 捐赠给 [CNCF](https://www.cncf.io/) 的一个容器编排框架，也是目前应用最为广泛的编排框架之一。这篇文章是对 Kubernetes 1.8 中的 Scheduler（以下称为 kube-scheduler）的介绍，如果要阅读本文，需要对 Kubernetes 的基本概念如 pod, node 等有所了解。\n\n## 调度过程\n\n在读代码之前，先对 Kubernetes 整体的调度过程做一个简单的介绍。如果你对 Kubernetes 的调度过程已经熟悉的话，可略过不读。\n\nKubernetes 的调度的目的是把一个 pod 放在它最合适的 node 上去运行，所以调度的过程可以被理解为找一个 node，把 pod 放上去的过程。整个过程可以类比为喜闻乐见的相亲活动，pod 是相亲者，node 是所有可选的相亲对象，而 kube-scheduler 就像是相亲网站，它会负责根据相亲者的要求，找到与其要求最接近的相亲对象。当然这里也有一些不同点，比如每个 node 可以运行多个 pod，但是相亲者与相亲对象绑定以后应该是不能再跟别人进行喜闻乐见的相亲活动了/w\\\n\n出于性能，可扩展性以及其他各个方面的考虑，Kubernetes 的调度分为两个过程，第一个过程叫做 Predicates，第二个过程叫做 Priorities。\n\n在 Predicates 过程中，kube-scheduler 会先执行一系列被称为 predicate 的函数，过滤掉不符合硬性条件的 node。这个环节可以对应相亲活动中的硬条件过滤，比如你想找一个程序员，那相亲网站会帮你过滤掉所有不是程序员的选择。而所有通过了 Predicates 过程的 node，都会进入下一个过程。\n\n在 Priorities 过程中，kube-scheduler 会将所有通过 Predicates 过程的 node 根据自己的标准打分，然后从中选择一个得分最高的 node，将其与 pod 绑定在一起，即在该 node 上运行此 pod。这就好比，相亲网站过滤好了潜在的相亲对象，会再帮你对他们做一个打分，然后推荐给你一个条件最好的给你。（不要问我为什么这么熟练）\n\n\u003cfigure\u003e\n\t\u003cimg src=\"/images/posts/kubernetes/initial-state.png\" alt=\"State\" height=\"300\"\u003e\n\u003c/figure\u003e\n\n文字性的叙述过于单调，这里用图来说明这个过程。在图中一共有 16 台服务器，有着不同的配置。\n\n\u003cfigure\u003e\n\t\u003cimg src=\"/images/posts/kubernetes/algorithm.png\" alt=\"State\" width=\"500\"\u003e\n\u003c/figure\u003e\n\n经过了两个 predicate 后，过滤掉了不满足条件的 node，剩下的 node 都足以运行 pod，这时候就需要 Priorities 过程来找到最适合的那个 node。经过两轮 priority 后，发现了一个最适合的 node，于是 pod 和 node 终于在一起了。在绝大多数情况下，调度就结束了。\n\n这里特此说明，这些图只是为了说明调度的过程，Kubernetes 支持的 predicate 和 priority 的维度并不是 CPU 和 Memory。\n\n## 代码编译\n\n编译 kube-scheduler 只需要运行 \n\n```bash\nmake all WHAT=plugin/cmd/kube-scheduler\n```\n\n就可以了，编译后的结果会被放置在 `${KUBERNETES_PATH}/_output/bin/` 下。\n\n## 浅入理解\n\n接下来就进入了最激动人心的代码阅读部分。kube-scheduler 的入口是在 [`plugin/cmd/kube-scheduler/scheduler.go`](https://github.com/kubernetes/kubernetes/blob/release-1.8/plugin/cmd/kube-scheduler/scheduler.go) 中，因此如果要阅读调度的代码，从这里开始就可以了。\n\n不过 `main` 函数只是告诉你他是怎么启动的，真正的逻辑是从 [`plugin/cmd/kube-scheduler/app/server.go#L68`](https://github.com/kubernetes/kubernetes/blob/release-1.8/plugin/cmd/kube-scheduler/app/server.go#L68) 开始的。在 `Run` 函数中，首先创建了一个 Kuberntes Clientset，这可以理解为是 kube-scheduler 跟 Kubernetes 进行交互的 client，通过它可以拿到集群上 node，pod 等等的信息。然后根据这个 client 创建出对 node 和 pod 等等资源的 informer，这里用了一点点的 trick，来规避 import cycle。以 node informer 的逻辑为例，其创建的逻辑在 [`staging/src/k8s.io/client-go/informers/core/v1/node.go#L47`](https://github.com/kubernetes/kubernetes/blob/release-1.8/staging/src/k8s.io/client-go/informers/core/v1/node.go#L47)。Informer 有点像是观察者模式的样子，会 watch 一种资源的变化，这里水很深，代码挺复杂的，好奇的话建议仔细看看。在创建 kube-scheduler 的过程中，几乎所有的 informer（除了 pod informer），都是通过一个 factory 来做的，这样可以防止频繁地创建。之后就是 scheduler 去获取 leader 的地位，然后执行 [`plugin/pkg/scheduler/scheduler.go#L159`](https://github.com/kubernetes/kubernetes/blob/release-1.8/plugin/pkg/scheduler/scheduler.go#L159) 中的 run 函数，真真正正地开始提供相亲服务了。\n\n而实际上，这个 scheduler 也只是一层抽象，真正的 scheduler 的算法，是由 [`plugin/pkg/scheduler/core/generic_scheduler.go`](https://github.com/kubernetes/kubernetes/blob/release-1.8/plugin/pkg/scheduler/core/generic_scheduler.go) 提供的。如果你想替换原本的调度算法，可以从这个地方入手，也可以自己重新写一个全新的 scheduler，然后在 [`pkg/scheduler/factory/factory.go#L708`](https://github.com/kubernetes/kubernetes/blob/release-1.8/plugin/pkg/scheduler/factory/factory.go#L708) 直接修改创建的逻辑。\n\n在真正的逻辑中，一共有这么两个比较重要的函数：`Schedule`，`Preempt`。分别对应着调度和抢占调度的逻辑。\n\n先看 `Schedule` 函数，之前讲的 Predicates 和 Priorities 过程，就是在这个函数里进行的。在有了之前的积淀，这个函数其实很容易读懂。不过值得注意的是，在 Predicates 过程中，因为数据是没有依赖的，就是说检查每个节点是不是合格的节点其实是独立的事件，所以可以被并发来处理，在目前的实现中是会有 16 个独立的 worker 来处理所有的 Predicates 检查的。在 Priorities 过程中，数据不是完全独立的，因此完全并发的做法是行不通的，于是被实现成了 Map Reduce 的模式。可以不依赖其他的节点进行的计算，在 Map phase 来做，不能的就在 Reduce phase 来做。但是由于之前是完全单线程的实现，为了兼容性，目前在代码里可以看到单线程和并发的实现都存在在代码中。Map Reduce 模式的新实现是从 [`generic_scheduler.go#L404`](https://github.com/kubernetes/kubernetes/blob/release-1.8/plugin/pkg/scheduler/core/generic_scheduler.go#L404) 开始的，Map phase 的做法跟之前的 Predicates 是一样的，而 Reduce 是单线程来做的。\n\n值得注意的是，有一些 priority 函数不太适合用 Map Reduce 模式来进行处理，如果你感兴趣的话可以看看 [issues#51455](https://github.com/kubernetes/kubernetes/issues/51455)，这里有一些关于这些函数的讨论。\n\n再看看 `Preempt` 函数，这是一个比较新的 feature，支持 Pod 的抢占，具体的设计可以参见 [design proposal](https://github.com/kubernetes/community/blob/master/contributors/design-proposals/scheduling/pod-preemption.md)。大致的意思是说如果一个高优先级的 pod 没地方跑了，就杀掉一个在跑的低优先级的 pod，优先运行高优先级的 pod。在实现上，首先调度器会先去寻找那些可能运行这个 pod 的 node，如果 node 是因为一些诸如 selector 不 match 之类的问题被过滤掉了，那是无解的，但是如果是不是因为特别硬性的要求，而是因为资源不够运行这个 pod 之类的，那就列为潜在的合格 node。然后，调度器会在所有潜在的合格 node 上寻找可以被杀掉的 pod，如果不止一个 node 在杀死 pod 后可以满足要求，那就再经过一番选择，主要是选出优先级最低的 pod 所在的 node。选择的过程在 [`generic_scheduler.go#L501`](https://github.com/kubernetes/kubernetes/blob/release-1.8/plugin/pkg/scheduler/core/generic_scheduler.go#L501)，并不难懂。\n\n总体来说，我觉得读起来还是很简单的，本文就到此为止，系列下一篇应该是对 [Nomad](https://www.nomadproject.io/) 的介绍，大概应该就是在最近吧。\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n","cover":"","link":"浅入了解容器编排框架调度器之_kubernetes.html","preview":"\u003cp\u003eKubernetes 是由 Google 捐赠给 CNCF 的一个容器编排框架，也是目前应用最为广泛的编排框架之一。这篇文章是对 Kubernetes 1.8 中的 Scheduler（以下称为 kube-scheduler）的介绍，如果要阅读本文，需要对 Kubernetes 的基本概念如 pod, node 等有所了解。\u003c/p\u003e\n","title":"浅入了解容器编排框架调度器之 Kubernetes"},{"content":"\n\n[Processing.R](https://github.com/gaocegege/Processing.R) 是我在 [Jeremy Douglass](http://jeremydouglass.com/) 指导下，为 [Processing](https://processing.org/) 实现的一个 R 语言模式，这是一个 [Google Summer of Code 2017](https://summerofcode.withgoogle.com/projects/) 项目。这篇文章会讲一讲它的应用，以及实现。\n\n至于这篇文章的受众，我也不是很清楚 ┑(￣Д ￣)┍ 爱看就看看吧 =。=\n\n## Processing 是什么\n\n这里有一篇文章：[Processing是干嘛的？艺术家学编程能做什么？](https://zhuanlan.zhihu.com/p/25432507)，我个人觉得介绍的很到位。我这里就再稍微说一下，我对 Processing 的看法。\n\nProcessing 从功能上而言，是一个做 creative coding 的编程语言，Processing 的 IDE 也直接被称作 Processing Development Environment（缩写 PDE）。\n\n从一个软件工程师的角度来讲，Processing 跟传统的编程语言最大的不同在于，一个完整的 Processing 程序（在 Processing 的语境中，完整的程序被称作 Sketch），一定会有一个图形化的输出。这个输出可以是 2D 图形，3D 图形，也可以是动画，等等。Processing 本身是用 Java 实现了一个编译器，其本身的语法也是跟 Java 几乎一致，因此可以把它当做 Java 的一个 DSL（对语言学不是很懂啦）。Processing 为了使得用户能够更好地进行图形化编程，实现了很多简练的函数，使得寥寥数行就可以实现一个非常简单的图形化应用。\n\n从一个艺术家的角度来讲，因为我不是一个艺术家，我也不知道怎么讲，所以就随便讲讲。绝大多数艺术家，在我看来，在写代码的能力上可能稍微有所欠缺（希望没有冒犯到你）。因此，Processing 对于他们而言，最吸引人的点应该是在于其简单易用。\n\n### 一个简单的例子\n\n这里以一个非常简单的例子介绍 Processing 可以做的事情。\n\n```\nvoid setup() {\n  size(640, 360);\n  background(102);\n}\n\nvoid draw() {\n  // Call the variableEllipse() method and send it the\n  // parameters for the current mouse position\n  // and the previous mouse position\n  variableEllipse(mouseX, mouseY, pmouseX, pmouseY);\n}\n\n\n// The simple method variableEllipse() was created specifically \n// for this program. It calculates the speed of the mouse\n// and draws a small ellipse if the mouse is moving slowly\n// and draws a large ellipse if the mouse is moving quickly \n\nvoid variableEllipse(int x, int y, int px, int py) {\n  float speed = abs(x-px) + abs(y-py);\n  stroke(speed);\n  ellipse(x, y, speed, speed);\n}\n```\n\n先讲效果，这段代码会根据鼠标的位置和鼠标移动的速度在画布上不停的画圆。\n\n在代码中可以看到三个函数，其中 `setup` 和 `draw` 是内置函数，就像是传统编程语言中的 main 函数一样，是整个程序的入口。`setup` 会进行一些设定，比如画布的大小，以及背景颜色。而每当需要绘制新的一帧时，Processing 就会调用 `draw` 函数。因此如果 `draw` 函数每次调用结果都一样，那就是一个静态的图形，如果是不一样的，得到的就是动态的效果。`variableEllipse` 是负责绘制圆形的函数。其参数是鼠标的当前坐标和上一帧的坐标，它会根据坐标计算速度，随后去绘制圆形。如果你感兴趣的话，可以去 [Examples - Pattern](https://processing.org/examples/pattern.html) 亲自试试效果 :)\n\n## Processing.R 是什么\n\n之前提到 Processing 是基于 Java 的 DSL，而且是运行在 JVM 上的。而诸如 Python, Ruby, R 等等语言，也都有在 JVM 上的实现，因此 Processing 也可以通过切换模式的方式来使用其他语言来写 Sketch 的逻辑。\n\n而 [Processing.R](https://github.com/gaocegege/Processing.R)，就是利用 [renjin](http://www.renjin.org/) 实现的 Processing 在 R 语言上的支持。\n\n\u003cfigure\u003e\n\t\u003cimg src=\"https://github.com/gaocegege/Processing.R/raw/master/raw-docs/img/editor.png\" alt=\"Processing.R\" width=\"500\"\u003e\n\u003c/figure\u003e\n\n\u003cfigure\u003e\n\t\u003cimg src=\"https://github.com/gaocegege/Processing.R/raw/master/raw-docs/img/demo.gif\" alt=\"Processing.R Demo\" width=\"200\"\u003e\n\u003c/figure\u003e\n\nProcessing 在 R 语言上的实现，依赖了一个 JVM 上的 R 解释器，每当 Processing 需要调用 draw 等等函数时，都会转而执行 R 代码中相对的定义。目前，Processing.R 支持了绝大多数 Processing 的语法，与此同时支持 Processing 自身众多的库以及 R 语言的包（两者只测试了部分）。这使得 Processing.R 能够在拥有便捷的图形化能力的同时，使用 R 语言中各种方便的包。\n\n## 下载与安装\n\nProcessing 本身下载和安装都特别简单，而且是多平台的，在[此处](https://processing.org/download/)即可找到适合你的版本。而在 Processing 的 Contribution Manager 中的 Modes 一栏中，可以下载 Processing.R。随后在主界面右上角的下拉框中选择 R 即可。\n\n\u003cfigure\u003e\n\t\u003cimg src=\"https://user-images.githubusercontent.com/5100735/29493417-df2b614e-85c7-11e7-98c5-d9f20cf780a4.PNG\" alt=\"下载与使用\" width=\"500\"\u003e\n\u003c/figure\u003e\n\n目前 Processing.R 仍然会积极地进行维护，如果你感兴趣，可以与我联系，还有很多坑等着呢，而且也可以以这个项目为蓝本，拿去申请下一年的 Google Summer of Code，总而言之欢迎各种形式的贡献。原本想做个标题党，发现自己没有 UC 小编的能力，只好找了这么一个不明所以的标题，谢谢你还不辞辛劳地点进来看看 :)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n","cover":"","link":"安利时间:_processing_+_r.html","preview":"\u003cp\u003eProcessing.R 是我在 Jeremy Douglass 指导下，为 Processing 实现的一个 R 语言模式，这是一个 Google Summer of Code 2017 项目。这篇文章会讲一讲它的应用，以及实现。\u003c/p\u003e\n","title":"安利时间: Processing + R = ?"},{"content":"\n\n[Unikernels: Beyond Containers to the Next Generation of Cloud](http://www.oreilly.com/webops-perf/free/unikernels.csp) 是 [Russ Pavlicek](https://www.linkedin.com/in/rcpavlicek/) 的一本动物书（虽然是 O'Reilly 的，但是封面不是动物，是石榴），这本书对 Unikernel 有着比较全面的介绍，而且电子书是免费的，值得一读。\n\n## 啥是 Unikernel？\n\n从 2014 年以来，容器以一种不可逆转的态势席卷了全球，Unikernel 是很多人眼中的下一个容器。如果要了解什么是 Unikernel，首先需要了解什么是 kernel，kernel 是操作系统中的一个概念。应用要运行起来，是肯定要跟硬件打交道的，但是如果让应用都直接操作硬件，那一定是一场灾难。那内核就是在应用与硬件中间的一层抽象，内核提供了对底层硬件的抽象，比如把硬盘抽象成了文件，通过文件系统进行管理。传统的内核会将所有的硬件抽象都实现在其中，其中的代表就是 Linux，这样的内核被称为宏内核（Monolithic Kernel)。在宏内核中，所有的模块，诸如进程管理，内存管理，文件系统等等都是实现在内核中的。这样虽然不存在通信问题，但是任何一个模块的 bug 会使得整个内核崩溃。\n\n于是学者们提出了微内核（Micro Kernel）的概念，在内核中只保留必要的模块，比如IPC，内存管理，CPU调度等等。而其他，诸如文件系统，网络IO等等，都放在用户态来实现。这样会使得内核不那么容易崩溃，而且内核需要的内存小了。但是由于模块间的通信需要以 IPC 的方式进行，因此有一定的 overhead，效率不如很莽的宏内核。\n\n那后来又有了混合内核（Hybrid Kernel)，把一部分不常使用的内核模块，或者是原本需要的时间就很长，因此 IPC 的 overhead 看起来就不是那么夸张的功能，移出内核，而其他的就会放在内核里。\n\n再后来还有 Exokernel，但是太长了就不讲了，这部分内容在 [CSP 课堂笔记之 UniKernel](http://gaocegege.com/Blog/csp/unikernel) 一文中有更详细的解释。\n\n直接说 Unikernel，[Unikernel 的官方解释](http://unikernel.org/)是\n\n\u003eUnikernels are specialised, single-address-space machine images constructed by using library operating systems.\n\n翻译一下就是\n\n\u003eUnikernel 是专用的，单地址空间的，使用 library OS 构建出来的镜像\n\n其最大的卖点就是在，没有用户空间与内核空间之分，只有一个连续的地址空间。这样使得 Unikernel 中只能运行一个应用，而且对于运行的应用而言，没有硬件抽象可言，所有的逻辑，包括应用逻辑和操作硬件的逻辑，都在一个地址空间中。\n\n## 这样有啥好？\n\n哦，原来 Unikernel 就是一个单一内存空间的内核镜像，其中只能有一个应用在运行，那这样有啥好呢，为啥值得我放弃 Linux 而用你这么一个看上去像是阉割版的内核呢？好处就在，小，快，安♂全 /w\\\n\nUnikernel 镜像都很小，由 [MirageOS](https://mirage.io/) 实现的一个 DNS server 才 184KB，实现的一个 web server 674 KB，小到恐怖的程度。\n\n然后就是快，启动很快。因为镜像都很小，所以起停都在毫秒级别，比传统的 kernel 要快多了。\n\n最后是安全，一般来讲，小的东西相对而言比较安全。Unikernel 中没有 Shell 可用，没有密码文件，没有多余的设备驱动，这使得 Unikernel 更加安全。\n\n## 开发测试与传统有啥不同？\n\nUnikernel 在真正实践中，如何开发与测试是一个值得关注的问题。在开发过程中，开发者可以假定自己在传统的操作系统上进行开发，而所有内核相关的功能，暂且由开发机的操作系统提供。\n\n而在测试环境中，大部分 Unikernel 的实现会将应用代码与需要的内核模块构建成 Unikernel 后，再将其跑在一个传统的操作系统上，利用传统操作系统上的工具来测试 Unikernel。以 [Rumprun](https://github.com/rumpkernel/rumprun) 为例，它可以通过 KVM/QEMU 来运行一个 Rumprun Unikernel VM，随后用 Host OS 上的 GDB 来对其进行调试，具体细节可见[此处](https://github.com/rumpkernel/wiki/wiki/Howto:-Debugging-Rumprun-with-gdb)。关于调试就介绍到此，如果你想了解更多，[Hacker News 上的这个 post](https://news.ycombinator.com/item?id=10954132) 可能会给你一些启发。\n\n在发布阶段，这是 Unikernel 最简单的事情了。Unikernel 最后的产物就是一个 kernel image，可以在 Hypervisor，Bare Metal 等等各种环境上运行。\n\n所以可以看到，其中 Unikernel 在软件过程中与传统方式最大的不同就在于调试与测试。而在发布的阶段，传统的方式可能发布的是一个应用，时髦一点那一个容器镜像，而 Unikernel 则是一个高度定制化的 kernel。\n\n## 目前的 Unikernel 项目\n\n介绍完 Unikernel，接下来将介绍下目前比较成气候的 Unikernel 项目，Unikernel 的实现大部分都是语言特定的。因为涉及到具体语言的运行时，所以很难有一个项目可以适配所有的技术栈。\n\n[MirageOS](https://mirage.io) 应该是名气最大的一个 Unikernel 项目，它是使用 OCaml 进行开发的，也是要求开发者懂 OCaml 才行。与其他 Unikernel 相比，它非常成熟，而且有一些[论文](https://mirage.io/wiki/papers)，对钟爱论文的同学非常友好。\n\n[HaLVM](https://github.com/GaloisInc/HaLVM#readme) 也是一个比较早的 Unikernel 项目，它可以帮助 Haskell 程序员们把自己的 Haskell 程序构建成 Unikernel。如果你不会 Haskell，那就算了 =。=\n\n[ClickOS](http://cnp.neclab.eu/clickos) 是一个比较独特的项目，他也非常古老了，但是原本 Click 并不是以 ClickOS 的形式出现的，原本它只是一个支持定制的 router，后来就变成了 ClickOS，一个基于 Unikernel 的 router。它也有很多[论文](http://www.read.cs.ucla.edu/click/publications)，大部分都是关于 Click 本身，而不是 Unikernel 实现的。\n\n[Rumprun](https://github.com/rumpkernel/rumprun/) 也是一个非常独特的项目，其利用了 [Rump Kernel](http://rumpkernel.org/)，理论上 POSIX 兼容的程序，都可以用 Rumprun 来构建成 Unikernel。\n\n如果这些还不能满足你的好奇心，[Open source work on unikernels](http://unikernel.org/projects/) 上列出了众多的 Unikernel 项目，如有需要还请自行浏览。\n\n## Unikernel, Docker，Hyper 与 Linuxkit\n\n对 Unikernel 的介绍就是这些了，最后再谈谈自己对目前很火的一些概念的看法，以及它们之间的联系。\n\nUnikernel，在我看来，是另一种形式上的容器。在一个 Unikernel 中，只能运行一个应用，这与容器的哲学不谋而合。但现在容器最吸引人的特性并不是它的便捷，而是在它的分发。Docker 让我们看到了，原来应用的分发可以这么无痛。而 Unikernel 与容器相比，虽然可以做的更小更安全，而且也不需要有 Docker Daemon 这样的后台程序存在，甚至不需要 Host OS，或者 Hypervisor，但是它一是与传统的软件过程有较大的出入，二是在分发等等方面不能做到像容器那样方便。所以它目前肯定不会成为主流的应用分发方式，还需要进一步探索。\n\n为了能够让 Unikernel 尽快进入生产环境，有一项工作很值得关注。\n\n![](/images/posts/unikernel/unikernel.png)\n\n在 Unikernel 里运行一个 Docker Container，想法很美好，但是同样也有很多问题。这样其实并没有利用到容器便于分发的优势，也没有完全发挥 Unikernel 的优势，我觉得这不是未来。不过作为一种折中方案值得一看，可惜从 DockerCon 15 之后就没听见什么动静了。\n\nHyper Container 的技术特别独特，之前在 [Docker 与 Hyper](http://gaocegege.com/Blog/docker-rambles) 一文中介绍过，这里不再多说。他们的实现很完整，有对标 runc 的 runv，有扩展 Kubernetes 中 container runtime 的 frakti，虽然我没有尝试过，但是我觉得是比 Docker in Unikernel 更加可行的方案，讲道理很有前途。\n\nLinuxkit 是 Docker 改名 Moby 后随之发布的一个项目。Linuxkit 严格来说是一个构建操作系统的工具集，可以用来构建 Unikernel，但是也可以用来构建最小化的 Linux Kernel，目前还不知道要往什么方向发展。\n\n这些概念或多或少都有相互重叠的部分，也没有谁一定胜过谁的说法，但都有一个特点：有趣。它们都有自己不同的应用场景，本来嘛，Docker 也不是银弹。\n\nPS：本文都是纸上谈兵，作者本人并无对 Unikernel 在生产环境中的使用经验（应该暂时也没有人有），大家看看就好，如有疏漏还请不吝指教:)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n","cover":"","link":"unikernel:_从不入门到入门.html","preview":"\u003cp\u003eUnikernels: Beyond Containers to the Next Generation of Cloud 是 Russ Pavlicek 的一本动物书（虽然是 O\u0026rsquo;Reilly 的，但是封面不是动物，是石榴），这本书对 Unikernel 有着比较全面的介绍，而且电子书是免费的，值得一读。\u003c/p\u003e\n","title":"Unikernel: 从不入门到入门"},{"content":"\n\n本文的受众主要是想在暑假找点事情做，挣点外快的同学，亦或是想积累一下参与真实软件开发经验的同学。\n\n## 背景介绍\n\n[Google Summer of Code](https://developers.google.com/open-source/gsoc/)（下称作 GSoC）是谷歌组织并提供经费，面对全球（绝大多数国家）在读学生的在线编程项目。它的[官方介绍](http://write.flossmanuals.net/gsocstudentguide/what-is-google-summer-of-code/)是：\n\n\u003e Google Summer of Code (GSoC) is a global program that matches students up with open source, free software and technology-related organizations to write code and get paid to do it! The organizations provide mentors who act as guides through the entire process, from learning about the community to contributing code. The idea is to get students involved in and familiar with the open source community and help them to put their summer break to good use.\n\n即是：\n\n\u003e Google 编程之夏是一个全球性项目，旨在为学生们和开源、自由软件、技术相关的组织建立联系，让学生们贡献代码并获得报酬！组织会提供导师，在学生从熟悉社区到贡献代码的整个过程中提供指导。这个想法的目的是让学生们参与和熟悉开源社区，并帮助他们充分利用暑假时间去得到锻炼。\n\n整个活动的流程是这样的：在每年的二月末，GSoC 会公布一个 Mentoring organizations 的列表，比如 [2017 Organizations](https://summerofcode.withgoogle.com/organizations/)，这个列表是受到谷歌认可的开源社区或者组织。随后学生可以从列表中挑选出适合自己的 organization，并且在 organization 的 ideas list 中找出自己感兴趣，觉得可以胜任的 idea，提出申请。一个学生最多可以申请 5 个 idea，申请在 3 月末开始，4 月初结束。在 5 月份，Google 会公布所有入选的学生，社区会给每个学生分配一个或多个 mentor，mentor 负责指导学生的工作，并评估学生的工作是否满足了社区的要求。\n\n公布之后，会有持续一个月的 Community Bonding Period，在这个阶段学生需要尽快融入社区，跟自己的 mentor 建立联系，熟悉社区工具链，交流工具等等。6 月份开始正式的开发工作。开发工作一共有三个阶段，同时也会有三个 evaluation。每个阶段大约一个月，会有一个小目标，如果 mentor 认为你完成了阶段性的目标，Google 会在每次 evaluation 结束后发放奖金。三次一共的奖金在 1200 - 6600 美刀之间。具体的数额跟所在地的 [Purchasing Power Parity](https://developers.google.com/open-source/gsoc/help/student-stipends) 有关,中国是 3600 美刀。\n\n整个项目大概在 9 月份结束，但是社区的期望肯定是学生能够继续进行贡献，这也是他们获得新的 contributors 的一个重要途径。并且在持续贡献后，学生可以在来年的 GSoC 时申请成为 mentor，虽然 mentor 没有奖金，但是有一个 Google 组织的 Mentor Summit，据说就是公费旅游。\n\n## 名词解释\n\n因为在申请的时候不同的社区对于同一个对象的叫法都有所不同，所以这里列一些常见的名词的解释。\n\n| 名词        | 解释           |\n| ------------- |-------------|\n| Organization，组织，社区    | Google 公布的 Mentoring organizations 中的组织，可以接收学生参与 GSoC |\n| 学生，申请者    | 申请参加 Google Summer of Code 的学生 |\n| Slots         | 社区可以接收的学生数量，由 Google 决定 |\n| Mentor，导师   | 学生申请成功后社区指定的导师，指导学生的具体工作，以及负责评估完成度  |\n| Stipend，奖金，奖励   | 在学生完成阶段性目标后由 Google 发放的奖金，具体的数额跟所在地的 [Purchasing Power Parity](https://developers.google.com/open-source/gsoc/help/student-stipends) 有关  |\n| Evaluation    | 阶段性检查，mentor 会检查学生有没有达成阶段性成果，影响奖金发放 |\n\n## 申请之前的准备\n\n申请是一个比较漫长的过程，如果想更加稳妥一点，建议不要在谷歌公布 Mentoring organizations 列表后再进行准备，而是要提前选定一个或几个社区，进行持续的贡献和交流，尽可能混一个脸熟。这样与后期才开始准备申请的同学而言就有了很大的优势。除此之外，要日常性地多给开源项目做贡献。在申请的时候很多社区会要求学生提供其开源贡献的经历，无论是不是对自己社区的。这时如果你已经是其他社区的积极贡献者了，那无疑是会加分的。\n\n对于社区的选择，如果你偏向保守，可以多回顾往年的列表，有一些组织是雷打不动的，比如 Python Software Foundation, Apache 这些老牌开源社区，这些相对于其他组织，有更大的可能被谷歌选中。如果你喜欢高风险，可以事先问问社区是否有申请 GSoC organization 的打算，如果有，而且你也看好，可以选择这样的社区进行贡献。\n\n至于贡献的时间，窃以为比较理想的时间是前一年的 12 月份或者同年的 1 月份开始，就要尝试着去给社区做一些微小的工作。这些工作包括但不限于：\n\n* 贡献代码，无论什么社区，都喜欢高质量的 PR\n* Review PR，给别人的 Review 点赞\n* 提交 Bug\n* 添补更新文档\n* 在 IRC 里解决别人的问题\n\n在贡献的过程中，要注意交流，不要只是提交了就走人了，最好是可以时刻跟进，及时回复别人的信息也是一种表明你的热情的方式。\n\n## 申请\n\n### Organization 介绍\n\n申请真正开始于谷歌公布的 Mentoring organizations 列表，这里大致介绍下其内容。\n\n![](/images/posts/gsoc/processing-org.png)\n\n以 2017 年 GSoC 其中的一个开源组织 [The Processing Foundation](https://summerofcode.withgoogle.com/organizations/4962961559912448/) 为例，介绍一下 GSoC 主页上 organization 的页面布局。每个 organization 都会有一段介绍性的文字，这个不是很关键。右边的一栏是比较重要的，其中 Technologies 是方便大家在搜索 organization 的。上面的 VIEW IDEAS LIST 比较重要，一般来说每个社区会事先提出一些他们期望的 idea，学生可以就这些 idea 进行申请。当然社区也鼓励学生提出自己的 idea。其下的 Chat 和 Email 一般来说会写明该社区常用的交流工具，在申请的过程中往往需要频繁地与社区相关人员交流。\n\n### 正式申请\n\nProposal，是一个申请时很关键的材料。它是学生在申请时需要提交的一个设计文件，在其中，学生往往需要写明自己的背景（学术背景，开源贡献经历等），对 idea 的了解与认识，以及大致的实现思路和方法。Proposal 的书写是没有定式的，只要可以突出你的长处就好，这是社区对你了解的唯一途径，所以需要你把自己所有的优势都要写在其中。\n\n[Proposal for Processing.R](https://docs.google.com/document/d/1b0HhRVKtCJkDaxP9dfSwzthzX0FRv6Y_0Yk58r634TA/edit?usp=sharing) 是我在申请时的一份 Proposal，可以列为参考，介绍下常规的写法。\n\n首先是 Project Description，这个部分就是让社区知道你对 idea 没有理解错，你深刻地了解这个 idea 想做的是什么。三言两语就好了。\n\n然后是 Implementation，我个人觉得是比较重要的部分。要向社区证明你已经有了完整的实现思路，现在差的就是写代码实现而已了。\n\n其次，是 Development Process，社区肯定更喜欢那些风险低，feature 吸引人的申请。一个好的 schedule 可以让社区相信你是真的已经做好了准备。精确到天自然最好，但是基本来说比较难，周和月都是不错的选择。不过有一点需要注意，不要把所有时间都安排的满满的，还是需要有一些 buffer 的。不然看起来太假了 =。=\n\n最后是 About Me，因为我对于申请的项目而言，没有什么积累，而且没有相关领域的贡献，所以把这一项放在了最后。如果你是申请 Kubernetes，而日常是 Docker 的 contributor，那把 About Me 放在最前面是更好的选择，完全看申请而定。\n\n一般来说这些是都要有的，还有一些其他的，社区特定的要求，这个也是要注意的。这里还有一些我认为写的比较好的 proposal：\n\n* [Integrate Unikernel Runtime](https://docs.google.com/document/d/1Vld4j0B-wk1A1827gIc5fzWHJlzQVqcYQnCAKJwe_ZM/edit?usp=sharing)\n* [Optimization of Distance Between Methods in Single Java Class](https://docs.google.com/document/d/1lWXpWhUN6cE06sjQANjWxamc_X3ddbSphTRSofChLyk/edit?usp=sharing)\n\n感兴趣也可以看下。\n\n### Community Bonding Period\n\n走到这一步，离拿钱就不远了，因为 GSoC 申请比完成更难。在 Community Bonding Period，你需要跟自己的 mentor 建立联系，积极融入社区等等，但是没有量化的标准，这个就不再多说了。\n\n### 开始写码\n\n写码这个，不同的项目有不同的要求。有一部分项目是给开源的 repo 贡献代码，因此要走整个 review 的流程，这想必大家都比较熟悉，不再多说。但是还有一部分项目，是 standalone 的，就是自己开了个 repo，自己写，比如我申请到的 [Processing.R](https://github.com/gaocegege/Processing.R)。这就会有很多问题，这里也着重说一下对于这一类项目的建议。\n\n首先，要明确之前 proposal 里写的 schedule 只是为了给社区信心的，事实上在开始写码之前，mentor 会跟你重新制定计划。所以如果你在 Community Bonding Period 写了很多 feature，很可能没有用，因为 mentor 说不定会给你重新制定要求。\n\n关于 standalone 的项目，跟 mentor 以及社区其他成员的交流是很关键的。因为你的 repo 别人都是没有 watch 的，所有的变动，可能只有你和你的 mentor 知道。如何让社区里的其他人看到你的贡献，非常重要。所以尽可能多在 IRC 里跟大家分享你遇到的问题，或者你的项目中的新 feature，可能会让你感觉到自己不是玩单机游戏。\n\n其次就是要尽早引入 CI，并且所有变动都以 PR merge 的方式进行，以保证代码质量。一个人的项目，质量很容易滑坡，CI 和 PR 可以让 mentor 对你的代码有一个很好的 review 体验，他也会更加积极一点。\n\n最后是不要太肝。因为自己的项目，每个 PR 的生命周期都是由自己负责的，很容易就会进入疯狂开发的状态，但是记住上面说的，在开始写码之前，mentor 会跟你重新制定计划 :)\n\n### Evaluation 与奖金发放\n\nEvaluation 是一个双向的评估，mentor 会评估学生的工作完成度如何，学生会评估社区和 mentor 对自己的帮助是否到位，学生对社区的评估可能会影响社区明年能够参加 GSoC 以及 slots 的数量，mentor 的评估决定学生能否拿到奖金。\n\n如果通过了 evaluation，奖金会在几天内到账。\n\n## 注意事项与 Tips\n\n1. **只有**学生才可以申请 GSoC。\n1. 一般来说 GSoC 主页需要科学上网才能访问。\n1. 时差问题是申请的时候需要注意的问题，这个需要格外注意，每年都有人错过申请。\n1. 奖金的发放是通过 [Payoneer](https://www.payoneer.com/home/) 发放的，如果是非美元账户，需要支付 4% 左右的换汇费用。\n1. 第一次入选 Mentoring organizations 的组织原则上只有 1 个或者 2 个 slots。\n\n## 结语\n\n这是一篇摸鱼作，希望能够对各位有所帮助。其实大家在选择开源社区的时候可以多问问有经验的人，尽可能选择一个友好的社区作为开始，这样会在开源的路上走的远一点。。\n\n## 相关文章\n\n* [Google 编程之夏(GSoC)：海量优质项目，丰厚报酬，你竟然还不知道？](https://zhuanlan.zhihu.com/p/27330699)\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n","cover":"","link":"google_summer_of_code_申请指南.html","preview":"\u003cp\u003e本文的受众主要是想在暑假找点事情做，挣点外快的同学，亦或是想积累一下参与真实软件开发经验的同学。\u003c/p\u003e\n","title":"Google Summer of Code 申请指南"},{"content":"\n\n这篇文章是写给[东岳网络工作室](https://github.com/dyweb)的小伙伴们的 (广告:欢迎在交大的同学加入)，适用于有一定数据库背景并且想要了解时间序列数据库的同学。\nPS: 中文版是在[英文版](/introduction_to_time_series_database.html)之后写的，很生硬，请见谅。\n\n目录\n\n- 什么是时间序列数据库 (TSDB)\n- 时间序列数据库数据模型\n- 时间序列数据库演变\n- 时间序列数据库类型\n  - KairosDB\n  - InfluxDB\n- 热点话题\n  - 低延迟\n  - 数据\n  - 元数据索引\n  - Tracing\n\n## 什么是时间序列数据库 (TSDB)\n\n时间序列数据库 `Time Series Database` (TSDB) 相对于关系型数据库 (RDBMS)，NoSQL，NewSQL 还很年轻。\n但是，随着系统监控以及物联网的发展，已经开始受到更多的关注。\n维基百科上对于时间序列的定义是‘一系列数据点按照时间顺序排列’，\n但是我个人的理解是**存储在服务端的客户端历史**。\n时间序列数据就是历史，它具有**不变性**, **唯一性**以及**可排序性**。\n比如`在2017年9月3日21点24分44秒，华东区的机器001的CPU使用率是10.02%`，\n这个值不会像银行存款一样随着时间发生变化，它一旦产生了就不会有更新。\n下一秒的使用率是一个新的数据点，其他机器的使用率在其他时间序列里。\n并且**数据到达服务器的顺序并不影响正确性**，根据数据本身可以直接进行排序和去重。\n客户端发送本地的历史到服务器端，即使服务器端挂掉了，客户端依旧继续他本来要做的事情而不受到影响。\n对于很多客户端来说，发送数据到 TSDB 跟它的本职工作并没有关联。\n比如一个静态文件服务器的主要职责是传送文件而不是上报 HTTP 状态码。\n关系型数据库则起着完全不一样的作用，它是客户端做决定的主要依据，\n这就导致时间序列数据库和关系型数据库的读写规律有很大的不同。\n比如你取钱之前，银行的程序必须从数据库里找到你的那条存款记录，读出你的余额，确认不会透支才能把钱给你，\n然后更新你的余额。\n然而大多数时间序列数据库的客户端是只读(监控系统)或者只写(被监控的系统)，\n并且读取数据是并不是读取特定的某条，而是读取某个时间区间内的大量数据，比如`最近1小时的CPU使用率`远比\n`2017年9月3日21点24分44秒的CPU使用率`有用，脱离上下文的时间序列数据并没有什么作用。\n\n时间序列数据跟关系型数据库有太多不同，但是很多公司并不想放弃关系型数据库。\n于是就产生了一些特殊的用法，比如用 [MySQL 的 VividCortex](https://www.vividcortex.com/blog/2014/12/16/in-case-you-missed-it-building-a-time-series-database-in-mysql/),\n用 [Postgres 的 Timescale](http://www.timescale.com/)。\n很多人觉得特殊的问题需要特殊的解决方法，于是很多时间序列数据库从头写起，不依赖任何现有的数据库,\n比如 [Graphite](https://graphiteapp.org/)，[InfluxDB](https://github.com/influxdata/influxdb)。\n\n## 时间序列数据库演变\n\n时间序列数据库有[很多](https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=All)，\n下面列出的是一些我个人认为具有里程碑意义的数据库。\n很多数据库主页上并没有最初版本的发布日期，因此以 GitHub 上最早的 tag 作为发布日期。\n\n- 1999/07/16 [RRDTool First release](https://en.wikipedia.org/wiki/RRDtool)\n- 2009/12/30 [Graphite 0.9.5](https://github.com/graphite-project/graphite-web/releases/tag/0.9.5)\n- 2011/12/23 [OpenTSDB 1.0.0](https://github.com/OpenTSDB/opentsdb/releases/tag/v1.0.0)\n- 2013/05/24 [KairosDB 1.0.0-beta](https://github.com/kairosdb/kairosdb/releases/tag/v1.0.0-beta2a)\n- 2013/10/24 [InfluxDB 0.0.1](https://github.com/influxdata/influxdb/releases/tag/v0.0.1)\n- 2014/08/25 [Heroic 0.3.0](https://github.com/spotify/heroic/releases/tag/0.3.0)\n- 2017/03/27 [TimescaleDB 0.0.1-beta](https://github.com/timescale/timescaledb/releases/tag/0.0.1-beta)\n\n[RRDTool](https://oss.oetiker.ch/rrdtool/) 是最早的时间序列数据库，它自带画图功能，现在大部分时间序列数据库都使用[Grafana](https://github.com/grafana/grafana)来画图。\n[Graphite](https://graphiteapp.org/) 是用 Python 写的 RRD 数据库，它的存储引擎 [Whisper](https://github.com/graphite-project/whisper) 也是 Python 写的，\n它画图和聚合能力都强了很多，但是很难水平扩展。\n来自雅虎的 [OpenTSDB](http://opentsdb.net/) 使用 HBase 解决了水平扩展的问题。\n[KairosDB](https://kairosdb.github.io/) 最初是基于OpenTSDB修改的，但是作者认为兼容HBase导致他们不能使用很多 Cassandra 独有的特性，\n于是就抛弃了HBase仅支持Cassandra。\n有趣的是，在[新发布的](http://opentsdb.net/docs/build/html/new.html) OpenTSDB 中也加入了对 Cassandra 的支持。\n故事还没完，Spotify 的人本来想使用 KairosDB，但是觉得[项目发展方向不对以及性能太差]((https://labs.spotify.com/2015/11/16/monitoring-at-spotify-the-story-so-far/))，就自己撸了一个 [Heroic](https://github.com/spotify/heroic)。\n[InfluxDB](https://github.com/influxdata/influxdb) 早期是完全开源的，后来为了维持公司运营，闭源了集群版本。\n在 Percona Live 上他们做了一个[开源数据库商业模型正面临危机](https://www.youtube.com/watch?v=Kvf5jWZjw0U)的演讲，里面调侃红帽的段子很不错。\n并且今年的 Percona Live 还有专门的[时间序列数据库单元](https://www.percona.com/live/17/program/schedule/time-series)。\n\n## 时间序列数据库数据模型\n\n时间序列数据可以分成两部分，**序列**和**数据点**。\n序列就是标识符，比如`华东区机器001的CPU使用率`。\n数据点是时间戳和数值构成的数组。\n\n对于序列，主要的目的是方便使用者进行搜索和筛选。\n比如你需要查询`华东区所有机器的CPU使用率`。\n序列 `华东区机器001的CPU使用率` 的标识符是 `name=cpu.usage machine=001 region=cn-east`，\n查询则是 `name=cpu.usage machine=* region=cn-east`。\n为了处理大量的序列，需要建立（倒排）索引来提高查询速度。\n一些时间序列数据库选择使用外部搜索引擎来解决这个问题，比如 [Heroic](https://github.com/spotify/heroic) 使用了 Elasticsearch,\n另一些则选择自己写索引，比如 [InfluxDB](https://github.com/influxdata/influxdb), [Prometheus](https://prometheus.io/)。\n\n对于数据点，有两种模型，一个数组的点  `[{t: 2017-09-03-21:24:44, v: 0.1002}, {t: 2017-09-03-21:24:45, v: 0.1012}]`\n或者两个数组，一个存时间戳，一个存数值。前者是行存，后者是列存（不是列簇）。\n大部分基于现有数据库( [Cassandra](https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=Cassandra), [HBase](https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=HBase) ) 的是第一种。\n对于新的时间序列数据库第二种更为普遍，TSDB 属于 OLAP 的一个子集，列存能有更好的压缩率和查询性能。\n\n## 时间序列数据库类型\n\n时间序列数据库可以分成两类，基于现有的数据库或者专门为时间序列数据写的数据库。\n我们以 [KairosDB](https://kairosdb.github.io/) 和 [InfluxDB](https://github.com/influxdata/influxdb) 为例来分析。\n有很多时间序列数据库是[基于 Cassandra](https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=Cassandra) 的，\n[KairosDB](https://kairosdb.github.io/) 是其中比较早的一个。\n[InfluxDB](https://github.com/influxdata/influxdb) 是专用于时间序列的数据库，他们尝试了很多存储引擎，最后写了自己的 `Time Structured Merge Tree`.\n\n### KairosDB\n\n在看 KairosDB 之前我们先用一个简化版本的预热一下。\n[Xephon-K](https://github.com/xephonhq/xephon-k) 是我写的一个有多种存储后端的时间序列数据库(专门用来对付各种课程大作业)。\n它有一个非常 naive 的基于 Cassandra 的实现。\n\n如果你对 Cassandra 不熟的话，这里有个简单的介绍。\nCassandra 是一个列簇数据库，是谷歌 BigTable 的开源实现。列簇又被称作宽列。\n实质上是一个多层嵌套的哈希表。它是一个行存储，不是列存储。\n一些 Cassandra 的名词可以跟关系型数据库中的对应起来。\nCassandra 中的 `Keyspace` 就是指的 `database`, 比如一个博客和一个网店虽然使用同一个 MySQL 服务器，但是各用一个数据库以进行隔离。\nCassandra 中的 `Table` 是一个哈希表，他的 `Partition Key` 是哈希表的键(也被叫做物理行键)，它的值也是一个哈希表，这个哈希表的键是 `Cluster Key`，\n它的值还是一个蛤希表。\n当使用 CQL 创建一个 `Table` 的时候，主键中的第一个列是 `Partition Key`，第二个列是 `Cluster Key`。\n比如在下面的 CQL 中， `Keyspace` 是 `naive`, `Table` 是 `metrics`，`Partition Key` 是 `metric_name`,\n`Cluster Key` 是 `metrics_timestamp`。\n最内层的哈希表是 `{value: 10.2}`, 如果需要我们可以存更多的值，比如 `{value: 10.2, annotation: '新 bug 上线啦'}`。\n\n````sql\nCREATE TABLE IF NOT EXISTS naive.metrics (\n    metric_name text, metric_timestamp timestamp, value int,\n    PRIMARY KEY (metric_name, metric_timestamp))\nINSERT INTO naive.metrics (metric_name, metric_timestamp, value) VALUES (cpu, 2017/03/17:13:24:00:20, 10.2)    \nINSERT INTO naive.metrics (metric_name, metric_timestamp, value) VALUES (mem, 2017/03/17:13:24:00:20, 80.3)   \n````\n\n![Cassandra Time Series Data model](images/posts/cassandra-tsdb-model.png)\n\n上图显示了使用 Cassandra 存储时间序列数据时 naive 的表结构，\n`Cluster Key` 存储时间戳，列的值存储实际的数值。\n它 naive 之处在于序列和 Cassandra 的物理行是一一对应的。\n当单一序列的数据点超过 Cassandra 的限制(20亿)时就会崩溃。\n\n一个更加成熟的表结构是把一个时间序列按时间范围分区，(KairosDB 按照 3 周来划分，但是可以根据数据量进行不定长的划分)。\n为了存储分区的信息，需要一张额外的表。\n同时在 naive 里序列的名称只是一个简单的字符串，如果需要按照多种条件进行筛选的话，需要存储更多的键值对，并且对于这些键值对需要建立索引以提高查询速度。\n\n下面是完整的 KairosDB 的表结构，`data_points` 表对应的是 naive 里的 `metrics` 表。\n它看上去不像人写的，因为它就是直接导出的，KairosDB 使用的旧版 Cassandra 的 Thrift API 创建表结构，没有 `.cql` 文件。\n\n````sql\nCREATE TABLE IF NOT EXISTS data_points (\n    key blob,\n    column1 blob,\n    value blob,\n    PRIMARY KEY ((key), column1)\n) WITH COMPACT STORAGE;\nCREATE TABLE IF NOT EXISTS row_key_index (\n    key blob,\n    column1 blob,\n    value blob,\n    PRIMARY KEY ((key), column1)\n) WITH COMPACT STORAGE;\nCREATE TABLE IF NOT EXISTS row_key_time_index (\n    metric text,\n    row_time timestamp,\n    value text,\n    PRIMARY KEY ((metric), row_time)\n)\nCREATE TABLE IF NOT EXISTS row_keys (\n    metric text,\n    row_time timestamp,\n    data_type text,\n    tags frozen\u003cmap\u003ctext, text\u003e\u003e,\n    value text,\n    PRIMARY KEY ((metric, row_time), data_type, tags)\n)\nCREATE TABLE IF NOT EXISTS string_index (\n    key blob,\n    column1 blob,\n    value blob,\n    PRIMARY KEY ((key), column1)\n) WITH COMPACT STORAGE\n````\n\n有很多基于 Cassandra 的时间序列数据库，他们的结构大多相同，你可以看[这个列表]((https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=Cassandra))。\n我最近正在写一个如何用 Cassandra 和 Golang 自己写个时间序列数据库的博客，写好之后会把地址更新在这里。\n\n### InfluxDB\n\n[InfluxDB](https://github.com/influxdata/influxdb) 在存储引擎上[纠结了很久](https://docs.influxdata.com/influxdb/v1.3/concepts/storage_engine/)，\nleveldb, rocksdb, boltdb 都玩了个遍，最后决定自己造个轮子叫 `Time Structured Merge Tree`。\n\n`Time Structured Merge Tree` (TSM) 和 `Log Structured Merge Tree` (LSM) 的名字都有点误导性，关键并不是树，也不是日志或者时间，而是 `Merge`。\n写入的时候，数据先写入到内存里，之后批量写入到硬盘。读的时候，同时读内存和硬盘然后合并结果。\n删除的时候，写入一个删除标记，被标记的数据在读取时不会被返回。\n后台会把小的块合并成大的块，此时被标记删除的数据才真正被删除，这个过程叫做 `Compaction`。\n相对于普通数据，有规律的时间序列数据在合并的过程中可以极大的提高压缩比。\n\n下图是一个简化版的 TSM，每个块包含序列标识符，一组时间戳，一组值。\n注意时间戳和值是分开存储的，而不是交替存储的，所以 InflxuDB 是一个列存储。\nInfluxDB 会根据数据来选择压缩的方法，如果可以使用行程编码是最好的，\n否则会使用 [Gorilla]((https://github.com/dgryski/go-tsz)) 中提到的浮点数压缩方法以及变长编码。\n时间戳和数值一般会使用不同的压缩方法，因为时间戳大多是非常大的整数而数值是非常小的浮点数。\n\n````\nchunk\n--------------------------------------------------\n| id | compressed timestamps | compressed values |\n--------------------------------------------------\ntsm file\n-------------------------------------------------------------------\n| header | chunk 0 | chunk 1 | ... | chunk 10086 | index | footer |\n-------------------------------------------------------------------\n````\n\n## 热点话题\n\n### 低延迟\n\n时间序列数据库主要是用来分析的，所以提高响应速度对于诊断生产环境的问题是十分重要的。\n\n最直接的提速方法就是把所有数据都放在内存，Facebook 写了叫 [Gorilla](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf) 的纯内存时间序列数据库发表在 VLDB 上，现在已经开源，改名为 [Beringei](https://github.com/facebookincubator/beringei)（都是猩猩...）。\n\n另一种提速的方法是提前聚合。因为查询中经常需要对一个很长的时间区间取一些粗粒度的值，比如`6月到8月每天的平均CPU使用率`。\n这些聚合值（均值，最大，最小) 都可以在存储数据的时候计算出来。\n[BtrDB](https://github.com/SoftwareDefinedBuildings/btrdb) 和 [Akumuli](https://github.com/akumuli/Akumuli)\n都在内部节点中存储聚合值，这样在很多查询中底层的节点不需要被访问就可以得到结果。\n\n同时一个好的数据传输格式也可以提高响应速度，虽然 JSON 被广泛使用，但是二进制的格式对于有大量数字的数据会显著的提升。\n[protobuf](https://github.com/golang/protobuf/) 可能会是一个更好的选择。\n\n### 处理旧数据\n\n很多时间序列数据都没有多大用处，特别是当系统长时间正常运行时，完整的历史数据意义并不大。\n所以有些数据库比如 [RDDTool](https://oss.oetiker.ch/rrdtool/) 和 [Graphite](https://graphiteapp.org/) 会自动删除高精度的数据，只保留低精度的。\n但是对于很多新的时间序列数据库，在聚合和删除大量旧数据的同时保证系统正常运行并不像删除一个本地文件那样简单。\n如果监控系统比被监控系统还不稳定就比较尴尬了。\n\n### 元数据索引\n\n时间序列的标识符是时间序列数据库里主要的元数据。\n[Heroic](https://github.com/spotify/heroic) 使用 Elasticsearch 来存储元数据，\n查询首先通过 Elasticsearch 来取得符合要求的序列标识符，之后从 Cassandra 根据标识符来读取对应的数据。\n但是维护一个完整的搜索引擎带来的运维压力和增加的通信时间都是不能忽视的。\n因此 InfluxDB 和 Prometheus 就[自己写了倒排索引]((https://fabxc.org/blog/2017-04-10-writing-a-tsdb/))来索引元数据。\n\n### Tracing\n\nInfluxDB 的人写了一篇博客 [Metrics are dead](https://www.influxdata.com/blog/metrics-are-dead/)，\n起因是在一个关于监控的会议 [Monitorama](http://monitorama.com/) 上有人说单纯的监控数据已经不能满足他们复杂的微服务架构了。\n于是 InfluxDB 的人反驳说并不是所有人都在使用大规模的分布式系统，对于很多简单的应用单纯的监控数据已经完全够用了。\n我的看法是**时间序列数据库是可以用来存 Trace 的**。\nTrace 是更加复杂的时间序列数据，把单纯的数值变成一个包含更多信息的对象，它就是一个 Trace。\n并且很多流行的 Tracer 的存储也是使用 Cassandra, 比如 [Zipkin](https://github.com/openzipkin/zipkin)，\nUber 的 [Jaeger](https://uber.github.io/jaeger/)。**更新:** InfluxDB 现在[已经支持存储 Trace 了](https://www.influxdata.com/blog/tracing-the-journey-of-a-transaction-as-it-propagates-through-a-distributed-system/)\n\n由于篇幅限制，有很多话题我们没有涉及，比如压缩，Pull vs Push, 写放大等，在以后的博客中会陆续介绍。\n\n## 参考\n\n- [Awesome Time Series Database](https://github.com/xephonhq/awesome-time-series-database)\n- [Akumuli](https://github.com/akumuli/Akumuli)\n- [Beringei](https://github.com/facebookincubator/beringei)\n- [BtrDB](https://github.com/SoftwareDefinedBuildings/btrdb)\n- [Gorilla](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf)\n- [Grafana](https://github.com/grafana/grafana)\n- [Graphite](https://graphiteapp.org/)\n- [Heroic](https://github.com/spotify/heroic)\n- [InfluxDB](https://github.com/influxdata/influxdb)\n- [Jaeger - Tracer](https://uber.github.io/jaeger/)\n- [KairosDB](https://kairosdb.github.io/)\n- [OpenTSDB](http://opentsdb.net/)\n- [Prometheus](https://prometheus.io/)\n- [RRDTool](https://oss.oetiker.ch/rrdtool/)\n- [Timescale - TSDB using Postgres](http://www.timescale.com/)\n- [VividCortex - TSDB using MySQL](https://www.vividcortex.com/blog/2014/12/16/in-case-you-missed-it-building-a-time-series-database-in-mysql/)\n- [Zipkin - Tracer](https://github.com/openzipkin/zipkin)\n\n## License\n\n- This article is licensed under [CC BY-NC-SA 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/).\n- Please contact \u003cmarketing@dongyue.io\u003e for commerical use.\n","cover":"","link":"时间序列数据库漫谈.html","preview":"\u003cp\u003e时间序列数据库基本概念和热点话题\u003c/p\u003e\n","title":"时间序列数据库漫谈"},{"content":"\n\nThis blog is written for fellow students at [dongyueweb](https://github.com/dyweb),\nso its targeted readers are people who have taken database class and want to know about time series database (TSDB).\n\nTable of content\n\n- What is time series database (TSDB)\n- Time series data model\n- Evolve of time series database\n- Types of time series database\n  - KairosDB\n  - InfluxDB\n- Hot topics in time series database\n  - Fast response\n  - Retention\n  - Meta data indexing\n  - Tracing\n\n## What is time series database (TSDB)\n\nTime series database (TSDB) is relative new compared with RDBMS, NoSQL, even NewSQL.\nHowever it is becoming trending with the growth of system monitoring and internet of things.\nThe [wiki](https://en.wikipedia.org/wiki/Time_series) definition of time series data is *a series of data points indexed (or listed or graphed) in time order*. When it comes to TSDB, I prefer my own definition: **store client history in server for analysis**.\nTime series data is history, it's **immutable**, **unique** and **sortable**. \nFor instance, `the CPU usage at 2017-09-03-21:24:44 is 10.02% for machine-01 in us-east region`, \nit won't change overtime like bank account balance, there will be no update once it's generated, \nthe CPU usage at next second, or from different machine are different data points. \nAnd **the order of data arriving at server does not effect correctness** because you can remove the duplicate and sort by client timestamp.\nClients of TSDB send their history to sever and is still functional when the server is down, \n**sending data to TSDB is not critical for many clients**;\nA http server's main job is serving content instead of reporting status code to TSDB.\nHowever, RDBMS is treated as single source of truth and effect client's critical decision making. \nThis lead to very different read and write pattern. \nFor instance, banking application need to query database for user's balance before proceed by reading and updating a single record.\nBut most TSDB clients are either write only (collectors) or read only (dashboard and alerting system). \nAnd when they read, they read in large batch, `show CPU usage of last 1h` is used more often than `show CPU usage at 2017-09-03-21:24:44` \nbecause time series data is not that useful without its context.\n\nTime series data is so different from what popular DBMS used to deal with that people are forced to use their favorite DB in very different ways (i.e. [VividCortex with MySQL](https://www.vividcortex.com/blog/2014/12/16/in-case-you-missed-it-building-a-time-series-database-in-mysql/), [Timescale with Postgres](http://www.timescale.com/)). \nSome decided for special problem special solution is needed, so many TSDBs are written from scratch ([Graphite](https://graphiteapp.org/), \n[InfluxDB](https://github.com/influxdata/influxdb) etc.) without dependencies to existing databases.\n\n## Evolve of time series database\n\nThere are [too many time series databases](https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=All),\nso I just list databases that I personally considered as milestone in the evolving of time series database, \nfeel free to comment the pieces I missed, I can't find the real initial release of many databases so I just use the oldest on github.\n\n- 1999/07/16 [RRDTool First release](https://en.wikipedia.org/wiki/RRDtool)\n- 2009/12/30 [Graphite 0.9.5](https://github.com/graphite-project/graphite-web/releases/tag/0.9.5)\n- 2011/12/23 [OpenTSDB 1.0.0](https://github.com/OpenTSDB/opentsdb/releases/tag/v1.0.0)\n- 2013/05/24 [KairosDB 1.0.0-beta](https://github.com/kairosdb/kairosdb/releases/tag/v1.0.0-beta2a)\n- 2013/10/24 [InfluxDB 0.0.1](https://github.com/influxdata/influxdb/releases/tag/v0.0.1)\n- 2014/08/25 [Heroic 0.3.0](https://github.com/spotify/heroic/releases/tag/0.3.0)\n- 2017/03/27 [TimescaleDB 0.0.1-beta](https://github.com/timescale/timescaledb/releases/tag/0.0.1-beta)\n\n[RRDTool](https://oss.oetiker.ch/rrdtool/) was created to graph network traffic, it ships with graphing tool while modern TSDB normally depends on [Grafana](https://github.com/grafana/grafana) for graphing. \n[Graphite](https://graphiteapp.org/) was created later using python instead of C like RRDTool, its storage engine is called [Whisper](https://github.com/graphite-project/whisper), it's much powerful when it comes to data processing and query, however it does not scale well.\n[OpenTSDB](http://opentsdb.net/) from Yahoo! solves the scale problem by using HBase.\n[KairosDB](https://kairosdb.github.io/) was a fork for OpenTSDB to support Cassandra as an alternative backend, but then they found being compatible with HBase limit the potential of Cassandra, so they dropped HBase and use Cassandra only. \nIronically, [recent release of OpenTSDB](http://opentsdb.net/docs/build/html/new.html) added support for Cassandra.\nThen [Heroic](https://github.com/spotify/heroic) came out because they are [not satisfied with KairosDB's performance and direction](https://labs.spotify.com/2015/11/16/monitoring-at-spotify-the-story-so-far/).\n[InfluxDB](https://github.com/influxdata/influxdb) started with full open source, \nbut then close sourced their cluster version because they need to keep the company running, there is a interesting talk called [The Open Source Database Business Model is Under Siege](https://www.youtube.com/watch?v=Kvf5jWZjw0U) during Percona Live which features [a time series session](https://www.percona.com/live/17/program/schedule/time-series).\n[TimeScaleDB](http://www.timescale.com/) is based on PostgreSQL with a plugin instead of special schema. \n\n## Time series data model\n\nTime series data can be split into two parts, **series** and **data points**.\nSeries is the identifier, like `CPU usage for machine-01 in us-east region`, \ndata points are an array of points where each point is a timestamp and value.\n\nFor series, the main goal is the extensibility for post processing (searching, filtering etc.).\ni.e. If you want `CPU usage of all machines in us-east region`,\nthe identifier of series `CPU usage for machine-01 in us-east region` is `name=cpu.usage machine=machine-01 region=us-east`, \nand the query becomes `name=cpu.usage machine=* region=us-east`.\nIt order to deal with large amount of series and wildcard matching, (inverted) index is needed,\nsome chose to use external search engine like [Heroic](https://github.com/spotify/heroic) is using Elasticsearch.\nSome chose to write their own like [InfluxDB](https://github.com/influxdata/influxdb), [Prometheus](https://prometheus.io/).\n\nFor data points there are two models, an array of points `[{t: 2017-09-03-21:24:44, v: 0.1002}, {t: 2017-09-03-21:24:45, v: 0.1012}]` \nor two arrays for timestamp and values respectively `[2017-09-03-21:24:44, 2017-09-03-21:24:45], [0.1002, 0.1012]`.\nThe former is row store, the latter is column store (not to be confused with column family).\nWhen building TSDB on top of existing databases ([Cassandra](https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=Cassandra), [HBase](https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=HBase) etc.), the former is used more,\nwhile for TSDB written from scratch, the latter is more popular, TSDB is actually a subset of OLAP and columnar format brings higher compression ratio and query speed.\n\n## Types of time series databases\n\nTime series databases can be split into two types, existing databases with time series specific special schema or databases built for time series data from scratch. \nWe use KairosDB and InfluxDB as example for following discussion. \nA lot of TSDB are [built on top of Cassandra](https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=Cassandra), \n[KairosDB](https://kairosdb.github.io/) is the pioneer of them.\n[InfluxDB](https://github.com/influxdata/influxdb) has tried many backends until they came up with their `Time Structured Merge Tree`.\n\n### KairosDB\n\nBefore dive into KairosDB, let's warm up using a simplified version called [Xephon-K](https://github.com/xephonhq/xephon-k).\n[Xephon-K](https://github.com/xephonhq/xephon-k) is a multi backend time series database I wrote for testing out different mechanism of building TSDB. Its immature Cassandra backend is simple and modeled after [KairosDB](https://kairosdb.github.io/).\n\nIf you are not familiar with Cassandra, here is a brief introduction.\nCassandra (C*) is a column family NoSQL database modeled after BigTable, people sometimes call it **wide column**. \nYou can think column family as a map of map of map. It's a row store, not a column store.\nWe can match some concept of Cassandra with RDBMS's.\n`Keyspace` in C* is database in RDBMS, i.e. your blog and ecommerce application use same MySQL Server but create different database for isolation.\n`Table` in C* is a map and `Partition Key` is its key, also known as (physical) row key, which is used to partition data to different nodes.\nThe value of the top level map is also map, and its key is the `Cluster key` (column), its value is also a map.\nWhen creating a table in CQL, the first column in primary key is partition key and the second is cluster key. i.e. In the following CQL, \n`Keyspace` is `naive`, `Table` is `metrics`, `Partition Key` is `metric_name`, `Cluster Key` is `metrics_timestamp`, \nthe inner most map is `{value: 10.2}`, we can have more than one keys for it if needed, i.e. `{value: 10.2, annotation: 'new app deployed'}`\n\n````sql\nCREATE TABLE IF NOT EXISTS naive.metrics (\n    metric_name text, metric_timestamp timestamp, value int, \n    PRIMARY KEY (metric_name, metric_timestamp))\nINSERT INTO naive.metrics (metric_name, metric_timestamp, value) VALUES (cpu, 2017/03/17:13:24:00:20, 10.2)    \nINSERT INTO naive.metrics (metric_name, metric_timestamp, value) VALUES (mem, 2017/03/17:13:24:00:20, 80.3)   \n````\n\n![Cassandra Time Series Data model](images/posts/cassandra-tsdb-model.png)\n\nThe figure above shows a naive schema when using Cassandra to store time series data, \n`Cluster key` is used to store timestamp and column value is the actual value.\nIt is naive because series and Cassandra's physical row is a one-to-one mapping, it won't scale when a single series grows larger than the hard limit of Cassandra (2 billion columns). \n \nA more mature schema would partition a single series by time range (might not be fixed, KairosDB use fixed 3 week time range) into several physical rows, an extra table is needed to keep this partition info. \nAlso the series name in naive schema is just a simple string, in order to filter series by different criteria, attributes (tags) need to be stored, and another table as index is needed to avoid iterate all the series. \n\nKairosDB's schema is listed below, the `data_points` table is same as `metrics` table in naive schema except `key` is \nnot for human like `metric_name` does. The naming of schema looks strange because it is dumped from Cassandra's shell (cqlsh), \nKairosDB didn't use a cql file to create schema like many other does because it was using the old thrift API.\n\n````sql\nCREATE TABLE IF NOT EXISTS data_points (\n    key blob,\n    column1 blob,\n    value blob,\n    PRIMARY KEY ((key), column1)\n) WITH COMPACT STORAGE;\nCREATE TABLE IF NOT EXISTS row_key_index (\n    key blob,\n    column1 blob,\n    value blob,\n    PRIMARY KEY ((key), column1)\n) WITH COMPACT STORAGE;\nCREATE TABLE IF NOT EXISTS row_key_time_index (\n    metric text,\n    row_time timestamp,\n    value text,\n    PRIMARY KEY ((metric), row_time)\n)\nCREATE TABLE IF NOT EXISTS row_keys (\n    metric text,\n    row_time timestamp,\n    data_type text,\n    tags frozen\u003cmap\u003ctext, text\u003e\u003e,\n    value text,\n    PRIMARY KEY ((metric, row_time), data_type, tags)\n)\nCREATE TABLE IF NOT EXISTS string_index (\n    key blob,\n    column1 blob,\n    value blob,\n    PRIMARY KEY ((key), column1)\n) WITH COMPACT STORAGE\n````\n\nThere are many more Cassandra based time series databases, they share very similar schema, you can find in [awesome time series database](https://xephonhq.github.io/awesome-time-series-database/?language=All\u0026backend=Cassandra). I am writing a new blog for more detailed survey on TSDB using Cassandra and how to write your own in Golang, I will update the link here once it's finished.\n\n### InfluxDB\n\n[InfluxDB](https://github.com/influxdata/influxdb) has [struggled a long time for their storage engine](https://docs.influxdata.com/influxdb/v1.3/concepts/storage_engine/) (leveldb, rocksdb, boltdb) before they settled with their time structured merge tree (TSM Tree). It can be separate into two parts, index for series identifiers and store for data points, we only focus on data points.\n\nTime structure merge tree (TSM), is a little bit misleading as log structured merge tree (LSM). \nThe key concept for both TSM and LSM is nor log or tree or time,\nit's **merge**. When write, data is stored in memory and then flushed to disk in large batch. When read, first read from memory, then read from disk and merge the result. When delete, a tombstone is added, and data with tombstone is not returned when read. In background, small chunks are merged into big chunks and items marked as deleted are truly removed to save disk space and speed up future query, this background procedure is called compaction. For time series data, compaction may increase compression ratio a lot for very regular data.\n\nA simplified version of TSM file is illustrated below, each chunk contains the series identifier, timestamps and values. \nNote that timestamps and values are stored separately instead of interleaved, which is why InfluxDB say they are using column format.\nInfluxDB use adaptive compression for data, it will loop through the data to see if it can be run length encoded, otherwise fallback to \n[Gorilla's](https://github.com/dgryski/go-tsz) or variable length encoding. Timestamps and value use different compression codec because\ntimestamps are normally very big integers (unix timestamp in millisecond or nanosecond) while value are normally small integer or float.\n\n````\nchunk\n--------------------------------------------------\n| id | compressed timestamps | compressed values |\n--------------------------------------------------\ntsm file\n-------------------------------------------------------------------\n| header | chunk 0 | chunk 1 | ... | chunk 10086 | index | footer |\n-------------------------------------------------------------------\n````\n\n## Hot topics in Time series databases\n\n### Fast response\n\nTime series database is used for analysis, and people don't want to wait in front of dashboard when production system is failing and \nuser's complain phone coming in, so fast response is a base requirement for any production ready time series database.\n\nThe most straight forward way is to put data into memory as much as possible.\nFacebook built [Gorilla](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf), now open sourced as [Beringei](https://github.com/facebookincubator/beringei), \nand its main contribution is using time series specific compression to store more data in memory.\n\nAnother way for speed up is pre-aggregation, also known as roll up. Because query often involve a long time range with coarse granularity, like\n`average daily cpu usage from June 1 to Aug 1`, those aggregations (average, min, max) can be computed when ingesting data, [BtrDB](https://github.com/SoftwareDefinedBuildings/btrdb) and [Akumuli](https://github.com/akumuli/Akumuli) store aggregation in upper level tree nodes so fine grained data won't be loaded when query is coarse grained.\n\nA proper ingest format could also reduce response time for both read and write, JSON is widely used, but Binary format is much better than textual format when a lot of number is involved, [protobuf](https://github.com/golang/protobuf/) could be a good choice.\n\n\u003c!-- ### Compression --\u003e\n\n### Retention\n\nNot all time series data is useful all time, if the system has been working well for the last two month, fine grained data can be dropped and only coarse grained is kept. \nThis is the default behavior of [RDDTool](https://oss.oetiker.ch/rrdtool/) and [Graphite](https://graphiteapp.org/), \nbut not the case for many newer scaled TSDB.\nDelete a file on local disk is easy but update a large amount of data in a distributed environment requires more caution to keep the system up all time, you don't want your monitoring systems failed before the system it is monitoring failed.\n\n\u003c!-- Also should the aggregated data get computed when data comes in or  --\u003e\n\n### Meta data indexing\n\nSeries identifier in general is the only meta data in time series database.\nDatabases like [Heroic](https://github.com/spotify/heroic) use ElasticSearch to store meta data, \nquery first goes to elasticsearch to retrieve the id for series, then data is loaded from Cassandra using id.\nA full search engine as Elasticsearch is powerful for sure, but the overhead of maintain another system and time spent\ncoordinating and communicating between two system can't be ignored. \nAlso some TSDB specific optimization may not be available when you don't have full control over metadata index building and storage.\nSo InfluxDB and Prometheus [wrote their own inverted index for indexing meta data](https://fabxc.org/blog/2017-04-10-writing-a-tsdb/).\n\n\u003c!-- ### Reduce write amplification --\u003e\n\u003c!-- ### Streaming --\u003e\n\u003c!-- Streaming has been hot for a long time, you must have heard Storm, Spark Streaming, Kafka etc. --\u003e\n\n### Tracing\n\nFolks from InfluxDB wrote a blog called [Metrics are dead](https://www.influxdata.com/blog/metrics-are-dead/) \nbecause during a conference for monitoring called [Monitorama](http://monitorama.com/) people say metrics can't provide enough insight as tracing does. \n(You can go to [OpenTracing](http://opentracing.io/) if you want to know more about tracing, and take a look at google's [Dapper paper](https://research.google.com/pubs/pub36356.html))\nTheir argument is tracing is for large scale distributed system, but there are many monolithic applications where metrics is enough (so metrics is not dead and you should use InfluxDB).\nI agree with them on the over emphasis of micro services, however my argument is **many time series database can be transfered into a tracing database**. \nTrace is a complex version of time series data points, \nif your value in a point is no longer a float value but a json payload with fields like parent span id, duration, it is a trace. \nOf course schema design, compression all need a lot of change, but many popular tracing solution like [Zipkin](https://github.com/openzipkin/zipkin)\n, Uber's [Jaeger](https://uber.github.io/jaeger/) is also using Cassandra like many TSDB do, there could be a middle ground.\n**Update:** InfluxDB already [tried to integrate Zipkin with their TICK stack](https://www.influxdata.com/blog/tracing-the-journey-of-a-transaction-as-it-propagates-through-a-distributed-system/)\nI spent too much time writing this blog.\n\nBecause the length of the blog we can't cover other hot topics like Compression, Pull vs Push, Streaming, Reduce write amplification etc,\nthey will be covered in future blogs.\n\n## Reference\n\n- [Awesome Time Series Database](https://github.com/xephonhq/awesome-time-series-database)\n- [Akumuli](https://github.com/akumuli/Akumuli)\n- [Beringei](https://github.com/facebookincubator/beringei)\n- [BtrDB](https://github.com/SoftwareDefinedBuildings/btrdb)\n- [Gorilla](http://www.vldb.org/pvldb/vol8/p1816-teller.pdf)\n- [Grafana](https://github.com/grafana/grafana)\n- [Graphite](https://graphiteapp.org/)\n- [Heroic](https://github.com/spotify/heroic)\n- [InfluxDB](https://github.com/influxdata/influxdb)\n- [Jaeger - Tracer](https://uber.github.io/jaeger/)\n- [KairosDB](https://kairosdb.github.io/)\n- [OpenTSDB](http://opentsdb.net/)\n- [Prometheus](https://prometheus.io/)\n- [RRDTool](https://oss.oetiker.ch/rrdtool/)\n- [Timescale - TSDB using Postgres](http://www.timescale.com/)\n- [VividCortex - TSDB using MySQL](https://www.vividcortex.com/blog/2014/12/16/in-case-you-missed-it-building-a-time-series-database-in-mysql/)\n- [Zipkin - Tracer](https://github.com/openzipkin/zipkin)\n\n## License\n\n- This article is licensed under [CC BY-NC-SA 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/).\n- Please contact \u003cmarketing@dongyue.io\u003e for commerical use.\n","cover":"","link":"introduction_to_time_series_database.html","preview":"\u003cp\u003eAn introduction to time series database basic concepts and hot topics\u003c/p\u003e\n","title":"Introduction to Time Series Database"},{"content":"\n\n#### 转载自 [gaocegege 的博客](http://gaocegege.com/Blog/%E9%98%85%E8%AF%BB/sre-0)\n\n## SRE 介绍\n\nSRE，全称是 Site Reilability Engineer，是一个类似于运维，但是跟传统运维不一样的职业，更加偏向于 DevOps。谷歌在 [SRE-谷歌运维解密](https://book.douban.com/subject/26875239/) 一书中分享了 SRE 的工作职责，以及谷歌在自己的运维工作中的一些经验。\n\n## 本文介绍\n\n这篇博客是系列文章中的第一篇，主要分享在阅读这本书时的一些感想。这本书在我看来更加适合在分布式领域或者在运维领域工作的工程师阅读，对于一个还在念书，没有完整接触过分布式系统实现的新手来说，有些过早了。因此就当是抛砖引玉，随便写写吧。\n\n这次关注的是书中的第六章，分布式系统的监控。\n\n## 关于作者\n\n第六章的作者是 [Rob Ewaschuk](https://www.linkedin.com/in/robewaschuk)。作者主要工作的领域是分布式存储，而且在自我介绍中写道自己在谷歌干的很过瘾，16-17年是不打算换工作的。O'Reilly 摘录了他在 SRE 一书中关于分布式监控的部分，做了一本电子书 [Monitoring Distributed Systems](http://www.oreilly.com/webops-perf/free/monitoring-distributed-systems.csp)。\n\n## 阅读之前\n\n在读文章之前，我对监控的了解非常浅薄。因为无论是在学校还是在之前实习，都没有涉及到对生产系统进行监控的工作。在念了研究生之后，稍微了解了一些关于分布式监控的知识。[Dapper, a Large-Scale Distributed Systems Tracing Infrastructure](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/36356.pdf) 是谷歌在 2010 年发表的论文，是关于其内部的分布式 tracing 系统的一个介绍性的论文。Tracing 在我的理解是细粒度监控中很关键的一部分。点评开源了一套这样的系统 [CAT](https://github.com/dianping/cat)。这些系统的作用就是跟踪系统相互之间的调用。比如 Web 前端调用了后端，就会生成一个从前端指向后端的 trace 记录。业界比较常见的实现是埋点或者修改字节码，前者更加可行。在阅读之前，对于分布式监控的了解也就仅限于此了。\n\n虽说读过相关论文，但是并没有真实使用过，最多就是去过点评，见过点评的 CAT 的 dashboard 是长什么样子的。\n\n## 正文\n\n全文中提到的一些东西让我非常感兴趣。其中有一句话：\n\n\u003e我们会避免任何『魔法』系统--例如视图自动学习阈值或者自动检测故障原因的系统。\n\n之前在去大众点评学习 CAT 系统时，听他们说下一步发展规划中，就有利用机器学习来学习阈值和原因的想法。我认为谷歌在为什么要保持监控系统简单时没有说清楚，这可能是跟他们的监控规模和信奉的哲学有关。他们把这类复杂的有各种特性的系统称为『魔法』系统，因为我也没有什么发言权。但是在我看来，随着复杂性的上升，引入机器学习等等是自动化的新阶段。现在可能人工的方式或者硬编码等等方式还是可以操作的，可能谷歌考虑到监控系统要尽可能稳定吧。但是机器学习可以更好地取代人工，就像在容量规划方面，我始终认为机器学习会比经验估计的更准。\n\n书中写了四个谷歌认为的黄金监控指标，分别是延迟、流量、错误和饱和度。对于延迟，他们提到的一点对我来说特别具有启发性，那就是要区分成功请求和错误请求的延迟。这两类请求有着不同的模式，是不能混为一谈的。之前用过的少数几个监控的工具都没有区分正确与错误请求的能力。这一点是在看了这本书后才学到的。\n\n还有一个比较有趣的指标，是饱和度。饱和度是指服务容量有多满，一般是用瓶颈资源的使用率来衡量。这样衡量饱和度的方式很取巧，之前没有过工程经验，都是各种指标全看一遍，最后看哪个资源不够用了，就断定服务满载了。如果事先判断好是 Memory-bound 还是 CPU-bound 类型的服务，然后每次只需要看对应的瓶颈资源就好了。\n\n关于长尾问题，谷歌给出了一种监控的方法，使用直方分布图而不是平均值来进行展示。因为可能一小部分请求导致了长尾，但是平均值是看不出这个问题的。\n\n在监控系统构建后，有一个值得考虑的问题，是短期可用性与长期可用性的冲突。短期的可用性体现在对问题的及时修复上，而长期的可用性在于对系统造成问题的根源的消除上。看起来这两者是统一的，但是其实是冲突的。人的精力是有限的，如果一直在处理 On-Call 的问题，那必然会导致缺少时间投入到根源性问题的解决上，这时需要权衡，放弃一些 On-Call 非核心的问题，去优化系统，提高长期预期的可用性。\n\n## 下文预告\n\n下一篇文章将会谈谈有关发布工程（Releasing Engineering）的事情。\n\n## 系列文章\n\n* [Google SRE 阅读笔记(1)-监控](http://blog.dongyueweb.com/google_sre_%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0%281%29-%E7%9B%91%E6%8E%A7.html)\n\n## License\n\n- This article is licensed under [CC BY-NC-SA 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/).\n- Please contact \u003cmarketing@dongyue.io\u003e for commerical use.\n","cover":"","link":"google_sre_阅读笔记(1)-监控.html","preview":"\u003cp\u003eSRE，全称是 Site Reilability Engineer，是一个类似于运维，但是跟传统运维不一样的职业，更加偏向于 DevOps。谷歌在 \u003ca href=\"https://book.douban.com/subject/26875239/\"\u003eSRE-谷歌运维解密\u003c/a\u003e 一书中分享了 SRE 的工作职责，以及谷歌在自己的运维工作中的一些经验。\u003c/p\u003e\n","title":"Google SRE 阅读笔记(1)-监控"},{"content":"\n\n今天为东岳搭建了一个饥荒的服务器，并不是特别复杂。饥荒对于服务器的要求是：\n\n```text\nInternet(Upload) = 8Kbytes/player/s\nRam = around 65Mbytes/player\nCPU = N/A\nVCRedist_2008 (x86)\n```\n\n因此选定配置的时候要计算下，服务器的最低配置要求。因为考虑到我们的玩家数最多也就20人左右，长期在线人数能在3-4人就不错了，因此一台1核2G内存的机器就可以满足我们的要求了。\n\n我们中的绝大多数玩家，都是在华东地区的，而只有一个美帝玩家。因此在服务器的选择上，华东节点是最合适的。在考察了包括阿里云、美团云、青云、腾讯云、Hyper.sh 在内的众多云服务提供商后，选择了最便宜的腾讯云。就流量来说，基本所有的服务商都是一个价钱，但是服务器的价格从 85 到 125 不等。Hyper.sh 因为没有华东节点，就没有关注价格。因为 steam 的 cmd 运行需要 32 位的环境，而且服务器的内存没有超过 4G，因此选择了 32位 Ubuntu 16.04.1 LTS。因为选择的云服务提供商和系统都很大众，因此在过程中并没有遇到什么坑。\n\n## 安装 steam 和 饥荒\n\n按照官方的文章，没什么好说的，不过为了简单，在搭建的过程中省略了创建用户的过程，直接在默认的用户目录下进行的。还有就是需要安装两个在官方教程中没有写到的东西：xfonts-75dpi 和 xfonts-100dpi，不然在运行 steamcmd.sh 的时候会报错 `Steam needs to be online to update`。\n\n```bash\nsudo apt-get install libgcc1\nsudo apt-get install xfonts-75dpi xfonts-100dpi\nmkdir ~/steamcmd\ncd ~/steamcmd\nwget https://steamcdn-a.akamaihd.net/client/installer/steamcmd_linux.tar.gz\ntar -xvzf steamcmd_linux.tar.gz\n./steamcmd.sh\nlogin anonymous\n# replace \u003cuser\u003e with your current user. if you use qcloud, ubuntu is the default username.\nforce_install_dir /home/\u003cuser\u003e/steamapps/DST\napp_update 343050 validate\nquit\ncd /home/steam/steamapps/DST/bin/\n```\n\n## 添加配置文件\n\n至此游戏服务器的所有二进制和依赖都安装好了，接下来需要进行配置。在 `/home/\u003cuser\u003e/.klei/DoNotStarveTogether/Cluster_1` 目录下需要建立两个文件，cluster.ini 和 cluster_token.txt。前者是对服务器的配置，后者是在饥荒的客户端游戏中生成的一个 token，猜测会用来校验玩家是否在使用正版游戏，等等。\n\ncluster.ini 文件内容很简单：\n\n```text\n[network]\ncluster_name = \u003ccluster_name\u003e\ncluster_intention = cooperative\ncluster_description = \u003ccluster_description\u003e\ncluster_port = 10999\ncluster_password = \u003cpasswd\u003e\n\n[misc]\nconsole_enabled = true\n\n[gameplay]\nmax_players = \u003cmax_players_num\u003e\npvp = false\ngame_mode = endless\npause_when_empty = true\n```\n\ncluster_token.txt 文件的内容需要用饥荒的客户端来生成，输入 `~` 打开游戏内置的 console，输入 `TheNet:GenerateClusterToken()`，不同系统会在不同位置生成一个 token：\n\n```text\nWindows:\n/My Documents/Klei/DoNotStarveTogether/cluster_token.txt\n\nLinux:\n ~/.klei/DoNotStarveTogether/cluster_token.txt\n\nMac OS X:\n~/Documents/Klei/DoNotStarveTogether/cluster_token.txt\n```\n\n然后将文件内容拷贝到 `/home/\u003cuser\u003e/.klei/DoNotStarveTogether/Cluster_1/cluster_token.txt` 中就行。\n\n## 运行\n\n```bash\n/home/\u003cuser\u003e/steamapps/DST/bin/dontstarve_dedicated_server_nullrenderer\n```\n\n官方推荐使用 screen 来维持服务器在退出 ssh 连接后依然在运行，但你喜欢怎么做就随便了。\n\n## Reference\n\n* [Don’t Starve Together（饥荒）服务器搭建](https://www.nevermoe.com/?p=695)\n\n## License\n\n- This article is licensed under [CC BY-NC-SA 3.0](https://creativecommons.org/licenses/by-nc-sa/3.0/).\n- Please contact \u003cmarketing@dongyue.io\u003e for commerical use.\n","cover":"","link":"在32位_ubuntu_16.04.1_lts_上安装饥荒服务器.html","preview":"\u003cp\u003e今天为东岳搭建了一个饥荒的服务器，并不是特别复杂。\u003c/p\u003e\n","title":"在32位 Ubuntu 16.04.1 LTS 上安装饥荒服务器"},{"content":"\n\n## 简介\n\n[Ayi](https://github.com/dyweb/Ayi) 是一个跨平台的命令行工具，类似于[busybox](https://busybox.net/about.html)。\n开始于 2015 年 7 月。主要目的是为了方便配置环境和解决各种由于配置环境导致的问题，比如:\n\n\u003e - 我这里跑的好好的，怎么到了你那(服务器上)就挂了\n\u003e - 我用 Mac 自带的 PHP 和 Apache 就挺好，我不用 Vagrant 和 Nginx\n\u003e - 我就想用 Windows 下的一键安装包\n\n考虑到没钱给大家每人配个 Mac，以及东岳的男女比例。\n我们需要一个跨平台的配置环境和收集环境信息的工具，用于**快速**的解决上述问题。\n\n## 技术选型\n\n在选择 Ayi 使用的技术时主要考虑的是以下几个问题\n\n- 跨平台\n- 可维护性\n- 对于东岳其他项目的帮助\n\n\u003c!-- TODO:找不到是哪个 issue 了，倒是找到了 commit https://github.com/dyweb/Ayi/commit/3a96921ccb6b5edb7c294e2a1eab2b9e63cc130b --\u003e\n最开始和咩的考虑是使用 shell 来进行操作， 但是 shell 的问题在于很难维护，基本不可能测试。\n东岳 shell 用的很少，并且 shell 对于其他项目帮助十分有限。\n\n之后考虑到 PHP, python, java 都需要运行时，C/C++ 写起来太累， Rust 没人会 (那会还没有 Ivan 和 Codeworm)，\n就选择了 Golang，当时版本是 1.5。\n\nGolang 的主要优点是\n\n- 跨平台 \u0026 交叉编译\n- 简洁的包管理\n- 性能好，可以用来改进东岳现有的纯 PHP 服务端体系\n- 一个活跃的社区，PHP 沉浸在 CMS 和抄 Rails 中不能自拔，JS 日新月异\n- Google 老爹\n\n## 主要问题\n\n- 人太少，基本只有 @at15 (我) 一个人\n- 需求不是很明确\n- 对 Golang 语言本身很不熟悉\n- Golang 的一些工具链不是很成熟，比如不支持依赖的 vendor 。\n\n但是由于项目拖了很长时间，后面三个问题基本都解决了\n\n- 主要需求是\n  - 生成器\n  - 环境检查\n  - [git 操作的简化](https://github.com/dyweb/Ayi/tree/master/app/git)\n  - [makefile 类似的自动化工具](https://github.com/dyweb/Ayi/tree/master/util/runner)\n  - [静态 web 服务器](https://github.com/dyweb/Ayi/tree/master/app/web)\n  - [进程管理](https://github.com/dyweb/Ayi/pull/64)\n  - waka time 服务器\n  - 文件传输\n- go 的版本从 1.5 跳到了 1.7。原生支持 vendor 并且有了很多更好的依赖管理工具，比如 [glide](https://github.com/Masterminds/glide)\n\n第一个问题的话，基本无解，目前东岳经常写 Golang 的人好像只有我和策策。策策有空就要去陪妹子，自然不可能陪我来填坑。\n(要有妹子的话我还会去填坑么?)\n\n## 实现的功能\n\n### Git 操作的简化\n\n前提是：你习惯使用 Golang 的 workspace，有关 workspace 我在以前东岳的讲座中[有提到](http://dongyueweb.com/course/web/2016_Spring/environment/slide.html#/4) (btw: 按方向键`下`而不是`右`)。我个人的工作区是这样的 (`cd ~/workspace \u0026\u0026 tree -L 4`)。\n\n````\n├── bin\n│   ├── Ayi\n│   ├── glide\n│   └── ink\n├── pkg\n│   └── linux_amd64\n│       └── github.com\n│           └── dyweb\n└── src\n    └── github.com\n        ├── at15\n        │   └── at15.github.io\n        ├── dyweb\n        │   ├── Ayi\n        │   └── blog\n        └── xephonhq\n            └── xephon-b\n````\n\n当使用 `git clone` 时后面必须跟完整的 remote 地址，并且默认 clone 到当前文件夹下，而使用\n`Ayi git clone` 地址可以是浏览器地址，并且根据配置文件，可以支持非默认端口的 ssh，比如东岳的 GitLab。\n从下面的输出可以看到 `Ayi git clone github.com/at15/at15.gihub.io` 被展开成了\n`git clone git@github.com:at15/at15.github.io.git /home/at15/workspace/src/github.com/at15/at15.github.io`。\n\n````\nat15@pc4038:~/workspace|⇒  Ayi git clone github.com/at15/at15.github.io\nINFO[0000] git clone git@github.com:at15/at15.github.io.git /home/at15/workspace/src/github.com/at15/at15.github.io pkg=a.a.git\nCloning into '/home/at15/workspace/src/github.com/at15/at15.github.io'...\nremote: Counting objects: 435, done.\nremote: Total 435 (delta 0), reused 0 (delta 0), pack-reused 435\nReceiving objects: 100% (435/435), 3.56 MiB | 1.64 MiB/s, done.\nResolving deltas: 100% (234/234), done.\nChecking connectivity... done.\nINFO[0002] Sucessfully cloned to: /home/at15/workspace/src/github.com/at15/at15.github.io pkg=a.cmd\n````\n\nbtw: `Ayi` 的 log 组件看上去很像 [logrus](https://github.com/sirupsen/logrus)，但其实是[自己的轮子](https://github.com/dyweb/Ayi/pull/60)\n\n### 自动化\n\n自动化部分很类似 `npm run`，但是主要有以下区别\n\n- 使用 yaml 而不是 json, json 不支持注释，而且即使使用支持注释的 parser，编辑器也会有提示\n- 支持一个指令对应一系列命令, 类似 Travis 等 CI 的配置文件\n- 目前[新的重构](https://github.com/dyweb/Ayi/pull/64)可能会把它改成类似 + 的工具\n\n````\ndebug: true\ndep-install:\n    - go get github.com/at15/go.rice/rice\n    - go get github.com/mitchellh/gox\n    - glide install\ninstall:\n    - go build -o Ayi\n    - rice append -i github.com/dyweb/Ayi/app/web --exec Ayi\n    - sh -c \"mv Ayi $GOPATH/bin/Ayi\"\ntest:\n    - go install\n    - sh -c \"go test -v -cover $(glide novendor)\"\nscripts:\n    build: gox -output=\"build/Ayi_{{.OS}}_{{.Arch}}\"\n````\n\n内置指令如`install`, `test` 跟 `Ayi run \u003cscript-name\u003e` 都是使用 `util/runner`。\n目前准备把 runner 做成一个通用的 package，\n因此[又在重构](https://github.com/dyweb/Ayi/pull/64)来增加如下的功能\n\n- 类似 [Ansible](https://www.ansible.com/) 的更丰富的配置\n- 类似[ PM2](http://pm2.keymetrics.io/) 和 [Foreman](https://github.com/ddollar/foreman) 的进程管理\n\n### 静态服务器\n\n双击一个 html 文件多半会看不了，经典的解决方案是 `python -m SimpleHTTPServer \u003cport\u003e`，\n然而 windows 并不预装 py，而且有时候我想侧边栏显示文件树，markdown 高亮，\n遇到学习文件夹自动播放并且在没有插耳机的情况下静音。\n以前自己挖了一个坑 [doc-viewer](https://github.com/at15/doc-viewer) 。\nAyi 里目前只实现了基本的静态服务器 `Ayi web static`（不要被 help 骗了，根本没有 highlight)。\n\n````\n⇒  Ayi web static -h\nserve static file like python's SimpleHTTPServer, support highlight and markdown render inspired by https://github.com/at15/doc-viewer\n\nUsage:\n  Ayi web static [flags]\n\nGlobal Flags:\n      --config string   config file (default is $HOME/.ayi.yaml)\n  -n, --dry-run         show commands to execute\n  -p, --port int        port to listen on (default 3000)\n      --root string     server root folder\n  -v, --verbose         verbose output\n````\n\n## 使用开源库中遇到的问题\n\n虽然我们要站在巨人的肩膀上，但是站的久了就会发现有些巨人其实也有点 low，比如\n\n- 不支持 windows 的 [overall](https://github.com/go-playground/overalls)，[fork](https://github.com/at15/overalls)\n- 不支持 ignore 的 [go.rice](https://github.com/GeertJohan/go.rice), [fork](https://github.com/at15/go.rice/tree/feature/ignore) 和 [issue](https://github.com/GeertJohan/go.rice/issues/83)\n- 不支持 filter 的 [logrus](https://github.com/sirupsen/logrus)，还自带[统计运行时间的 bug](https://github.com/sirupsen/logrus/issues/457)\n\n一些库虽然 star 很高，但是其实如果仔细看代码的话会发现很多问题，同时看别人的代码可以学到一些自己以前忽略的问题，比如 Golang 里 struct 的方法的 thread safe。\n相关的 issue [dyweb/Ayi#59](https://github.com/dyweb/Ayi/issues/59) [at15/go-learning#3](https://github.com/at15/go-learning/issues/3)。\nlogrus 里对应的代码如下，作为**读者的练习**。\n\n\u003c!-- TODO: no highlight --\u003e\n````golang\n// This function is not declared with a pointer value because otherwise\n// race conditions will occur when using multiple goroutines\nfunc (entry Entry) log(level Level, msg string) {\n        var buffer *bytes.Buffer\n\tentry.Time = time.Now()\n\tentry.Level = level\n\tentry.Message = msg\n````\n\n一些(很多)开源库都维护状态都是很不乐观的，上面提到的几个开 PR 和 Feature Request 的 issue\n都是没人鸟的，既然已经看了那么多了，为什么不自己写呢？ 所以就开始造轮子了(其实还是想造轮子)。\n\nbtw: 在使用开源项目的过程中完全没有必要去埋怨作者无视你的各种请求和贡献，换位思考一下，\n你是愿意陪妹子玩一晚上呢，还是愿意改 Gayhub 上某个不认识的人反馈的 bug 呢 (没有妹子的人表示思考不出来，我选择去改 bug)。\n\n## 通用库 (轮子)\n\n自己造轮子有以下几个优点:\n\n- 方便维护\n- 代码风格一致，比如 [spf13](https://github.com/spf13/) 的 [viper](https://github.com/spf13/viper) 和 [cobra](https://github.com/spf13/cobra/)\n- 可以共用很多 code base\n\n当然关键还是程序员的天性，上面的都是借口。\n\nAyi 里抽出来的库有以下几个\n\n### Log\n\nhttps://github.com/dyweb/Ayi/tree/master/common/log 仿照 [logrus](https://github.com/sirupsen/logrus) 实现,\n目标功能类似 log4j ([logback](http://logback.qos.ch/))\n\n有以下几个特点\n\n- 支持类似 log4j 的按照 package 进行 filter，避免了:\n  - 开启 debug 之后大量输出淹没了需要的信息\n  - 为了 debug，把代码里的 debug 改成 info，忘记改回去\n- 支持更多的 Level (你想加个 Hearbreak 什么的 Level 也可以 `log.Hearbreak(\"got a good man card on New Year's Eve\")`)\n- 减少了 lock (不过没做 benchmark)\n- 移除了 logger 上与 logEntry 重复的接口\n\n之后计划\n\n- 改用 generator 生成代码，`Debugf` 和其他所有 `*f` 都只差一个单词，为什么要人写呢 (我就不说我拼写错误然后 painc 了)。\n- 支持 log4j 的 appender, transformer, xml etc.\n\n### Runner\n\n之前在自动化的部分已经基本说过了，所以就不说了(就是想加个标题)。\n\n### Structure\n\nGolang 内置的数据结构少的可怜，作为一个用了3天 python 的人当然要加一点数据结构。\n\n目前实现的有\n\n- [Set](https://github.com/dyweb/Ayi/tree/common-util/runner/common/structure)\n(一开始只有 Contains 没有 Add 用了才发现这个 Set 是 immutable 的)。\n- 没有然后了\n\n### Requests\n\n`net/http` 很好用，但是 `python` 的 `requests` 更简洁，不过这个轮子目前在[另一项目(xephon-b)里](https://github.com/xephonhq/xephon-b/tree/master/pkg/util/requests)\n\nBefore\n\n````golang\nfunc (client *KairosDBHTTPClient) Ping() error {\n\tres, err := http.Get(client.Config.Host.HostURL() + \"/api/v1/version\")\n\tif err != nil {\n\t\tlog.Warn(\"can't get kairosdb version\")\n\t\tlog.Debug(err.Error())\n\t\treturn err\n\t}\n\tdefer res.Body.Close()\n\tresContent, err := ioutil.ReadAll(res.Body)\n\tif err != nil {\n\t\tlog.Warn(\"can't read response body\")\n\t\tlog.Debug(err.Error())\n\t\treturn err\n\t}\n\tvar resData map[string]string\n\tif err := json.Unmarshal(resContent, \u0026resData); err != nil {\n\t\tlog.Warn(\"can't parse json\")\n\t\tlog.Debug(err.Error())\n\t\treturn err\n\t}\n\tlog.Info(\"KairosDB version is \" + resData[\"version\"])\n\treturn nil\n}\n````\n\nAfter\n\n````golang\nfunc (client *KairosDBHTTPClient) Ping() error {\n\tversionURL := client.Config.Host.HostURL() + \"/api/v1/version\"\n\tres, err := requests.GetJSON(versionURL)\n\tif err != nil {\n\t\treturn errors.Wrapf(err, \"can't reach KairosDB via %s\", versionURL)\n\t}\n\tlog.Info(\"KairosDB version is \" + res[\"version\"])\n\treturn nil\n}\n````\n\n## 开发计划\n\n上面说了那么多，一半都是画饼，可以从 issue 里看最近的进度\n\n- [正在开发的部分](https://github.com/dyweb/Ayi/issues?q=is%3Aopen+is%3Aissue+label%3Aworking)\n- [想做但是被搁置了的 issue](https://github.com/dyweb/Ayi/issues?utf8=%E2%9C%93\u0026q=is%3Aissue%20label%3Abacklog)\n\n~~欢迎感兴趣的女同学联系我! 我的微信是 `uictor`~~\n\n预计等到国内寒假的时候很多坑可以填完了，到时候欢迎假期想了解一下 Golang 的小伙伴来玩，我会加 `help wanted` 和难度的 label。\n\n## 开发人员\n\n[GitHub 传送门](https://github.com/dyweb/Ayi/graphs/contributors)\n\n- 咩在项目开始时提交了一些 shell 脚本，但是由于转到了 Golang 以及咩一向很忙，遂弃婶\n- @kdplus (丘) 参与过 `Ayi check` 的开发，不过那时我 Golang 菜的抠脚，导致丘也在划水。\n- @gaocegege (策策) 因为周报的功能，参与过一段时间的开发，\n引入了`Godep` 交叉编译，不过最后周报的功能并没有投入实用。\n\n## 总结\n\n- 等有钱了，给大家都配 MBP\n- 自己开的坑，不能让别人填 (我去开个找妹子的坑先)\n\n## 杂项\n\n- 使用 `git log -reverse` 可以反过来看 log, 可以用来找第一个提交。\n- shell 在 windows 下基本不会有问题，因为为了使用 git，东岳所有的 windows 用户都安装了\nmsysgit (现在叫 git for windows)，它自带了 bash 和一些基本的工具。\n- 周报的功能作为 MOS 的一个项目交给了 @codeworm96, 进度见[这个issue](https://github.com/dyweb/mos/issues/1)\n- [所有带 `backlog` 标签的 issue](https://github.com/dyweb/Ayi/issues?q=is%3Aissue+label%3Abacklog+is%3Aclosed)\n\n第一个提交\n````\ncommit 19858fe3958317da08dc512116c58acbd82b2a35\nAuthor: At15 \u003cat15@outlook.com\u003e\nDate:   Sun Jul 26 13:24:38 2015 +0800\n\n    Initial commit\n````\n\n## 更新\n\n## 引用\n\n## 许可协议\n\n- 本文遵守[创作共享CC BY-NC-SA 3.0协议](https://creativecommons.org/licenses/by-nc-sa/3.0/cn/)\n- 网络平台转载请联系 \u003cmarketing@dongyue.io\u003e\n","cover":"","link":"ayi.html","preview":"\u003cp\u003eAyi 跨平台的命令行工具(库)\u003c/p\u003e\n","title":"Ayi"}]